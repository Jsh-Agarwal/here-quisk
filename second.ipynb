{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "Comprehensive solution for automatic roundabout identification<br>\n",
    "using probe data and machine learning techniques.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures.process import BrokenProcessPool\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, pairwise_distances\n",
    "from sklearn.utils import class_weight\n",
    "from scipy.spatial import KDTree\n",
    "import hdbscan\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from kneed import KneeLocator\n",
    "import matplotlib.patches as patches\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======== Utility Functions ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy(lnglat, truncate=False):\n",
    "    \"\"\"Convert longitude and latitude to web mercator x, y\"\"\"\n",
    "    if isinstance(lnglat, pd.DataFrame):\n",
    "        lng, lat = lnglat['longitude'].values, lnglat['latitude'].values\n",
    "    else:\n",
    "        lng, lat = lnglat[:, 0], lnglat[:, 1]\n",
    "    \n",
    "    if truncate:\n",
    "        lng = np.clip(lng, -180.0, 180.0)\n",
    "        lat = np.clip(lat, -90.0, 90.0)\n",
    "    \n",
    "    x = 6378137.0 * np.radians(lng)\n",
    "    y = 6378137.0 * np.log(np.tan((math.pi * 0.25) + (0.5 * np.radians(lat))))\n",
    "    \n",
    "    if isinstance(lnglat, pd.DataFrame):\n",
    "        return pd.DataFrame({'x': x, 'y': y})\n",
    "    return np.array((x, y)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latlng_to_tile(lng, lat, zoom):\n",
    "    \"\"\"Convert latitude and longitude to tile coordinates\"\"\"\n",
    "    n = 2 ** zoom\n",
    "    lat_rad = np.radians(lat)\n",
    "    tile_x = int((lng + 180.0) / 360.0 * n)\n",
    "    tile_y = int((1.0 - np.log(np.tan(lat_rad) + (1 / np.cos(lat_rad))) / math.pi) / 2.0 * n)\n",
    "    return tile_x, tile_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_heading_change(group):\n",
    "    \"\"\"Calculate mean absolute heading change for a group\"\"\"\n",
    "    return np.abs(np.diff(group['heading'])).mean() if len(group) > 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_curvature(group):\n",
    "    \"\"\"Calculate mean curvature for a group\"\"\"\n",
    "    if len(group) < 3:\n",
    "        return 0\n",
    "    x = group['longitude'].values\n",
    "    y = group['latitude'].values\n",
    "    dx = np.gradient(x)\n",
    "    dy = np.gradient(y)\n",
    "    ds = np.sqrt(dx*dx + dy*dy)\n",
    "    d2x = np.gradient(dx, ds, edge_order=1)\n",
    "    d2y = np.gradient(dy, ds, edge_order=1)\n",
    "    curvature = np.abs(dx * d2y - dy * d2x) / (dx * dx + dy * dy)**1.5\n",
    "    return np.mean(curvature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_heading(cluster_df):\n",
    "    \"\"\"Analyze heading changes within a cluster\"\"\"\n",
    "    heading_changes = np.diff(cluster_df['heading'].values)\n",
    "    return np.mean(heading_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cluster_features(cluster_df):\n",
    "    \"\"\"Extract comprehensive features from a cluster\"\"\"\n",
    "    features = {\n",
    "        'mean_heading_change': cluster_df['heading_change'].mean(),\n",
    "        'std_heading_change': cluster_df['heading_change'].std(),\n",
    "        'mean_speed': cluster_df['speed'].mean(),\n",
    "        'std_speed': cluster_df['speed'].std(),\n",
    "        'cluster_size': len(cluster_df),\n",
    "        'latitude': cluster_df['latitude'].mean(),\n",
    "        'longitude': cluster_df['longitude'].mean(),\n",
    "        'density': len(cluster_df) / (np.max(cluster_df['longitude']) - np.min(cluster_df['longitude'])) / \n",
    "                  (np.max(cluster_df['latitude']) - np.min(cluster_df['latitude'])) if len(cluster_df) > 1 else 0,\n",
    "        'radius': np.mean(np.sqrt((cluster_df['longitude'] - cluster_df['longitude'].mean())**2 + \n",
    "                               (cluster_df['latitude'] - cluster_df['latitude'].mean())**2))\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(fname):\n",
    "    \"\"\"Process a single probe data file\"\"\"\n",
    "    try:\n",
    "        print(f\"Processing {os.path.basename(fname)}\")\n",
    "        df = pd.read_csv(fname)\n",
    "        coords_list = []\n",
    "        df['sampledate'] = pd.to_datetime(df['sampledate'])\n",
    "        for _, group in df.groupby(['traceid']):\n",
    "            try:\n",
    "                group = group[group['speed'] != 0].sort_values('sampledate')\n",
    "                if len(group) < 2 or group['speed'].max() < 15:\n",
    "                    continue\n",
    "                time_diff = group['sampledate'].diff().dt.total_seconds()\n",
    "                heading_diff = group['heading'].diff()\n",
    "                derivative = heading_diff / time_diff\n",
    "                filtered_group = group[derivative < -20]  # Adjust threshold as needed\n",
    "                coords_list.extend(filtered_group[['latitude', 'longitude']].values.tolist())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing group in {os.path.basename(fname)}: {e}\")\n",
    "                continue\n",
    "        return coords_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {os.path.basename(fname)}: {e}\")\n",
    "        return []  # Return empty list on error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New helper function for safe parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_process_files(files, process_func, max_workers=None):\n",
    "    \"\"\"Safely process files in parallel with fallback to sequential processing\"\"\"\n",
    "    if max_workers is None:\n",
    "        # Use half the CPU cores to avoid memory issues\n",
    "        max_workers = max(1, os.cpu_count() // 2)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Attempting parallel processing with {max_workers} workers\")\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Use timeout to avoid hanging workers\n",
    "            results = list(executor.map(process_func, files, timeout=300))\n",
    "        return results\n",
    "    except (BrokenProcessPool, TimeoutError, Exception) as e:\n",
    "        print(f\"Parallel processing failed: {e}\")\n",
    "        print(\"Falling back to sequential processing...\")\n",
    "        results = []\n",
    "        for file in files:\n",
    "            result = process_func(file)\n",
    "            results.append(result)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======== Trajectory-based Roundabout Detection ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_based_detection(data_dir, known_roundabouts_path, num_files=10):\n",
    "    \"\"\"Detect roundabouts using trajectory-based approach\"\"\"\n",
    "    print(\"\\n=== Starting Trajectory-Based Detection ===\")\n",
    "    \n",
    "    # Load data\n",
    "    start_time = time.time()\n",
    "    csv_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
    "    \n",
    "    if num_files > 0:\n",
    "        csv_files = csv_files[:num_files]  # Limit the number of files for testing\n",
    "        \n",
    "    print(f\"Loading {len(csv_files)} probe data files...\")\n",
    "    \n",
    "    # Process files using the safe parallel processing function\n",
    "    results = safe_process_files(csv_files, process_file)\n",
    "    \n",
    "    # Flatten results into a single list of coordinates\n",
    "    coords_list = [coord for sublist in results for coord in sublist if sublist]  # Skip empty results\n",
    "    \n",
    "    if not coords_list:\n",
    "        print(\"No valid coordinates found. Check your data sources.\")\n",
    "        return None, None\n",
    "    \n",
    "    coords = np.array(coords_list)\n",
    "    \n",
    "    print(f\"Processed {len(coords)} data points in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Load known roundabouts for validation\n",
    "    roundabout_data = pd.read_csv(known_roundabouts_path)\n",
    "    print(f\"Loaded {len(roundabout_data)} known roundabouts\")\n",
    "    \n",
    "    # Prepare data for clustering\n",
    "    features = pd.DataFrame(coords, columns=['latitude', 'longitude'])\n",
    "    \n",
    "    # First-level clustering to group nearby points\n",
    "    print(\"Performing DBSCAN clustering...\")\n",
    "    epsilon = 0.0005  # Adjust based on data scale\n",
    "    min_samples = 5\n",
    "    db = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "    features['cluster'] = db.fit_predict(coords)\n",
    "    \n",
    "    print(f\"Found {len(set(features[features['cluster'] != -1]['cluster']))} clusters\")\n",
    "    \n",
    "    # Extract cluster features\n",
    "    clustered_points = features[features['cluster'] != -1]\n",
    "    cluster_features_list = []\n",
    "    \n",
    "    for cluster_id, cluster_df in clustered_points.groupby('cluster'):\n",
    "        # Skip clusters with too few points\n",
    "        if len(cluster_df) < 10:\n",
    "            continue\n",
    "            \n",
    "        # Calculate cluster centroid\n",
    "        centroid_lat = cluster_df['latitude'].mean()\n",
    "        centroid_lng = cluster_df['longitude'].mean()\n",
    "        \n",
    "        # Calculate distance of each point to centroid\n",
    "        cluster_df['distance_to_center'] = np.sqrt(\n",
    "            (cluster_df['latitude'] - centroid_lat)**2 +\n",
    "            (cluster_df['longitude'] - centroid_lng)**2\n",
    "        )\n",
    "        \n",
    "        # Extract features for this cluster\n",
    "        cluster_features = {\n",
    "            'cluster_id': cluster_id,\n",
    "            'latitude': centroid_lat,\n",
    "            'longitude': centroid_lng,\n",
    "            'point_count': len(cluster_df),\n",
    "            'mean_distance': cluster_df['distance_to_center'].mean(),\n",
    "            'std_distance': cluster_df['distance_to_center'].std(),\n",
    "            'max_distance': cluster_df['distance_to_center'].max(),\n",
    "            'circularity': cluster_df['distance_to_center'].std() / cluster_df['distance_to_center'].mean()\n",
    "        }\n",
    "        \n",
    "        # Label as roundabout if close to a known roundabout (for training)\n",
    "        min_distance = float('inf')\n",
    "        for _, roundabout in roundabout_data.iterrows():\n",
    "            dist = np.sqrt((centroid_lat - roundabout['latitude'])**2 + \n",
    "                          (centroid_lng - roundabout['longitude'])**2)\n",
    "            if dist < min_distance:\n",
    "                min_distance = dist\n",
    "                \n",
    "        cluster_features['is_roundabout'] = 1 if min_distance < 0.001 else 0  # Threshold for matching\n",
    "        cluster_features_list.append(cluster_features)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    cluster_features_df = pd.DataFrame(cluster_features_list)\n",
    "    \n",
    "    if len(cluster_features_df) == 0:\n",
    "        print(\"No valid clusters found for analysis\")\n",
    "        return None, None\n",
    "        \n",
    "    print(f\"Extracted features for {len(cluster_features_df)} clusters\")\n",
    "    \n",
    "    # Train a machine learning model to identify roundabouts\n",
    "    X = cluster_features_df.drop(['cluster_id', 'latitude', 'longitude', 'is_roundabout'], axis=1)\n",
    "    y = cluster_features_df['is_roundabout']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns, \n",
    "        'importance': rf_model.feature_importances_\n",
    "    })\n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(feature_importance.sort_values('importance', ascending=False))\n",
    "    \n",
    "    # Predict on all clusters\n",
    "    cluster_features_df['predicted_roundabout'] = rf_model.predict(X)\n",
    "    \n",
    "    # Display results\n",
    "    detected_roundabouts = cluster_features_df[cluster_features_df['predicted_roundabout'] == 1]\n",
    "    print(f\"\\nDetected {len(detected_roundabouts)} potential roundabouts\")\n",
    "    \n",
    "    return cluster_features_df, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======== Advanced Trajectory Analysis ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_trajectory_analysis(data_dir, known_roundabouts_path, num_files=10):\n",
    "    \"\"\"Perform more advanced trajectory analysis using HDBSCAN\"\"\"\n",
    "    print(\"\\n=== Starting Advanced Trajectory Analysis ===\")\n",
    "    \n",
    "    # Load data\n",
    "    csv_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
    "    \n",
    "    if num_files > 0:\n",
    "        csv_files = csv_files[:num_files]  # Limit the number of files for testing\n",
    "    \n",
    "    print(f\"Loading {len(csv_files)} probe data files...\")\n",
    "    \n",
    "    # Load data safely (one file at a time to avoid memory issues)\n",
    "    dataframes = []\n",
    "    for f in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {os.path.basename(f)}: {e}\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        print(\"No valid data loaded.\")\n",
    "        return None\n",
    "        \n",
    "    probe_data = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Load known roundabouts\n",
    "    roundabout_data = pd.read_csv(known_roundabouts_path)\n",
    "    print(f\"Loaded {len(roundabout_data)} known roundabouts\")\n",
    "    \n",
    "    print(f\"Initial probe data shape: {probe_data.shape}\")\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    probe_data.dropna(inplace=True)\n",
    "    probe_data = probe_data[probe_data['speed'] > 0]\n",
    "    \n",
    "    # Filter out pedestrians\n",
    "    max_speeds = probe_data.groupby('traceid')['speed'].max()\n",
    "    pedestrians = max_speeds[max_speeds < 15].index\n",
    "    probe_data = probe_data[~probe_data['traceid'].isin(pedestrians)]\n",
    "    \n",
    "    print(f\"Probe data shape after filtering: {probe_data.shape}\")\n",
    "    \n",
    "    # Feature engineering\n",
    "    probe_data['heading_change'] = probe_data.groupby('traceid')['heading'].diff().fillna(0).abs()\n",
    "    \n",
    "    # Group by traceid and calculate features\n",
    "    grouped = probe_data.groupby('traceid')\n",
    "    features = grouped.agg({\n",
    "        'latitude': 'mean',\n",
    "        'longitude': 'mean',\n",
    "        'speed': 'mean',\n",
    "    }).reset_index()\n",
    "    \n",
    "    features['heading_change'] = grouped.apply(calculate_heading_change)\n",
    "    features['curvature'] = grouped.apply(calculate_curvature)\n",
    "    \n",
    "    # Add additional features\n",
    "    features['point_count'] = grouped.size()\n",
    "    features['distance'] = grouped.apply(lambda g: np.sum(np.sqrt(\n",
    "        np.diff(g['longitude'])**2 + np.diff(g['latitude'])**2)) if len(g) > 1 else 0\n",
    "    )\n",
    "    \n",
    "    # Remove any rows with NaN values\n",
    "    features = features.dropna()\n",
    "    print(f\"Features shape after processing: {features.shape}\")\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features.drop(['traceid', 'latitude', 'longitude'], axis=1))\n",
    "    \n",
    "    # Apply HDBSCAN clustering\n",
    "    if scaled_features.shape[0] > 0:\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3)\n",
    "        clusterer.fit(scaled_features)\n",
    "        \n",
    "        # Add cluster labels to features\n",
    "        features['cluster'] = clusterer.labels_\n",
    "        \n",
    "        # Label data points as roundabouts or not\n",
    "        def is_roundabout(lat, lon):\n",
    "            return any((roundabout_data['latitude'] - lat)**2 + \n",
    "                      (roundabout_data['longitude'] - lon)**2 < 1e-6)\n",
    "        \n",
    "        features['is_roundabout'] = features.apply(\n",
    "            lambda row: is_roundabout(row['latitude'], row['longitude']), axis=1\n",
    "        )\n",
    "        \n",
    "        # Prepare data for classification\n",
    "        X = features.drop(['traceid', 'latitude', 'longitude', 'is_roundabout', 'cluster'], axis=1)\n",
    "        y = features['is_roundabout']\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Train Random Forest model\n",
    "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "        print(\"\\nModel Evaluation:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns, \n",
    "            'importance': rf_model.feature_importances_\n",
    "        })\n",
    "        print(\"\\nFeature Importance:\")\n",
    "        print(feature_importance.sort_values('importance', ascending=False))\n",
    "        \n",
    "        # Predict roundabouts on all data\n",
    "        features['predicted_roundabout'] = rf_model.predict(X)\n",
    "        \n",
    "        print(f\"\\nDetected {features['predicted_roundabout'].sum()} potential roundabouts\")\n",
    "        \n",
    "        return features\n",
    "    else:\n",
    "        print(\"No data available after preprocessing.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======== K-means Clustering with Optimal K Selection ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_detection(data_dir, known_roundabouts_path, batch_id=0, api_key=None):\n",
    "    \"\"\"Detect roundabouts using k-means with optimal clustering\"\"\"\n",
    "    print(f\"\\n=== Starting K-means Detection for Batch {batch_id} ===\")\n",
    "    \n",
    "    # Setup paths\n",
    "    path = os.path.join(data_dir, f\"{batch_id}/*.csv\")\n",
    "    output_dir = f\"centroid_images/{batch_id}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load roundabouts\n",
    "    roundabouts = pd.read_csv(known_roundabouts_path)\n",
    "    roundabout = roundabouts[roundabouts['bbox'] == batch_id]\n",
    "    \n",
    "    # Process files using the safe parallel processing function\n",
    "    results = safe_process_files(glob.glob(path), process_file_for_kmeans)\n",
    "    \n",
    "    coords_list = [coord for sublist in results for coord in sublist]\n",
    "    coords = np.array(coords_list)\n",
    "    \n",
    "    if len(coords) == 0:\n",
    "        print(\"No coordinates found for analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Filter points by distance threshold for density\n",
    "    filtered_coords = filter_points_by_distance(coords, threshold=0.0005)\n",
    "    print(f\"Filtered {len(coords) - len(filtered_coords)} points, {len(filtered_coords)} remaining\")\n",
    "    \n",
    "    # Determine optimal number of clusters using elbow method\n",
    "    distortions = []\n",
    "    k_range = range(2, 9)\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(filtered_coords)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    \n",
    "    # Use the KneeLocator to find the \"elbow\" point\n",
    "    kneedle = KneeLocator(k_range, distortions, curve='convex', direction='decreasing')\n",
    "    optimal_clusters = kneedle.elbow\n",
    "    if optimal_clusters is None:\n",
    "        optimal_clusters = 5  # Default if no clear elbow is found\n",
    "    else:\n",
    "        optimal_clusters += 2  # Add a bit more clusters for better granularity\n",
    "    \n",
    "    print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
    "    \n",
    "    # Perform k-means clustering with optimal k\n",
    "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=420, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(filtered_coords)\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    filtered_data = pd.DataFrame(filtered_coords, columns=['latitude', 'longitude'])\n",
    "    filtered_data['cluster'] = cluster_labels\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    scatter = ax.scatter(filtered_data['longitude'], filtered_data['latitude'], \n",
    "                       c=filtered_data['cluster'], cmap='viridis', \n",
    "                       marker='o', s=10, alpha=0.6)\n",
    "    \n",
    "    ax.scatter(roundabout['longitude'], roundabout['latitude'], \n",
    "               c='red', marker='x', s=50, label='Known Roundabouts')\n",
    "    \n",
    "    # Get centroids\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # If API key is provided, fetch map tiles for centroids\n",
    "    if api_key:\n",
    "        for i, centroid in enumerate(centroids):\n",
    "            lat, lon = centroid[0], centroid[1]\n",
    "            image_data = fetch_map_image(lon, lat, api_key)\n",
    "            \n",
    "            if image_data:\n",
    "                # Save the image\n",
    "                image_filename = os.path.join(output_dir, f\"map_image_{lat},{lon}.png\")\n",
    "                with open(image_filename, \"wb\") as image_file:\n",
    "                    image_file.write(image_data)\n",
    "                print(f\"Saved image for centroid {i + 1} at coordinates ({lat}, {lon})\")\n",
    "    \n",
    "    # Add squares and centroids to plot\n",
    "    plot_cluster_squares_and_centroids(ax, filtered_data, cluster_labels)\n",
    "    \n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title(f'Clustered Probe Data {batch_id} with {optimal_clusters} clusters')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{batch_id}_cluster_graph.png\")\n",
    "    \n",
    "    return filtered_data, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======== Image-based Roundabout Detection ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_roundabout_cnn(input_shape=(224, 224, 3), num_classes=1):\n",
    "    \"\"\"Create an advanced CNN model for roundabout detection from satellite imagery\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        # First Convolutional Block\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "        # Fully connected layers\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_roundabout_cnn(train_dir, test_dir, epochs=5):\n",
    "    \"\"\"Train a CNN model for roundabout detection\"\"\"\n",
    "    # Constants\n",
    "    IMG_HEIGHT = 224\n",
    "    IMG_WIDTH = 224\n",
    "    BATCH_SIZE_TRAIN = 32\n",
    "    BATCH_SIZE_TEST = 20\n",
    "    \n",
    "    # Data Augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255,\n",
    "        rotation_range=360,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "    \n",
    "    # Data Generators\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE_TRAIN,\n",
    "        class_mode='binary',\n",
    "        subset='training',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE_TEST,\n",
    "        class_mode='binary',\n",
    "        subset='validation',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE_TEST,\n",
    "        class_mode='binary',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Calculate class weights\n",
    "    labels = np.array(train_generator.labels)\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(labels),\n",
    "        y=labels\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(\"Class weights:\", class_weight_dict)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = create_advanced_roundabout_cnn(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy(threshold=0.3),\n",
    "            tf.keras.metrics.Precision(thresholds=0.3),\n",
    "            tf.keras.metrics.Recall(thresholds=0.3)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=0.00001\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training CNN model...\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        class_weight=class_weight_dict\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_generator)\n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test precision: {test_precision:.4f}\")\n",
    "    print(f\"Test recall: {test_recall:.4f}\")\n",
    "    \n",
    "    # Predict with custom threshold\n",
    "    predictions = model.predict(test_generator)\n",
    "    y_pred = (predictions > 0.3).astype(int)\n",
    "    y_true = test_generator.labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(f\"Custom threshold accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Custom threshold precision: {precision:.4f}\")\n",
    "    print(f\"Custom threshold recall: {recall:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_threshold(model, image_path, threshold=0.3):\n",
    "    \"\"\"Make prediction with custom threshold\"\"\"\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = img_array / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    prediction = model.predict(img_array)[0][0]\n",
    "    result = \"roundabout\" if prediction > threshold else \"no roundabout\"\n",
    "    \n",
    "    return prediction, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_map_tiles(coordinates, api_key, zoom_level=15, output_dir=\"map_tiles\"):\n",
    "    \"\"\"Fetch map tiles from HERE API for image-based detection\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    image_paths = []\n",
    "    for i, (lng, lat) in enumerate(coordinates):\n",
    "        # Get tile column and row coordinates\n",
    "        tile_x, tile_y = latlng_to_tile(lng, lat, zoom_level)\n",
    "        \n",
    "        # Prepare the URL for the API request\n",
    "        url = f\"https://maps.hereapi.com/v3/base/mc/{zoom_level}/{tile_x}/{tile_y}/png?apiKey={api_key}\"\n",
    "        \n",
    "        # Call the API to get the tile image\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            file_path = os.path.join(output_dir, f'tile_{tile_x}_{tile_y}.png')\n",
    "            \n",
    "            # Save the image to the specified folder\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Image {i+1}/{len(coordinates)} saved at {file_path}\")\n",
    "            image_paths.append(file_path)\n",
    "        else:\n",
    "            print(f\"Failed to fetch tile for {lng}, {lat}. Status code: {response.status_code}\")\n",
    "    \n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======== Visualization Functions ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(features_df, probe_points=None, roundabout_data=None):\n",
    "    \"\"\"Visualize clustering results\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot all probe points if provided\n",
    "    if probe_points is not None:\n",
    "        plt.scatter(probe_points['longitude'], probe_points['latitude'], \n",
    "                   c='lightgray', s=1, alpha=0.3, label='Probe Data')\n",
    "    \n",
    "    # Plot detected roundabouts\n",
    "    detected = features_df[features_df['predicted_roundabout'] == 1]\n",
    "    plt.scatter(detected['longitude'], detected['latitude'], \n",
    "               c='red', s=50, marker='o', label='Detected Roundabouts')\n",
    "    \n",
    "    # Plot known roundabouts if provided\n",
    "    if roundabout_data is not None:\n",
    "        plt.scatter(roundabout_data['longitude'], roundabout_data['latitude'], \n",
    "                   c='blue', s=30, marker='x', label='Known Roundabouts')\n",
    "    \n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title('Roundabout Detection Results')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('roundabout_detection_results.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======== Main Function ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the roundabout detection pipeline\"\"\"\n",
    "    print(\"======== Roundabout Detection Pipeline ========\")\n",
    "    \n",
    "    # Setup paths\n",
    "    data_dir = \"data/probe_data\"\n",
    "    roundabout_csv = \"data/roundabouts.csv\"\n",
    "    \n",
    "    # Check if data exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Error: Data directory '{data_dir}' not found\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(roundabout_csv):\n",
    "        print(f\"Error: Roundabout data '{roundabout_csv}' not found\")\n",
    "        return\n",
    "    \n",
    "    # Load known roundabouts for validation\n",
    "    roundabout_data = pd.read_csv(roundabout_csv)\n",
    "    \n",
    "    # Method 1: Trajectory-based detection\n",
    "    cluster_features, probe_points = trajectory_based_detection(os.path.join(data_dir, \"0\"), roundabout_csv, num_files=5)\n",
    "    \n",
    "    # Method 2: Advanced trajectory analysis\n",
    "    advanced_features = advanced_trajectory_analysis(os.path.join(data_dir, \"0\"), roundabout_csv, num_files=5)\n",
    "    \n",
    "    # Method 3: K-means with optimal k selection\n",
    "    # Replace with your actual HERE API key\n",
    "    api_key = \"YOUR_HERE_API_KEY\"\n",
    "    \n",
    "    # Process batches of data\n",
    "    batch_results = []\n",
    "    for batch_id in range(3):  # Process first 3 batches for testing\n",
    "        kmeans_results, centroids = kmeans_detection(data_dir, roundabout_csv, batch_id=batch_id, api_key=api_key)\n",
    "        if kmeans_results is not None:\n",
    "            batch_results.append((kmeans_results, centroids, batch_id))\n",
    "    \n",
    "    # Visualize results\n",
    "    if cluster_features is not None:\n",
    "        visualize_clusters(cluster_features, probe_points, roundabout_data)\n",
    "    \n",
    "    if advanced_features is not None:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        detected = advanced_features[advanced_features['predicted_roundabout'] == 1]\n",
    "        plt.scatter(advanced_features['longitude'], advanced_features['latitude'], \n",
    "                   c='lightgray', s=1, alpha=0.3, label='Trajectories')\n",
    "        plt.scatter(detected['longitude'], detected['latitude'], \n",
    "                   c='red', s=50, marker='o', label='Detected Roundabouts')\n",
    "        plt.scatter(roundabout_data['longitude'], roundabout_data['latitude'], \n",
    "                   c='blue', s=30, marker='x', label='Known Roundabouts')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.title('Advanced Roundabout Detection Results')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('advanced_roundabout_detection_results.png')\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n======== Roundabout Detection Complete ========\")\n",
    "    \n",
    "    # For image-based detection \n",
    "    # Uncomment and set paths to your dataset\n",
    "    \"\"\"\n",
    "    # Method 4: Image-based CNN detection\n",
    "    print(\"\\n=== Image-Based Detection ===\")\n",
    "    \n",
    "    # Set paths to your dataset\n",
    "    train_dir = \"data_chicago_hackathon_2024/cnn_model/datasets/train\"\n",
    "    test_dir = \"data_chicago_hackathon_2024/cnn_model/datasets/val\"\n",
    "    \n",
    "    if os.path.exists(train_dir) and os.path.exists(test_dir):\n",
    "        # Train the CNN model\n",
    "        cnn_model, history = train_roundabout_cnn(train_dir, test_dir, epochs=5)\n",
    "        \n",
    "        # Save the model\n",
    "        cnn_model.save('roundabout_detection_model.h5')\n",
    "        \n",
    "        # Example prediction on a sample image\n",
    "        if cluster_features is not None:\n",
    "            detected = cluster_features[cluster_features['predicted_roundabout'] == 1]\n",
    "            if len(detected) > 0:\n",
    "                coordinates = list(zip(detected['longitude'], detected['latitude']))\n",
    "                image_paths = fetch_map_tiles(coordinates[:5], api_key, zoom_level=15)\n",
    "                \n",
    "                for image_path in image_paths:\n",
    "                    prediction, result = predict_with_threshold(cnn_model, image_path)\n",
    "                    print(f\"Image: {image_path}\")\n",
    "                    print(f\"Prediction: {prediction:.4f}, Result: {result}\")\n",
    "    else:\n",
    "        print(\"Image dataset not found. Skipping CNN training.\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Roundabout Detection Pipeline ========\n",
      "\n",
      "=== Starting Trajectory-Based Detection ===\n",
      "Loading 5 probe data files...\n",
      "Attempting parallel processing with 6 workers\n",
      "Parallel processing failed: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Falling back to sequential processing...\n",
      "Processing probe_2024_07_08_00_00_00.csv\n",
      "Processing probe_2024_07_08_06_00_00.csv\n",
      "Processing probe_2024_07_08_12_00_00.csv\n",
      "Processing probe_2024_07_08_18_00_00.csv\n",
      "Processing probe_2024_07_09_00_00_00.csv\n",
      "Processed 22475 data points in 16.26 seconds\n",
      "Loaded 15 known roundabouts\n",
      "Performing DBSCAN clustering...\n",
      "Found 9 clusters\n",
      "Extracted features for 7 clusters\n",
      "\n",
      "Model Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00         3\n",
      "   macro avg       1.00      1.00      1.00         3\n",
      "weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "\n",
      "Feature Importance:\n",
      "         feature  importance\n",
      "0    point_count         0.0\n",
      "1  mean_distance         0.0\n",
      "2   std_distance         0.0\n",
      "3   max_distance         0.0\n",
      "4    circularity         0.0\n",
      "\n",
      "Detected 0 potential roundabouts\n",
      "\n",
      "=== Starting Advanced Trajectory Analysis ===\n",
      "Loading 5 probe data files...\n",
      "Loaded 15 known roundabouts\n",
      "Initial probe data shape: (1312832, 7)\n",
      "Probe data shape after filtering: (1239506, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jsh Agarwal\\AppData\\Local\\Temp\\ipykernel_21248\\353439911.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  features['heading_change'] = grouped.apply(calculate_heading_change)\n",
      "d:\\HERE\\Actual\\venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1307: RuntimeWarning: invalid value encountered in divide\n",
      "  a = -(dx2)/(dx1 * (dx1 + dx2))\n",
      "d:\\HERE\\Actual\\venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1308: RuntimeWarning: divide by zero encountered in divide\n",
      "  b = (dx2 - dx1) / (dx1 * dx2)\n",
      "d:\\HERE\\Actual\\venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1308: RuntimeWarning: invalid value encountered in divide\n",
      "  b = (dx2 - dx1) / (dx1 * dx2)\n",
      "d:\\HERE\\Actual\\venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1309: RuntimeWarning: divide by zero encountered in divide\n",
      "  c = dx1 / (dx2 * (dx1 + dx2))\n",
      "d:\\HERE\\Actual\\venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1309: RuntimeWarning: invalid value encountered in divide\n",
      "  c = dx1 / (dx2 * (dx1 + dx2))\n",
      "d:\\HERE\\Actual\\venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1315: RuntimeWarning: invalid value encountered in multiply\n",
      "  out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] + c * f[tuple(slice4)]\n",
      "d:\\HERE\\Actual\\venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1331: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n\n",
      "d:\\HERE\\Actual\\venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1307: RuntimeWarning: divide by zero encountered in divide\n",
      "  a = -(dx2)/(dx1 * (dx1 + dx2))\n",
      "d:\\HERE\\Actual\\venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1315: RuntimeWarning: invalid value encountered in add\n",
      "  out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] + c * f[tuple(slice4)]\n",
      "C:\\Users\\Jsh Agarwal\\AppData\\Local\\Temp\\ipykernel_21248\\1641750859.py:12: RuntimeWarning: invalid value encountered in divide\n",
      "  curvature = np.abs(dx * d2y - dy * d2x) / (dx * dx + dy * dy)**1.5\n",
      "C:\\Users\\Jsh Agarwal\\AppData\\Local\\Temp\\ipykernel_21248\\1641750859.py:12: RuntimeWarning: invalid value encountered in multiply\n",
      "  curvature = np.abs(dx * d2y - dy * d2x) / (dx * dx + dy * dy)**1.5\n",
      "C:\\Users\\Jsh Agarwal\\AppData\\Local\\Temp\\ipykernel_21248\\1641750859.py:12: RuntimeWarning: invalid value encountered in subtract\n",
      "  curvature = np.abs(dx * d2y - dy * d2x) / (dx * dx + dy * dy)**1.5\n",
      "d:\\HERE\\Actual\\venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1324: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_0\n",
      "d:\\HERE\\Actual\\venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1303: RuntimeWarning: invalid value encountered in divide\n",
      "  out[tuple(slice1)] = (f[tuple(slice4)] - f[tuple(slice2)]) / (2. * ax_dx)\n",
      "C:\\Users\\Jsh Agarwal\\AppData\\Local\\Temp\\ipykernel_21248\\353439911.py:57: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  features['curvature'] = grouped.apply(calculate_curvature)\n",
      "C:\\Users\\Jsh Agarwal\\AppData\\Local\\Temp\\ipykernel_21248\\353439911.py:61: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  features['distance'] = grouped.apply(lambda g: np.sum(np.sqrt(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape after processing: (0, 8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 5)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     22\u001b[39m cluster_features, probe_points = trajectory_based_detection(os.path.join(data_dir, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m), roundabout_csv, num_files=\u001b[32m5\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Method 2: Advanced trajectory analysis\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m advanced_features = \u001b[43madvanced_trajectory_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroundabout_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_files\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Method 3: K-means with optimal k selection\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Replace with your actual HERE API key\u001b[39;00m\n\u001b[32m     29\u001b[39m api_key = \u001b[33m\"\u001b[39m\u001b[33mYOUR_HERE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36madvanced_trajectory_analysis\u001b[39m\u001b[34m(data_dir, known_roundabouts_path, num_files)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Normalize features\u001b[39;00m\n\u001b[32m     70\u001b[39m scaler = StandardScaler()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m scaled_features = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraceid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlongitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Apply HDBSCAN clustering\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scaled_features.shape[\u001b[32m0\u001b[39m] > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HERE\\Actual\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HERE\\Actual\\venv\\Lib\\site-packages\\sklearn\\base.py:918\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    903\u001b[39m         warnings.warn(\n\u001b[32m    904\u001b[39m             (\n\u001b[32m    905\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    914\u001b[39m         )\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    920\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HERE\\Actual\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HERE\\Actual\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HERE\\Actual\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:930\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[32m    899\u001b[39m \n\u001b[32m    900\u001b[39m \u001b[33;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    927\u001b[39m \u001b[33;03m    Fitted scaler.\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    929\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    938\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HERE\\Actual\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HERE\\Actual\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1130\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1128\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1133\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1134\u001b[39m         )\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1137\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 5)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
