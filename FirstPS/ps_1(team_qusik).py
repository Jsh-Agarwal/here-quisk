# -*- coding: utf-8 -*-
"""PS-1(Team-QUSIK)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RxopOtWVeEoeUWN0nq_xlCv7mfGeAIND
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install geopandas

import geopandas as gpd

# Path to your .shp file
shapefile_path = "/content/Singapore_Prime_LAT.shx"

# Load shapefile
gdf = gpd.read_file(shapefile_path)

# Display the first few rows
print(gdf.head())

# Convert geometry to WKT string format
gdf['geometry'] = gdf['geometry'].apply(lambda geom: geom.wkt)

# Save to CSV
gdf.to_csv("output_poi.csv", index=False)

# Commented out IPython magic to ensure Python compatibility.
# %pip install simpledbf

from simpledbf import Dbf5

# Load your .dbf file
dbf = Dbf5("/content/Singapore_Prime_LAT.dbf")

# Convert to pandas DataFrame
df = dbf.to_dataframe()

# Save to CSV
df.to_csv("output_attributes.csv", index=False)

# Commented out IPython magic to ensure Python compatibility.
# %pip install dbfread

from dbfread import DBF
import pandas as pd

# Try with Latin-1 (common for DBF files); you can try 'cp1252' if this fails
table = DBF("/content/Singapore_Prime_LAT.dbf", encoding='latin-1')

# Convert to DataFrame
df = pd.DataFrame(iter(table))

# Save to CSV
df.to_csv("output.csv", index=False)

import geopandas as gpd
import pandas as pd

# 1. Load your POI CSV ---------------------------------------------------------
poi = pd.read_csv("/content/output.csv")

# 2. Rename obvious fields -----------------------------------------------------
poi = poi.rename(columns={
    "OBJECTID":   "POI_ID",
    "DISPLAY_LA": "y",
    "DISPLAY_LO": "x"
})

# 3. Build geometry column -----------------------------------------------------
gdf = gpd.GeoDataFrame(
    poi,
    geometry=gpd.points_from_xy(poi["x"], poi["y"]),
    crs="EPSG:4326"        # WGS-84 lat/lon
)

# 4. Attach LINK_ID ------------------------------------------------------------
streets = gpd.read_file("STREETS_NAV/my_tile.geojson")[["link_id", "geometry"]]

# spatial join ‚Äì finds nearest link for each POI
joined = gpd.sjoin_nearest(gdf, streets, how="left", distance_col="distance_m")
joined = joined.rename(columns={"link_id": "LINK_ID"})

# 5. Compute PERCFRREF ---------------------------------------------------------
def percent_along(row):
    link_geom = streets.loc[streets["link_id"] == row["LINK_ID"], "geometry"].values[0]
    proj_dist = link_geom.project(row.geometry)
    return round(100 * proj_dist / link_geom.length, 1)

joined["PERCFRREF"] = joined.apply(percent_along, axis=1)

# 6. Compute POI_ST_SD (side-of-street) ---------------------------------------
def side_of_street(row):
    link_geom = streets.loc[streets["link_id"] == row["LINK_ID"], "geometry"].values[0]
    # buffer 5 m on each side, test which buffer contains the POI
    left  = link_geom.parallel_offset(5,  "left")
    right = link_geom.parallel_offset(5,  "right")
    if row.geometry.within(right): return 1   # right
    if row.geometry.within(left):  return 2   # left
    return 0                                   # unknown

joined["POI_ST_SD"] = joined.apply(side_of_street, axis=1)

# 7. Save in the format the pipeline expects -----------------------------------
joined[["POI_ID", "LINK_ID", "PERCFRREF", "POI_ST_SD",
        "x", "y", "STREET_NAM", "NT_CITY", "STATE"]
       ].to_csv("POIs/pois_ready.csv", index=False)

import math
import requests
from typing import List, Tuple, Set
import time

def latlon_to_here_tile_id_quad_corrected(lat, lon, level=11):
    """
    Corrected quad-tree method that found the valid tile 30558.
    This appears to be closer to HERE's actual system.
    """
    # Normalize coordinates to [0,1] range
    norm_lon = (lon + 180.0) / 360.0
    norm_lat = (lat + 90.0) / 180.0

    # Convert to tile coordinates
    scale = 2 ** level
    tile_x = int(norm_lon * scale)
    tile_y = int(norm_lat * scale)

    # Ensure bounds
    tile_x = min(max(tile_x, 0), scale - 1)
    tile_y = min(max(tile_y, 0), scale - 1)

    # Different quad-tree encoding - this one worked!
    tile_id = 0
    for i in range(level):
        bit_pos = level - 1 - i
        if tile_x & (1 << bit_pos):
            tile_id |= (1 << (2 * i + 1))
        if tile_y & (1 << bit_pos):
            tile_id |= (1 << (2 * i))

    return tile_id

def validate_tile_with_here_api(tile_id, api_key, delay=0.1):
    """
    Validate a tile ID by testing it with HERE API with rate limiting.
    """
    time.sleep(delay)  # Rate limiting

    test_url = (f"https://smap.hereapi.com/v8/maps/attributes"
                f"?in=tile:{tile_id}"
                f"&layers=ROAD_GEOM_FC1"
                f"&format=geojson"
                f"&apikey={api_key}")

    try:
        response = requests.get(test_url, timeout=10)
        if response.status_code == 200:
            return True, "Valid"
        elif response.status_code == 400:
            return False, "Invalid tile ID"
        else:
            return False, f"HTTP {response.status_code}"
    except Exception as e:
        return False, str(e)

def find_neighboring_tiles(center_tile_id: int, radius: int = 50) -> List[int]:
    """
    Find tiles in the neighborhood of a known valid tile.
    Since we know 30558 is valid, we'll search around it.
    """
    neighbors = []

    # Try tiles in a range around the known valid tile
    for offset in range(-radius, radius + 1):
        neighbors.append(center_tile_id + offset)

    # Also try some systematic patterns that might work for HERE's system
    for i in range(1, 10):
        neighbors.extend([
            center_tile_id + i * 1024,      # Different row
            center_tile_id - i * 1024,      # Different row
            center_tile_id + i * 2048,      # Different block
            center_tile_id - i * 2048,      # Different block
            center_tile_id + i,             # Adjacent
            center_tile_id - i,             # Adjacent
        ])

    return list(set(neighbors))  # Remove duplicates

def systematic_tile_search(api_key: str, max_tiles: int = 20) -> List[int]:
    """
    Systematically search for valid tiles using patterns based on the known valid tile 30558.
    """
    known_valid = 30558
    valid_tiles = [known_valid]

    print(f"üîç Starting systematic search around known valid tile {known_valid}...")

    # Pattern 1: Sequential search around known tile
    print("\nüìç Pattern 1: Sequential search...")
    candidates = find_neighboring_tiles(known_valid, radius=100)

    tested = 0
    for tile_id in candidates:
        if tile_id <= 0:  # Skip negative or zero tiles
            continue

        is_valid, result = validate_tile_with_here_api(tile_id, api_key)
        tested += 1

        if is_valid:
            valid_tiles.append(tile_id)
            print(f"  ‚úÖ Found valid tile: {tile_id}")

            if len(valid_tiles) >= max_tiles:
                break
        else:
            if tested % 20 == 0:
                print(f"  üîÑ Tested {tested} tiles, found {len(valid_tiles)} valid")

    print(f"  üìä Pattern 1 results: {len(valid_tiles)} valid tiles")

    # Pattern 2: Grid-based search around Singapore coordinates
    if len(valid_tiles) < max_tiles:
        print("\nüìç Pattern 2: Singapore coordinate grid...")
        singapore_tiles = generate_singapore_grid_tiles(api_key, max_tiles - len(valid_tiles))
        valid_tiles.extend(singapore_tiles)

    # Pattern 3: Bit pattern analysis
    if len(valid_tiles) < max_tiles:
        print(f"\nüìç Pattern 3: Bit pattern analysis of {known_valid}...")
        pattern_tiles = analyze_bit_patterns(known_valid, api_key, max_tiles - len(valid_tiles))
        valid_tiles.extend(pattern_tiles)

    return sorted(list(set(valid_tiles)))  # Remove duplicates and sort

def generate_singapore_grid_tiles(api_key: str, max_tiles: int) -> List[int]:
    """
    Generate tiles using a fine grid over Singapore and test them.
    """
    valid_tiles = []

    # Singapore bounds with higher resolution
    lat_range = (1.15, 1.55)  # Slightly expanded
    lon_range = (103.5, 104.2)  # Slightly expanded

    # Create a fine grid
    lat_steps = 20
    lon_steps = 20

    lat_step = (lat_range[1] - lat_range[0]) / lat_steps
    lon_step = (lon_range[1] - lon_range[0]) / lon_steps

    tested = 0
    for i in range(lat_steps + 1):
        for j in range(lon_steps + 1):
            lat = lat_range[0] + i * lat_step
            lon = lon_range[0] + j * lon_step

            tile_id = latlon_to_here_tile_id_quad_corrected(lat, lon, level=11)

            if tile_id > 0:  # Valid tile ID
                is_valid, _ = validate_tile_with_here_api(tile_id, api_key)
                tested += 1

                if is_valid and tile_id not in valid_tiles:
                    valid_tiles.append(tile_id)
                    print(f"  ‚úÖ Grid tile {tile_id} at ({lat:.3f}, {lon:.3f})")

                    if len(valid_tiles) >= max_tiles:
                        break

                if tested % 50 == 0:
                    print(f"  üîÑ Grid tested {tested}, found {len(valid_tiles)} valid")

        if len(valid_tiles) >= max_tiles:
            break

    return valid_tiles

def analyze_bit_patterns(known_tile: int, api_key: str, max_tiles: int) -> List[int]:
    """
    Analyze the bit pattern of the known valid tile to find similar tiles.
    """
    valid_tiles = []

    print(f"  üî¨ Analyzing tile {known_tile} (binary: {bin(known_tile)})")

    # Try flipping individual bits
    for bit_pos in range(20):  # Level 11 tiles shouldn't need more than 20 bits
        for flip_val in [0, 1]:
            if flip_val == 0:
                test_tile = known_tile & ~(1 << bit_pos)  # Clear bit
            else:
                test_tile = known_tile | (1 << bit_pos)   # Set bit

            if test_tile != known_tile and test_tile > 0:
                is_valid, _ = validate_tile_with_here_api(test_tile, api_key)

                if is_valid and test_tile not in valid_tiles:
                    valid_tiles.append(test_tile)
                    print(f"  ‚úÖ Bit pattern tile: {test_tile}")

                    if len(valid_tiles) >= max_tiles:
                        break

        if len(valid_tiles) >= max_tiles:
            break

    return valid_tiles

def main():
    API_KEY = "D1h_2_LLo1C-VaUmIlgljOhlzQqkDkVBr3L5F3LjZOo"

    print("üá∏üá¨ Enhanced Singapore Tile Search")
    print("=" * 50)

    # First, verify our known valid tile still works
    print("üîç Verifying known valid tile 30558...")
    is_valid, result = validate_tile_with_here_api(30558, API_KEY)

    if not is_valid:
        print(f"‚ùå Known tile 30558 is no longer valid: {result}")
        print("This might indicate API changes or access issues.")
        return

    print("‚úÖ Tile 30558 confirmed valid")

    # Find more valid tiles
    print(f"\nüöÄ Searching for more valid tiles...")
    valid_tiles = systematic_tile_search(API_KEY, max_tiles=15)

    print(f"\n" + "=" * 50)
    print(f"üéâ FINAL RESULTS: Found {len(valid_tiles)} valid tiles")
    print(f"üìã Valid tiles: {valid_tiles}")

    print(f"\nüìù Use this in your main script:")
    print(f"TILES = {valid_tiles}")

    # Test a few tiles to show they work
    print(f"\nüß™ Final validation of first 5 tiles:")
    for tile_id in valid_tiles[:5]:
        is_valid, result = validate_tile_with_here_api(tile_id, API_KEY)
        status = "‚úÖ" if is_valid else "‚ùå"
        print(f"  {status} Tile {tile_id}: {result}")

    if len(valid_tiles) >= 10:
        print(f"\nüéØ SUCCESS! Found {len(valid_tiles)} tiles for your Singapore analysis.")
    else:
        print(f"\n‚ö†Ô∏è  Found only {len(valid_tiles)} tiles. You might need to:")
        print("   1. Check if your API key has full Level-11 access")
        print("   2. Try Level-10 tiles instead (fewer, larger tiles)")
        print("   3. Contact HERE support for Singapore tile access")

if __name__ == "__main__":
    main()

#!/usr/bin/env python
"""
Download ROAD_GEOM_FC1 + ROAD_NAMING_FC1 + POIs for Singapore Level-11 tiles
and write them to the three folders GeoFix295 expects.

üê±‚Äçüë§  WARNING: Keep real API keys out of public repos.
üá∏üá¨  Updated with correct Singapore tile IDs from tile search results.
"""
import os
import requests
import json
import pandas as pd
import geopandas as gpd
from pathlib import Path
from shapely.geometry import Point
from tqdm import tqdm
import logging
from typing import List, Tuple, Optional
import time

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration
API_KEY = "D1h_2_LLo1C-VaUmIlgljOhlzQqkDkVBr3L5F3LjZOo"  # ‚Üê Replace with your key

# Updated Singapore tiles from your search results (first 10 tiles)
SINGAPORE_TILES = [
    12126, 14174, 20318, 21342, 23390,
    26462, 27486, 28510, 29534, 30558
]

# Directory structure
BASE_DIR = Path.cwd()
DIR_NAV = BASE_DIR / "STREETS_NAV"
DIR_NAMING = BASE_DIR / "STREETS_NAMING_ADDRESSING"
DIR_POI = BASE_DIR / "POIs"

# Create directories
for directory in (DIR_NAV, DIR_NAMING, DIR_POI):
    directory.mkdir(exist_ok=True)
    logger.info(f"Created/verified directory: {directory}")

class GeoDataDownloader:
    """Handles downloading and processing of Singapore geodata."""

    def __init__(self, api_key: str, max_retries: int = 3, retry_delay: float = 1.0):
        self.api_key = api_key
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Singapore-GeoData-Downloader/1.0'})

    def download_with_retry(self, url: str, output_path: Path) -> bool:
        """Download file with retry logic and proper error handling."""
        if output_path.exists():
            logger.info(f"File already exists, skipping: {output_path.name}")
            return True

        for attempt in range(self.max_retries):
            try:
                logger.info(f"Downloading: {output_path.name} (attempt {attempt + 1})")
                response = self.session.get(url, timeout=60)
                response.raise_for_status()

                # Validate response content
                if len(response.content) == 0:
                    raise ValueError("Empty response received")

                output_path.write_bytes(response.content)
                logger.info(f"Successfully downloaded: {output_path.name}")
                return True

            except (requests.RequestException, ValueError) as e:
                logger.warning(f"Download failed (attempt {attempt + 1}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay * (2 ** attempt))  # Exponential backoff
                else:
                    logger.error(f"Failed to download after {self.max_retries} attempts: {url}")
                    return False

        return False

    def download_road_geometry(self, tile_id: int) -> bool:
        """Download ROAD_GEOM_FC1 data for a tile."""
        url = (f"https://smap.hereapi.com/v8/maps/attributes"
               f"?in=tile:{tile_id}"
               f"&layers=ROAD_GEOM_FC1"
               f"&format=geojson"
               f"&apikey={self.api_key}")

        output_path = DIR_NAV / f"nav_{tile_id}.geojson"
        return self.download_with_retry(url, output_path)

    def download_road_naming(self, tile_id: int) -> bool:
        """Download ROAD_NAMING_FC1 data for a tile."""
        url = (f"https://smap.hereapi.com/v8/maps/attributes"
               f"?in=tile:{tile_id}"
               f"&layers=ROAD_NAMING_FC1"
               f"&format=geojson"
               f"&apikey={self.api_key}")

        output_path = DIR_NAMING / f"naming_{tile_id}.geojson"
        return self.download_with_retry(url, output_path)

    def calculate_poi_attributes(self, pois_gdf: gpd.GeoDataFrame,
                               roads_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
        """Efficiently calculate POI attributes using vectorized operations."""
        if pois_gdf.empty or roads_gdf.empty:
            return pois_gdf

        logger.info("Calculating POI-to-road relationships...")

        # Perform spatial join to find nearest roads
        joined = gpd.sjoin_nearest(
            pois_gdf,
            roads_gdf[["LINK_ID", "geometry"]],
            how="left",
            distance_col="dist_m"
        )

        # Calculate percentage along road and side of street
        def calculate_road_attributes(row):
            if pd.isna(row.LINK_ID):
                return 0.0, 0

            try:
                # Get the road geometry
                road_geom = roads_gdf.loc[roads_gdf.LINK_ID == row.LINK_ID, "geometry"].iloc[0]
                poi_point = row.geometry

                # Calculate percentage along road
                percent_along = round(100 * road_geom.project(poi_point) / road_geom.length, 3)

                # Calculate side of street (simplified approach)
                try:
                    right_offset = road_geom.parallel_offset(5, "right")
                    left_offset = road_geom.parallel_offset(5, "left")

                    if poi_point.within(right_offset.buffer(1)):
                        side = 1  # Right side
                    elif poi_point.within(left_offset.buffer(1)):
                        side = 2  # Left side
                    else:
                        side = 0  # Unknown
                except:
                    side = 0

                return percent_along, side

            except Exception as e:
                logger.warning(f"Error calculating road attributes: {e}")
                return 0.0, 0

        # Apply calculations
        tqdm.pandas(desc="Processing POI attributes")
        road_attrs = joined.progress_apply(calculate_road_attributes, axis=1, result_type='expand')
        joined["PERCFRREF"] = road_attrs[0]
        joined["POI_ST_SD"] = road_attrs[1]

        return joined

    def download_and_process_pois(self, tile_id: int) -> bool:
        """Download and process POIs for a tile."""
        url = (f"https://places.ls.hereapi.com/places/v1/places/tile"
               f"?tile_id={tile_id}&level=11&apiKey={self.api_key}")

        output_path = DIR_POI / f"pois_{tile_id}.csv"

        if output_path.exists():
            logger.info(f"POI file already exists, skipping: {output_path.name}")
            return True

        try:
            logger.info(f"Downloading POIs for tile {tile_id}")
            response = self.session.get(url, timeout=60)
            response.raise_for_status()

            data = response.json()
            items = data.get("results", [{}])[0].get("items", [])

            if not items:
                logger.info(f"No POIs found for tile {tile_id}, creating empty file")
                output_path.touch()
                return True

            # Create POI DataFrame
            poi_data = []
            for item in items:
                if "position" in item and len(item["position"]) >= 2:
                    poi_data.append({
                        "POI_ID": item.get("id", ""),
                        "lat": item["position"][0],
                        "lon": item["position"][1]
                    })

            if not poi_data:
                logger.info(f"No valid POIs found for tile {tile_id}")
                output_path.touch()
                return True

            # Create GeoDataFrame
            df = pd.DataFrame(poi_data)
            pois_gdf = gpd.GeoDataFrame(
                df,
                geometry=gpd.points_from_xy(df.lon, df.lat),
                crs="EPSG:4326"
            )

            # Load road geometry for spatial calculations
            nav_file = DIR_NAV / f"nav_{tile_id}.geojson"
            if nav_file.exists():
                try:
                    roads_gdf = gpd.read_file(nav_file)
                    if not roads_gdf.empty and "LINK_ID" in roads_gdf.columns:
                        pois_processed = self.calculate_poi_attributes(pois_gdf, roads_gdf)

                        # Save processed POIs
                        columns_to_save = ["POI_ID", "LINK_ID", "PERCFRREF", "POI_ST_SD"]
                        available_columns = [col for col in columns_to_save if col in pois_processed.columns]
                        pois_processed[available_columns].to_csv(output_path, index=False)
                    else:
                        logger.warning(f"Invalid road data for tile {tile_id}, saving basic POI data")
                        df.to_csv(output_path, index=False)
                except Exception as e:
                    logger.error(f"Error processing POIs for tile {tile_id}: {e}")
                    df.to_csv(output_path, index=False)
            else:
                logger.warning(f"No road geometry found for tile {tile_id}, saving basic POI data")
                df.to_csv(output_path, index=False)

            logger.info(f"Successfully processed {len(poi_data)} POIs for tile {tile_id}")
            return True

        except Exception as e:
            logger.error(f"Failed to download/process POIs for tile {tile_id}: {e}")
            return False

def main():
    """Main execution function."""
    logger.info("üá∏üá¨ Starting Singapore GeoData Download")
    logger.info(f"Processing {len(SINGAPORE_TILES)} tiles: {SINGAPORE_TILES}")

    downloader = GeoDataDownloader(API_KEY)

    success_count = 0
    failed_tiles = []

    # Process each tile
    for tile_id in tqdm(SINGAPORE_TILES, desc="Processing tiles"):
        logger.info(f"\nüìç Processing tile {tile_id}")

        tile_success = True

        # Download road geometry
        if not downloader.download_road_geometry(tile_id):
            logger.error(f"Failed to download road geometry for tile {tile_id}")
            tile_success = False

        # Download road naming
        if not downloader.download_road_naming(tile_id):
            logger.error(f"Failed to download road naming for tile {tile_id}")
            tile_success = False

        # Download and process POIs
        if not downloader.download_and_process_pois(tile_id):
            logger.error(f"Failed to download POIs for tile {tile_id}")
            tile_success = False

        if tile_success:
            success_count += 1
            logger.info(f"‚úÖ Successfully processed tile {tile_id}")
        else:
            failed_tiles.append(tile_id)
            logger.error(f"‚ùå Failed to process tile {tile_id}")

    # Summary
    logger.info(f"\nüìä DOWNLOAD SUMMARY")
    logger.info(f"Successfully processed: {success_count}/{len(SINGAPORE_TILES)} tiles")

    if failed_tiles:
        logger.error(f"Failed tiles: {failed_tiles}")

    if success_count > 0:
        logger.info(f"\n‚úÖ Downloads finished! Files saved to:")
        logger.info(f"   üìÅ Navigation: {DIR_NAV}")
        logger.info(f"   üìÅ Naming: {DIR_NAMING}")
        logger.info(f"   üìÅ POIs: {DIR_POI}")
        logger.info(f"\nüöÄ Run GeoFix295 with:")
        logger.info(f"   python Main.py --mode test   # quick check")
        logger.info(f"   python Main.py --mode full   # all tiles")
    else:
        logger.error("‚ùå No tiles were successfully processed!")

if __name__ == "__main__":
    main()

#!/usr/bin/env python
"""
Download ROAD_GEOM_FC1 + ROAD_NAMING_FC1 + POIs for Singapore Level-11 tiles
and write them to the three folders GeoFix295 expects.

üê±‚Äçüë§  WARNING: Keep real API keys out of public repos.
üá∏üá¨  Updated with correct Singapore tile IDs and fixed API endpoints.
"""
import os
import requests
import json
import pandas as pd
import geopandas as gpd
from pathlib import Path
from shapely.geometry import Point
from tqdm import tqdm
import logging
from typing import List, Tuple, Optional
import time

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration
API_KEY = "D1h_2_LLo1C-VaUmIlgljOhlzQqkDkVBr3L5F3LjZOo"  # ‚Üê Replace with your key

# Updated Singapore tiles from your search results (first 10 tiles)
SINGAPORE_TILES = [
    12126, 14174, 20318, 21342, 23390,
    26462, 27486, 28510, 29534, 30558
]

# Directory structure - Choose one of these options:

# Option 1: Use current directory (default)
BASE_DIR = Path.cwd()

# Option 2: Use a specific path
# BASE_DIR = Path("C:/Users/YourName/GeoData")  # Windows
# BASE_DIR = Path("/home/username/geodata")     # Linux
# BASE_DIR = Path.home() / "geodata"            # User's home directory

# Option 3: Use script's directory (recommended if running from different locations)
# BASE_DIR = Path(__file__).parent

DIR_NAV = BASE_DIR / "STREETS_NAV"
DIR_NAMING = BASE_DIR / "STREETS_NAMING_ADDRESSING"
DIR_POI = BASE_DIR / "POIs"

# Create directories
for directory in (DIR_NAV, DIR_NAMING, DIR_POI):
    directory.mkdir(exist_ok=True)
    logger.info(f"Created/verified directory: {directory}")

class GeoDataDownloader:
    """Handles downloading and processing of Singapore geodata."""

    def __init__(self, api_key: str, max_retries: int = 3, retry_delay: float = 1.0):
        self.api_key = api_key
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Singapore-GeoData-Downloader/1.0'})

        # Test API access on initialization
        self._test_api_access()

    def _test_api_access(self):
        """Test API access and available services."""
        logger.info("üîç Testing API access...")

        # Test a simple tile request
        test_url = (f"https://smap.hereapi.com/v8/maps/attributes"
                   f"?in=tile:{SINGAPORE_TILES[0]}"
                   f"&layers=ROAD_GEOM_FC1"
                   f"&format=geojson"
                   f"&apikey={self.api_key}")

        try:
            response = self.session.get(test_url, timeout=30)
            if response.status_code == 200:
                logger.info("‚úÖ Map Attributes API access confirmed")
            else:
                logger.warning(f"‚ö†Ô∏è  Map Attributes API returned {response.status_code}")
                logger.warning(f"Response: {response.text[:200]}")
        except Exception as e:
            logger.error(f"‚ùå Map Attributes API test failed: {e}")

        # Test Places API
        places_url = f"https://places.ls.hereapi.com/places/v1/places/tile?tile_id={SINGAPORE_TILES[0]}&level=11&apiKey={self.api_key}"
        try:
            response = self.session.get(places_url, timeout=30)
            if response.status_code == 200:
                logger.info("‚úÖ Places API access confirmed")
            elif response.status_code == 403:
                logger.warning("‚ö†Ô∏è  Places API access denied (403) - check your API key permissions")
            else:
                logger.warning(f"‚ö†Ô∏è  Places API returned {response.status_code}")
        except Exception as e:
            logger.error(f"‚ùå Places API test failed: {e}")

    def get_detailed_error(self, response: requests.Response) -> str:
        """Get detailed error information from API response."""
        try:
            error_data = response.json()
            if 'error' in error_data:
                return f"{error_data['error'].get('message', 'Unknown error')}"
            elif 'title' in error_data:
                return f"{error_data.get('title', 'Unknown error')}"
        except:
            pass
        return f"HTTP {response.status_code}: {response.text[:200]}"

    def download_with_retry(self, url: str, output_path: Path, description: str = "") -> bool:
        """Download file with retry logic and proper error handling."""
        if output_path.exists():
            logger.info(f"File already exists, skipping: {output_path.name}")
            return True

        for attempt in range(self.max_retries):
            try:
                logger.info(f"Downloading {description}: {output_path.name} (attempt {attempt + 1})")
                response = self.session.get(url, timeout=60)

                if response.status_code == 200:
                    # Validate response content
                    if len(response.content) == 0:
                        raise ValueError("Empty response received")

                    output_path.write_bytes(response.content)
                    logger.info(f"‚úÖ Successfully downloaded: {output_path.name}")
                    return True
                else:
                    error_msg = self.get_detailed_error(response)
                    logger.warning(f"Download failed (attempt {attempt + 1}): {error_msg}")

            except (requests.RequestException, ValueError) as e:
                logger.warning(f"Download failed (attempt {attempt + 1}): {e}")

            if attempt < self.max_retries - 1:
                time.sleep(self.retry_delay * (2 ** attempt))  # Exponential backoff

        logger.error(f"‚ùå Failed to download after {self.max_retries} attempts: {description}")
        return False

    def download_road_geometry(self, tile_id: int) -> bool:
        """Download ROAD_GEOM_FC1 data for a tile."""
        url = (f"https://smap.hereapi.com/v8/maps/attributes"
               f"?in=tile:{tile_id}"
               f"&layers=ROAD_GEOM_FC1"
               f"&format=geojson"
               f"&apikey={self.api_key}")

        output_path = DIR_NAV / f"nav_{tile_id}.geojson"
        return self.download_with_retry(url, output_path, f"Road Geometry for tile {tile_id}")

    def download_road_naming(self, tile_id: int) -> bool:
        """Download road naming data for a tile - try multiple layer names."""
        # Try different possible layer names for road naming
        layer_names = [
            "ROAD_NAMING_FC1",
            "ROAD_NAMES_FC1",
            "STREET_NAMES_FC1",
            "NAMING_FC1"
        ]

        output_path = DIR_NAMING / f"naming_{tile_id}.geojson"

        if output_path.exists():
            logger.info(f"File already exists, skipping: {output_path.name}")
            return True

        for layer_name in layer_names:
            url = (f"https://smap.hereapi.com/v8/maps/attributes"
                   f"?in=tile:{tile_id}"
                   f"&layers={layer_name}"
                   f"&format=geojson"
                   f"&apikey={self.api_key}")

            logger.info(f"Trying layer: {layer_name} for tile {tile_id}")

            try:
                response = self.session.get(url, timeout=60)
                if response.status_code == 200 and len(response.content) > 0:
                    output_path.write_bytes(response.content)
                    logger.info(f"‚úÖ Successfully downloaded road naming with layer {layer_name}: {output_path.name}")
                    return True
                else:
                    error_msg = self.get_detailed_error(response)
                    logger.warning(f"Layer {layer_name} failed: {error_msg}")

            except Exception as e:
                logger.warning(f"Layer {layer_name} failed: {e}")

        # If all layers fail, create an empty file to continue processing
        logger.warning(f"‚ö†Ô∏è  All road naming layers failed for tile {tile_id}, creating empty file")
        output_path.write_text('{"type":"FeatureCollection","features":[]}')
        return True  # Don't fail the entire tile for naming issues

    def calculate_poi_attributes(self, pois_gdf: gpd.GeoDataFrame,
                               roads_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
        """Efficiently calculate POI attributes using vectorized operations."""
        if pois_gdf.empty or roads_gdf.empty:
            logger.warning("Empty POI or road data, skipping attribute calculation")
            return pois_gdf

        logger.info("Calculating POI-to-road relationships...")

        try:
            # Perform spatial join to find nearest roads
            joined = gpd.sjoin_nearest(
                pois_gdf,
                roads_gdf[["LINK_ID", "geometry"]],
                how="left",
                distance_col="dist_m"
            )

            # Calculate percentage along road and side of street
            def calculate_road_attributes(row):
                if pd.isna(row.LINK_ID):
                    return 0.0, 0

                try:
                    # Get the road geometry
                    road_matches = roads_gdf.loc[roads_gdf.LINK_ID == row.LINK_ID, "geometry"]
                    if road_matches.empty:
                        return 0.0, 0

                    road_geom = road_matches.iloc[0]
                    poi_point = row.geometry

                    # Calculate percentage along road
                    if road_geom.length > 0:
                        percent_along = round(100 * road_geom.project(poi_point) / road_geom.length, 3)
                    else:
                        percent_along = 0.0

                    # Calculate side of street (simplified approach)
                    try:
                        right_offset = road_geom.parallel_offset(5, "right")
                        left_offset = road_geom.parallel_offset(5, "left")

                        if poi_point.within(right_offset.buffer(1)):
                            side = 1  # Right side
                        elif poi_point.within(left_offset.buffer(1)):
                            side = 2  # Left side
                        else:
                            side = 0  # Unknown
                    except:
                        side = 0

                    return percent_along, side

                except Exception as e:
                    logger.debug(f"Error calculating road attributes: {e}")
                    return 0.0, 0

            # Apply calculations with progress bar
            tqdm.pandas(desc="Processing POI attributes")
            road_attrs = joined.progress_apply(calculate_road_attributes, axis=1, result_type='expand')
            joined["PERCFRREF"] = road_attrs[0]
            joined["POI_ST_SD"] = road_attrs[1]

            return joined

        except Exception as e:
            logger.error(f"Error in POI attribute calculation: {e}")
            return pois_gdf

    def download_and_process_pois(self, tile_id: int) -> bool:
        """Download and process POIs for a tile."""
        output_path = DIR_POI / f"pois_{tile_id}.csv"

        if output_path.exists():
            logger.info(f"POI file already exists, skipping: {output_path.name}")
            return True

        # Try multiple POI endpoints
        endpoints = [
            f"https://places.ls.hereapi.com/places/v1/places/tile?tile_id={tile_id}&level=11&apiKey={self.api_key}",
            f"https://places.api.here.com/places/v1/places/tile?tile_id={tile_id}&level=11&apiKey={self.api_key}",
        ]

        for endpoint in endpoints:
            try:
                logger.info(f"Trying POI endpoint for tile {tile_id}")
                response = self.session.get(endpoint, timeout=60)

                if response.status_code == 403:
                    logger.error("‚ùå Places API access denied - your API key doesn't have Places API permissions")
                    break
                elif response.status_code != 200:
                    error_msg = self.get_detailed_error(response)
                    logger.warning(f"POI endpoint failed: {error_msg}")
                    continue

                data = response.json()
                items = data.get("results", [{}])[0].get("items", [])

                if not items:
                    logger.info(f"No POIs found for tile {tile_id}, creating empty file")
                    output_path.touch()
                    return True

                # Create POI DataFrame
                poi_data = []
                for item in items:
                    if "position" in item and len(item["position"]) >= 2:
                        poi_data.append({
                            "POI_ID": item.get("id", ""),
                            "lat": item["position"][0],
                            "lon": item["position"][1]
                        })

                if not poi_data:
                    logger.info(f"No valid POIs found for tile {tile_id}")
                    output_path.touch()
                    return True

                # Create GeoDataFrame
                df = pd.DataFrame(poi_data)
                pois_gdf = gpd.GeoDataFrame(
                    df,
                    geometry=gpd.points_from_xy(df.lon, df.lat),
                    crs="EPSG:4326"
                )

                # Load road geometry for spatial calculations
                nav_file = DIR_NAV / f"nav_{tile_id}.geojson"
                if nav_file.exists():
                    try:
                        roads_gdf = gpd.read_file(nav_file)
                        if not roads_gdf.empty and "LINK_ID" in roads_gdf.columns:
                            pois_processed = self.calculate_poi_attributes(pois_gdf, roads_gdf)

                            # Save processed POIs
                            columns_to_save = ["POI_ID", "LINK_ID", "PERCFRREF", "POI_ST_SD"]
                            available_columns = [col for col in columns_to_save if col in pois_processed.columns]
                            pois_processed[available_columns].to_csv(output_path, index=False)
                        else:
                            logger.warning(f"Invalid road data for tile {tile_id}, saving basic POI data")
                            df.to_csv(output_path, index=False)
                    except Exception as e:
                        logger.error(f"Error processing POIs for tile {tile_id}: {e}")
                        df.to_csv(output_path, index=False)
                else:
                    logger.warning(f"No road geometry found for tile {tile_id}, saving basic POI data")
                    df.to_csv(output_path, index=False)

                logger.info(f"‚úÖ Successfully processed {len(poi_data)} POIs for tile {tile_id}")
                return True

            except Exception as e:
                logger.error(f"POI processing error for tile {tile_id}: {e}")
                continue

        # If all POI methods fail, create empty file to continue
        logger.warning(f"‚ö†Ô∏è  All POI methods failed for tile {tile_id}, creating empty file")
        output_path.touch()
        return True  # Don't fail entire process for POI issues

def main():
    """Main execution function."""
    logger.info("üá∏üá¨ Starting Singapore GeoData Download")
    logger.info(f"Processing {len(SINGAPORE_TILES)} tiles: {SINGAPORE_TILES}")

    downloader = GeoDataDownloader(API_KEY)

    success_count = 0
    failed_tiles = []
    partial_success_tiles = []

    # Process each tile
    for tile_id in tqdm(SINGAPORE_TILES, desc="Processing tiles"):
        logger.info(f"\nüìç Processing tile {tile_id}")

        nav_success = downloader.download_road_geometry(tile_id)
        naming_success = downloader.download_road_naming(tile_id)
        poi_success = downloader.download_and_process_pois(tile_id)

        if nav_success and naming_success and poi_success:
            success_count += 1
            logger.info(f"‚úÖ Fully processed tile {tile_id}")
        elif nav_success:  # At least navigation data succeeded
            partial_success_tiles.append(tile_id)
            logger.info(f"‚ö†Ô∏è  Partially processed tile {tile_id} (navigation only)")
        else:
            failed_tiles.append(tile_id)
            logger.error(f"‚ùå Failed to process tile {tile_id}")

    # Summary
    logger.info(f"\nüìä DOWNLOAD SUMMARY")
    logger.info(f"Fully successful: {success_count}/{len(SINGAPORE_TILES)} tiles")
    logger.info(f"Partially successful: {len(partial_success_tiles)} tiles")
    logger.info(f"Failed: {len(failed_tiles)} tiles")

    if failed_tiles:
        logger.error(f"Failed tiles: {failed_tiles}")
    if partial_success_tiles:
        logger.warning(f"Partial success tiles: {partial_success_tiles}")

    if success_count > 0 or partial_success_tiles:
        logger.info(f"\n‚úÖ Some data downloaded! Files saved to:")
        logger.info(f"   üìÅ Navigation: {DIR_NAV}")
        logger.info(f"   üìÅ Naming: {DIR_NAMING}")
        logger.info(f"   üìÅ POIs: {DIR_POI}")
        logger.info(f"\nüöÄ Try running GeoFix295 with available data:")
        logger.info(f"   python Main.py --mode test   # quick check")
        logger.info(f"   python Main.py --mode full   # all available tiles")
    else:
        logger.error("‚ùå No tiles were successfully processed!")
        logger.error("Check your API key permissions and network connection.")

if __name__ == "__main__":
    main()

!pip install pandas geopandas osmnx numpy shapely pyproj tqdm requests

#!/usr/bin/env python
"""
DIY Singapore GeoData Downloader using FREE OpenStreetMap data
Creates the same folder structure as HERE API version for GeoFix295 compatibility.

üÜì Uses OpenStreetMap data via OSMnx - completely free!
üá∏üá¨ Focused on Singapore with proper tile-based organization
üìÅ Generates same folder structure: STREETS_NAV, STREETS_NAMING_ADDRESSING, POIs
"""
import os
import pandas as pd
import geopandas as gpd
import osmnx as ox
import numpy as np
from pathlib import Path
from shapely.geometry import Point, Polygon, box
from shapely.ops import transform
import pyproj
from tqdm import tqdm
import logging
from typing import List, Tuple, Dict, Optional
import requests
import json
import warnings
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configure OSMnx
ox.config(use_cache=True, log_console=True)

# Singapore bounding box (approximate)
SINGAPORE_BOUNDS = {
    'north': 1.4504,
    'south': 1.2303,
    'east': 104.0117,
    'west': 103.5986
}

# Level 11 tile size approximation (for organizing data)
TILE_SIZE_DEGREES = 0.02  # Approximate tile size in degrees

# Directory structure - matches HERE API version
BASE_DIR = Path.cwd()
DIR_NAV = BASE_DIR / "STREETS_NAV"
DIR_NAMING = BASE_DIR / "STREETS_NAMING_ADDRESSING"
DIR_POI = BASE_DIR / "POIs"

# Create directories
for directory in (DIR_NAV, DIR_NAMING, DIR_POI):
    directory.mkdir(exist_ok=True)
    logger.info(f"Created/verified directory: {directory}")

class OSMDataProcessor:
    """Processes OpenStreetMap data to match GeoFix295 expected format."""

    def __init__(self):
        self.crs_utm = "EPSG:3414"  # Singapore's projected CRS (SVY21)
        self.crs_wgs84 = "EPSG:4326"
        self.transformer_to_utm = pyproj.Transformer.from_crs(self.crs_wgs84, self.crs_utm, always_xy=True)
        self.transformer_to_wgs84 = pyproj.Transformer.from_crs(self.crs_utm, self.crs_wgs84, always_xy=True)

    def create_tile_grid(self) -> List[Dict]:
        """Create a grid of tiles covering Singapore."""
        tiles = []
        tile_id = 10000  # Start with a base tile ID

        lat_start = SINGAPORE_BOUNDS['south']
        lng_start = SINGAPORE_BOUNDS['west']

        lat = lat_start
        row = 0
        while lat < SINGAPORE_BOUNDS['north']:
            lng = lng_start
            col = 0
            while lng < SINGAPORE_BOUNDS['east']:
                tile = {
                    'tile_id': tile_id + row * 100 + col,
                    'bounds': {
                        'west': lng,
                        'south': lat,
                        'east': min(lng + TILE_SIZE_DEGREES, SINGAPORE_BOUNDS['east']),
                        'north': min(lat + TILE_SIZE_DEGREES, SINGAPORE_BOUNDS['north'])
                    }
                }
                tiles.append(tile)
                lng += TILE_SIZE_DEGREES
                col += 1
            lat += TILE_SIZE_DEGREES
            row += 1

        logger.info(f"Created {len(tiles)} tiles covering Singapore")
        return tiles

    def download_osm_network(self, bounds: Dict) -> Optional[gpd.GeoDataFrame]:
        """Download road network from OpenStreetMap for a tile."""
        try:
            # Create polygon from bounds
            polygon = box(bounds['west'], bounds['south'], bounds['east'], bounds['north'])

            # Download road network
            logger.info(f"Downloading OSM network for bounds: {bounds}")
            G = ox.graph_from_polygon(
                polygon,
                network_type='drive',
                simplify=True,
                retain_all=False,
                truncate_by_edge=True
            )

            if len(G.edges) == 0:
                logger.warning("No road network found in this tile")
                return None

            # Convert to GeoDataFrame
            gdf_edges = ox.graph_to_gdfs(G, nodes=False, edges=True)

            # Reset index to get edge IDs as columns
            gdf_edges = gdf_edges.reset_index()

            # Create LINK_ID (unique identifier for each road segment)
            gdf_edges['LINK_ID'] = gdf_edges.apply(
                lambda row: f"{row['u']}_{row['v']}_{row.get('key', 0)}", axis=1
            )

            # Extract road names and clean them
            gdf_edges['name'] = gdf_edges.get('name', '').fillna('')
            gdf_edges['name'] = gdf_edges['name'].apply(self._clean_road_name)

            # Add MULTIDIGIT flag (check if parallel roads share same name within 5m)
            gdf_edges['MULTIDIGIT'] = self._calculate_multidigit_flag(gdf_edges)

            # Add other required fields
            gdf_edges['ROAD_CLASS'] = gdf_edges.get('highway', 'unclassified')
            gdf_edges['SPEED_LIMIT'] = gdf_edges.get('maxspeed', '').apply(self._parse_speed_limit)
            gdf_edges['LENGTH_M'] = gdf_edges.geometry.length

            # Select required columns for navigation
            nav_columns = ['LINK_ID', 'name', 'MULTIDIGIT', 'ROAD_CLASS', 'SPEED_LIMIT', 'LENGTH_M', 'geometry']
            available_columns = [col for col in nav_columns if col in gdf_edges.columns]

            return gdf_edges[available_columns]

        except Exception as e:
            logger.error(f"Error downloading OSM network: {e}")
            return None

    def _clean_road_name(self, name) -> str:
        """Clean and standardize road names."""
        if pd.isna(name) or name == '':
            return 'UNNAMED'

        # Handle lists of names (OSM sometimes returns multiple names)
        if isinstance(name, list):
            name = name[0] if name else 'UNNAMED'

        # Basic cleaning
        name = str(name).strip()
        if not name:
            return 'UNNAMED'

        return name

    def _parse_speed_limit(self, speed) -> int:
        """Parse speed limit from OSM data."""
        if pd.isna(speed) or speed == '':
            return 50  # Default speed limit

        try:
            # Extract numeric part
            if isinstance(speed, str):
                speed = ''.join(filter(str.isdigit, speed))
            return int(speed) if speed else 50
        except:
            return 50

    def _calculate_multidigit_flag(self, gdf: gpd.GeoDataFrame) -> pd.Series:
        """Calculate MULTIDIGIT flag - True if parallel roads share same name within 5m."""
        multidigit = pd.Series(False, index=gdf.index)

        if gdf.empty:
            return multidigit

        try:
            # Convert to UTM for accurate distance calculation
            gdf_utm = gdf.to_crs(self.crs_utm)

            # Group by road name (excluding unnamed roads)
            named_roads = gdf_utm[gdf_utm['name'] != 'UNNAMED']

            for name, group in named_roads.groupby('name'):
                if len(group) < 2:
                    continue

                # Check distances between roads with same name
                for i, road1 in group.iterrows():
                    for j, road2 in group.iterrows():
                        if i >= j:
                            continue

                        try:
                            distance = road1.geometry.distance(road2.geometry)
                            if distance <= 5:  # Within 5 meters
                                multidigit.loc[i] = True
                                multidigit.loc[j] = True
                        except:
                            continue

            return multidigit

        except Exception as e:
            logger.warning(f"Error calculating MULTIDIGIT flag: {e}")
            return multidigit

    def create_naming_data(self, nav_gdf: gpd.GeoDataFrame, tile_id: int) -> bool:
        """Create road naming data from navigation data."""
        try:
            if nav_gdf is None or nav_gdf.empty:
                # Create empty file
                empty_data = gpd.GeoDataFrame(columns=['LINK_ID', 'ST_NAME'], crs=self.crs_wgs84)
                output_path = DIR_NAMING / f"naming_{tile_id}.geojson"
                empty_data.to_file(output_path, driver='GeoJSON')
                return True

            # Create naming data with same LINK_ID and road name as ST_NAME
            naming_data = nav_gdf[['LINK_ID', 'name', 'geometry']].copy()
            naming_data = naming_data.rename(columns={'name': 'ST_NAME'})

            # Save as GeoJSON
            output_path = DIR_NAMING / f"naming_{tile_id}.geojson"
            naming_data.to_file(output_path, driver='GeoJSON')

            logger.info(f"‚úÖ Created naming data for tile {tile_id}: {len(naming_data)} roads")
            return True

        except Exception as e:
            logger.error(f"Error creating naming data for tile {tile_id}: {e}")
            return False

    def download_osm_pois(self, bounds: Dict) -> Optional[gpd.GeoDataFrame]:
        """Download POIs from OpenStreetMap."""
        try:
            # Create polygon from bounds
            polygon = box(bounds['west'], bounds['south'], bounds['east'], bounds['north'])

            # Download POI features from OSM
            tags = {
                'amenity': True,
                'shop': True,
                'tourism': True,
                'leisure': True,
                'office': True,
                'building': ['commercial', 'retail', 'office', 'hospital', 'school']
            }

            logger.info(f"Downloading OSM POIs for bounds: {bounds}")
            pois = ox.features_from_polygon(polygon, tags=tags)

            if pois.empty:
                logger.info("No POIs found in this tile")
                return gpd.GeoDataFrame(columns=['POI_ID', 'name', 'category'], crs=self.crs_wgs84)

            # Convert to points (take centroids for non-point geometries)
            pois = pois.copy()
            pois['geometry'] = pois.geometry.apply(
                lambda geom: geom.centroid if geom.geom_type != 'Point' else geom
            )

            # Create POI_ID and extract relevant information
            pois['POI_ID'] = pois.index.astype(str)
            pois['name'] = pois.get('name', '').fillna('UNNAMED')

            # Create category from OSM tags
            pois['category'] = pois.apply(self._determine_poi_category, axis=1)

            # Select required columns
            poi_columns = ['POI_ID', 'name', 'category', 'geometry']
            available_columns = [col for col in poi_columns if col in pois.columns]

            return pois[available_columns].reset_index(drop=True)

        except Exception as e:
            logger.error(f"Error downloading OSM POIs: {e}")
            return gpd.GeoDataFrame(columns=['POI_ID', 'name', 'category'], crs=self.crs_wgs84)

    def _determine_poi_category(self, row) -> str:
        """Determine POI category from OSM tags."""
        if not pd.isna(row.get('amenity')):
            return f"amenity_{row['amenity']}"
        elif not pd.isna(row.get('shop')):
            return f"shop_{row['shop']}"
        elif not pd.isna(row.get('tourism')):
            return f"tourism_{row['tourism']}"
        elif not pd.isna(row.get('leisure')):
            return f"leisure_{row['leisure']}"
        elif not pd.isna(row.get('office')):
            return f"office_{row['office']}"
        elif not pd.isna(row.get('building')):
            return f"building_{row['building']}"
        else:
            return 'unknown'

    def calculate_poi_attributes(self, pois_gdf: gpd.GeoDataFrame,
                               roads_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
        """Calculate POI attributes (LINK_ID, PERCFRREF, POI_ST_SD)."""
        if pois_gdf.empty or roads_gdf.empty:
            logger.warning("Empty POI or road data, skipping attribute calculation")
            return pois_gdf

        logger.info(f"Calculating POI-to-road relationships for {len(pois_gdf)} POIs...")

        try:
            # Perform spatial join to find nearest roads
            joined = gpd.sjoin_nearest(
                pois_gdf,
                roads_gdf[["LINK_ID", "geometry"]],
                how="left",
                distance_col="dist_m"
            )

            # Calculate percentage along road and side of street
            def calculate_road_attributes(row):
                if pd.isna(row.get('LINK_ID')):
                    return 0.0, 0

                try:
                    # Get the road geometry
                    road_matches = roads_gdf.loc[roads_gdf.LINK_ID == row.LINK_ID, "geometry"]
                    if road_matches.empty:
                        return 0.0, 0

                    road_geom = road_matches.iloc[0]
                    poi_point = row.geometry

                    # Calculate percentage along road
                    if road_geom.length > 0:
                        percent_along = round(100 * road_geom.project(poi_point) / road_geom.length, 3)
                        percent_along = max(0.0, min(100.0, percent_along))  # Clamp to [0, 100]
                    else:
                        percent_along = 0.0

                    # Calculate side of street (simplified approach)
                    try:
                        # Convert to UTM for accurate offset calculation
                        road_utm = transform(self.transformer_to_utm.transform, road_geom)
                        poi_utm = transform(self.transformer_to_utm.transform, poi_point)

                        # Create offset lines
                        right_offset = road_utm.parallel_offset(5, "right")
                        left_offset = road_utm.parallel_offset(5, "left")

                        right_dist = poi_utm.distance(right_offset)
                        left_dist = poi_utm.distance(left_offset)

                        if right_dist < left_dist:
                            side = 1  # Right side
                        else:
                            side = 2  # Left side
                    except:
                        side = 0  # Unknown

                    return percent_along, side

                except Exception as e:
                    logger.debug(f"Error calculating road attributes: {e}")
                    return 0.0, 0

            # Apply calculations with progress bar
            tqdm.pandas(desc="Processing POI attributes")
            road_attrs = joined.progress_apply(calculate_road_attributes, axis=1, result_type='expand')
            joined["PERCFRREF"] = road_attrs[0]
            joined["POI_ST_SD"] = road_attrs[1]

            return joined

        except Exception as e:
            logger.error(f"Error in POI attribute calculation: {e}")
            return pois_gdf

    def process_tile(self, tile: Dict) -> bool:
        """Process a single tile - download and save all data types."""
        tile_id = tile['tile_id']
        bounds = tile['bounds']

        logger.info(f"\nüìç Processing tile {tile_id}")

        success_flags = {'nav': False, 'naming': False, 'poi': False}

        # 1. Download road network (STREETS_NAV)
        nav_gdf = self.download_osm_network(bounds)
        if nav_gdf is not None and not nav_gdf.empty:
            try:
                output_path = DIR_NAV / f"nav_{tile_id}.geojson"
                nav_gdf.to_file(output_path, driver='GeoJSON')
                logger.info(f"‚úÖ Saved navigation data: {len(nav_gdf)} road segments")
                success_flags['nav'] = True
            except Exception as e:
                logger.error(f"Error saving navigation data: {e}")
        else:
            # Create empty file
            empty_gdf = gpd.GeoDataFrame(columns=['LINK_ID', 'name', 'MULTIDIGIT'], crs=self.crs_wgs84)
            output_path = DIR_NAV / f"nav_{tile_id}.geojson"
            empty_gdf.to_file(output_path, driver='GeoJSON')
            logger.info("Created empty navigation file")

        # 2. Create road naming data (STREETS_NAMING_ADDRESSING)
        success_flags['naming'] = self.create_naming_data(nav_gdf, tile_id)

        # 3. Download and process POIs
        pois_gdf = self.download_osm_pois(bounds)
        if pois_gdf is not None and not pois_gdf.empty and nav_gdf is not None and not nav_gdf.empty:
            try:
                # Calculate POI attributes
                pois_processed = self.calculate_poi_attributes(pois_gdf, nav_gdf)

                # Save processed POIs as CSV
                output_path = DIR_POI / f"pois_{tile_id}.csv"
                columns_to_save = ["POI_ID", "LINK_ID", "PERCFRREF", "POI_ST_SD", "name", "category"]
                available_columns = [col for col in columns_to_save if col in pois_processed.columns]

                if available_columns:
                    pois_processed[available_columns].to_csv(output_path, index=False)
                    logger.info(f"‚úÖ Saved POI data: {len(pois_processed)} POIs")
                    success_flags['poi'] = True
                else:
                    # Create empty file
                    output_path.touch()
                    logger.info("Created empty POI file")
            except Exception as e:
                logger.error(f"Error processing POIs: {e}")
                # Create empty file
                output_path = DIR_POI / f"pois_{tile_id}.csv"
                output_path.touch()
        else:
            # Create empty POI file
            output_path = DIR_POI / f"pois_{tile_id}.csv"
            output_path.touch()
            logger.info("Created empty POI file")

        return any(success_flags.values())

def main():
    """Main execution function."""
    logger.info("üÜì Starting Singapore GeoData Download (FREE OSM Version)")
    logger.info("üìç Downloading data for Singapore using OpenStreetMap")

    processor = OSMDataProcessor()

    # Create tile grid
    tiles = processor.create_tile_grid()
    logger.info(f"Processing {len(tiles)} tiles covering Singapore")

    success_count = 0
    failed_tiles = []

    # Process each tile
    for tile in tqdm(tiles, desc="Processing tiles"):
        try:
            if processor.process_tile(tile):
                success_count += 1
                logger.info(f"‚úÖ Successfully processed tile {tile['tile_id']}")
            else:
                failed_tiles.append(tile['tile_id'])
                logger.error(f"‚ùå Failed to process tile {tile['tile_id']}")
        except Exception as e:
            failed_tiles.append(tile['tile_id'])
            logger.error(f"‚ùå Error processing tile {tile['tile_id']}: {e}")

    # Summary
    logger.info(f"\nüìä DOWNLOAD SUMMARY")
    logger.info(f"Successful: {success_count}/{len(tiles)} tiles")
    logger.info(f"Failed: {len(failed_tiles)} tiles")

    if failed_tiles:
        logger.error(f"Failed tiles: {failed_tiles}")

    if success_count > 0:
        logger.info(f"\n‚úÖ Data downloaded successfully! Files saved to:")
        logger.info(f"   üìÅ Navigation: {DIR_NAV}")
        logger.info(f"   üìÅ Naming: {DIR_NAMING}")
        logger.info(f"   üìÅ POIs: {DIR_POI}")
        logger.info(f"\nüöÄ Ready for GeoFix295! Try:")
        logger.info(f"   python Main.py --mode test   # quick check")
        logger.info(f"   python Main.py --mode full   # process all tiles")
        logger.info(f"\nüÜì All data sourced from OpenStreetMap - completely free!")
    else:
        logger.error("‚ùå No tiles were successfully processed!")
        logger.error("Check your internet connection and try again.")

if __name__ == "__main__":
    # Install required packages if needed
    try:
        import osmnx
        import geopandas
        import pyproj
    except ImportError as e:
        print("‚ùå Missing required packages. Install them with:")
        print("pip install osmnx geopandas pyproj tqdm pandas numpy requests")
        print("Note: This may take a few minutes as it installs spatial libraries.")
        exit(1)

    main()

#!/usr/bin/env python
"""
DIY Singapore GeoData Downloader using FREE OpenStreetMap data
Creates the same folder structure as HERE API version for GeoFix295 compatibility.

üÜì Uses OpenStreetMap data via OSMnx - completely free!
üá∏üá¨ Focused on Singapore with proper tile-based organization
üìÅ Generates same folder structure: STREETS_NAV, STREETS_NAMING_ADDRESSING, POIs
"""
import os
import pandas as pd
import geopandas as gpd
import osmnx as ox
import numpy as np
from pathlib import Path
from shapely.geometry import Point, Polygon, box
from shapely.ops import transform
import pyproj
from tqdm import tqdm
import logging
from typing import List, Tuple, Dict, Optional
import requests
import json
import warnings
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configure OSMnx
ox.config(use_cache=True, log_console=True)

# Singapore bounding box (approximate)
SINGAPORE_BOUNDS = {
    'north': 1.4504,
    'south': 1.2303,
    'east': 104.0117,
    'west': 103.5986
}

# Level 11 tile size approximation (for organizing data)
TILE_SIZE_DEGREES = 0.02  # Approximate tile size in degrees

# Directory structure - matches HERE API version
BASE_DIR = Path.cwd()
DIR_NAV = BASE_DIR / "STREETS_NAV"
DIR_NAMING = BASE_DIR / "STREETS_NAMING_ADDRESSING"
DIR_POI = BASE_DIR / "POIs"

# Create directories
for directory in (DIR_NAV, DIR_NAMING, DIR_POI):
    directory.mkdir(exist_ok=True)
    logger.info(f"Created/verified directory: {directory}")

class OSMDataProcessor:
    """Processes OpenStreetMap data to match GeoFix295 expected format."""

    def __init__(self):
        self.crs_utm = "EPSG:3414"  # Singapore's projected CRS (SVY21)
        self.crs_wgs84 = "EPSG:4326"
        self.transformer_to_utm = pyproj.Transformer.from_crs(self.crs_wgs84, self.crs_utm, always_xy=True)
        self.transformer_to_wgs84 = pyproj.Transformer.from_crs(self.crs_utm, self.crs_wgs84, always_xy=True)

    def create_tile_grid(self) -> List[Dict]:
        """Create a grid of tiles covering Singapore."""
        tiles = []
        tile_id = 10000  # Start with a base tile ID

        lat_start = SINGAPORE_BOUNDS['south']
        lng_start = SINGAPORE_BOUNDS['west']

        lat = lat_start
        row = 0
        while lat < SINGAPORE_BOUNDS['north']:
            lng = lng_start
            col = 0
            while lng < SINGAPORE_BOUNDS['east']:
                tile = {
                    'tile_id': tile_id + row * 100 + col,
                    'bounds': {
                        'west': lng,
                        'south': lat,
                        'east': min(lng + TILE_SIZE_DEGREES, SINGAPORE_BOUNDS['east']),
                        'north': min(lat + TILE_SIZE_DEGREES, SINGAPORE_BOUNDS['north'])
                    }
                }
                tiles.append(tile)
                lng += TILE_SIZE_DEGREES
                col += 1
            lat += TILE_SIZE_DEGREES
            row += 1

        logger.info(f"Created {len(tiles)} tiles covering Singapore")
        return tiles

    def download_osm_network(self, bounds: Dict) -> Optional[gpd.GeoDataFrame]:
        """Download road network from OpenStreetMap for a tile."""
        try:
            # Create polygon from bounds
            polygon = box(bounds['west'], bounds['south'], bounds['east'], bounds['north'])

            # Download road network
            logger.info(f"Downloading OSM network for bounds: {bounds}")
            G = ox.graph_from_polygon(
                polygon,
                network_type='drive',
                simplify=True,
                retain_all=False,
                truncate_by_edge=True
            )

            if len(G.edges) == 0:
                logger.warning("No road network found in this tile")
                return None

            # Convert to GeoDataFrame
            gdf_edges = ox.graph_to_gdfs(G, nodes=False, edges=True)

            # Reset index to get edge IDs as columns
            gdf_edges = gdf_edges.reset_index()

            # Create LINK_ID (unique identifier for each road segment)
            gdf_edges['LINK_ID'] = gdf_edges.apply(
                lambda row: f"{row['u']}_{row['v']}_{row.get('key', 0)}", axis=1
            )

            # Extract road names and clean them
            gdf_edges['name'] = gdf_edges.get('name', '').fillna('')
            gdf_edges['name'] = gdf_edges['name'].apply(self._clean_road_name)

            # Add MULTIDIGIT flag (check if parallel roads share same name within 5m)
            gdf_edges['MULTIDIGIT'] = self._calculate_multidigit_flag(gdf_edges)

            # Add other required fields
            gdf_edges['ROAD_CLASS'] = gdf_edges.get('highway', 'unclassified')
            gdf_edges['SPEED_LIMIT'] = gdf_edges.get('maxspeed', '').apply(self._parse_speed_limit)
            gdf_edges['LENGTH_M'] = gdf_edges.geometry.length

            # Select required columns for navigation
            nav_columns = ['LINK_ID', 'name', 'MULTIDIGIT', 'ROAD_CLASS', 'SPEED_LIMIT', 'LENGTH_M', 'geometry']
            available_columns = [col for col in nav_columns if col in gdf_edges.columns]

            return gdf_edges[available_columns]

        except Exception as e:
            logger.error(f"Error downloading OSM network: {e}")
            return None

    def _clean_road_name(self, name) -> str:
        """Clean and standardize road names."""
        if pd.isna(name) or name == '':
            return 'UNNAMED'

        # Handle lists of names (OSM sometimes returns multiple names)
        if isinstance(name, list):
            name = name[0] if name else 'UNNAMED'

        # Basic cleaning
        name = str(name).strip()
        if not name:
            return 'UNNAMED'

        return name

    def _parse_speed_limit(self, speed) -> int:
        """Parse speed limit from OSM data."""
        if pd.isna(speed) or speed == '':
            return 50  # Default speed limit

        try:
            # Extract numeric part
            if isinstance(speed, str):
                speed = ''.join(filter(str.isdigit, speed))
            return int(speed) if speed else 50
        except:
            return 50

    def _calculate_multidigit_flag(self, gdf: gpd.GeoDataFrame) -> pd.Series:
        """Calculate MULTIDIGIT flag - True if parallel roads share same name within 5m."""
        multidigit = pd.Series(False, index=gdf.index)

        if gdf.empty:
            return multidigit

        try:
            # Convert to UTM for accurate distance calculation
            gdf_utm = gdf.to_crs(self.crs_utm)

            # Group by road name (excluding unnamed roads)
            named_roads = gdf_utm[gdf_utm['name'] != 'UNNAMED']

            for name, group in named_roads.groupby('name'):
                if len(group) < 2:
                    continue

                # Check distances between roads with same name
                for i, road1 in group.iterrows():
                    for j, road2 in group.iterrows():
                        if i >= j:
                            continue

                        try:
                            distance = road1.geometry.distance(road2.geometry)
                            if distance <= 5:  # Within 5 meters
                                multidigit.loc[i] = True
                                multidigit.loc[j] = True
                        except:
                            continue

            return multidigit

        except Exception as e:
            logger.warning(f"Error calculating MULTIDIGIT flag: {e}")
            return multidigit

    def create_naming_data(self, nav_gdf: gpd.GeoDataFrame, tile_id: int) -> bool:
        """Create road naming data from navigation data."""
        try:
            if nav_gdf is None or nav_gdf.empty:
                # Create empty file
                empty_data = gpd.GeoDataFrame(columns=['LINK_ID', 'ST_NAME'], crs=self.crs_wgs84)
                output_path = DIR_NAMING / f"naming_{tile_id}.geojson"
                empty_data.to_file(output_path, driver='GeoJSON')
                return True

            # Create naming data with same LINK_ID and road name as ST_NAME
            naming_data = nav_gdf[['LINK_ID', 'name', 'geometry']].copy()
            naming_data = naming_data.rename(columns={'name': 'ST_NAME'})

            # Save as GeoJSON
            output_path = DIR_NAMING / f"naming_{tile_id}.geojson"
            naming_data.to_file(output_path, driver='GeoJSON')

            logger.info(f"‚úÖ Created naming data for tile {tile_id}: {len(naming_data)} roads")
            return True

        except Exception as e:
            logger.error(f"Error creating naming data for tile {tile_id}: {e}")
            return False

    def download_osm_pois(self, bounds: Dict) -> Optional[gpd.GeoDataFrame]:
        """Download POIs from OpenStreetMap."""
        try:
            # Create polygon from bounds
            polygon = box(bounds['west'], bounds['south'], bounds['east'], bounds['north'])

            # Download POI features from OSM
            tags = {
                'amenity': True,
                'shop': True,
                'tourism': True,
                'leisure': True,
                'office': True,
                'building': ['commercial', 'retail', 'office', 'hospital', 'school']
            }

            logger.info(f"Downloading OSM POIs for bounds: {bounds}")
            pois = ox.features_from_polygon(polygon, tags=tags)

            if pois.empty:
                logger.info("No POIs found in this tile")
                return gpd.GeoDataFrame(columns=['POI_ID', 'name', 'category'], crs=self.crs_wgs84)

            # Convert to points (take centroids for non-point geometries)
            pois = pois.copy()
            pois['geometry'] = pois.geometry.apply(
                lambda geom: geom.centroid if geom.geom_type != 'Point' else geom
            )

            # Create POI_ID and extract relevant information
            pois['POI_ID'] = pois.index.astype(str)
            pois['name'] = pois.get('name', '').fillna('UNNAMED')

            # Create category from OSM tags
            pois['category'] = pois.apply(self._determine_poi_category, axis=1)

            # Select required columns
            poi_columns = ['POI_ID', 'name', 'category', 'geometry']
            available_columns = [col for col in poi_columns if col in pois.columns]

            return pois[available_columns].reset_index(drop=True)

        except Exception as e:
            logger.error(f"Error downloading OSM POIs: {e}")
            return gpd.GeoDataFrame(columns=['POI_ID', 'name', 'category'], crs=self.crs_wgs84)

    def _determine_poi_category(self, row) -> str:
        """Determine POI category from OSM tags."""
        if not pd.isna(row.get('amenity')):
            return f"amenity_{row['amenity']}"
        elif not pd.isna(row.get('shop')):
            return f"shop_{row['shop']}"
        elif not pd.isna(row.get('tourism')):
            return f"tourism_{row['tourism']}"
        elif not pd.isna(row.get('leisure')):
            return f"leisure_{row['leisure']}"
        elif not pd.isna(row.get('office')):
            return f"office_{row['office']}"
        elif not pd.isna(row.get('building')):
            return f"building_{row['building']}"
        else:
            return 'unknown'

    def calculate_poi_attributes(self, pois_gdf: gpd.GeoDataFrame,
                               roads_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
        """Calculate POI attributes (LINK_ID, PERCFRREF, POI_ST_SD)."""
        if pois_gdf.empty or roads_gdf.empty:
            logger.warning("Empty POI or road data, skipping attribute calculation")
            return pois_gdf

        logger.info(f"Calculating POI-to-road relationships for {len(pois_gdf)} POIs...")

        try:
            # Perform spatial join to find nearest roads
            joined = gpd.sjoin_nearest(
                pois_gdf,
                roads_gdf[["LINK_ID", "geometry"]],
                how="left",
                distance_col="dist_m"
            )

            # Calculate percentage along road and side of street
            def calculate_road_attributes(row):
                if pd.isna(row.get('LINK_ID')):
                    return 0.0, 0

                try:
                    # Get the road geometry
                    road_matches = roads_gdf.loc[roads_gdf.LINK_ID == row.LINK_ID, "geometry"]
                    if road_matches.empty:
                        return 0.0, 0

                    road_geom = road_matches.iloc[0]
                    poi_point = row.geometry

                    # Calculate percentage along road
                    if road_geom.length > 0:
                        percent_along = round(100 * road_geom.project(poi_point) / road_geom.length, 3)
                        percent_along = max(0.0, min(100.0, percent_along))  # Clamp to [0, 100]
                    else:
                        percent_along = 0.0

                    # Calculate side of street (simplified approach)
                    try:
                        # Convert to UTM for accurate offset calculation
                        road_utm = transform(self.transformer_to_utm.transform, road_geom)
                        poi_utm = transform(self.transformer_to_utm.transform, poi_point)

                        # Create offset lines
                        right_offset = road_utm.parallel_offset(5, "right")
                        left_offset = road_utm.parallel_offset(5, "left")

                        right_dist = poi_utm.distance(right_offset)
                        left_dist = poi_utm.distance(left_offset)

                        if right_dist < left_dist:
                            side = 1  # Right side
                        else:
                            side = 2  # Left side
                    except:
                        side = 0  # Unknown

                    return percent_along, side

                except Exception as e:
                    logger.debug(f"Error calculating road attributes: {e}")
                    return 0.0, 0

            # Apply calculations with progress bar
            tqdm.pandas(desc="Processing POI attributes")
            road_attrs = joined.progress_apply(calculate_road_attributes, axis=1, result_type='expand')
            joined["PERCFRREF"] = road_attrs[0]
            joined["POI_ST_SD"] = road_attrs[1]

            return joined

        except Exception as e:
            logger.error(f"Error in POI attribute calculation: {e}")
            return pois_gdf

    def process_tile(self, tile: Dict) -> bool:
        """Process a single tile - download and save all data types."""
        tile_id = tile['tile_id']
        bounds = tile['bounds']

        logger.info(f"\nüìç Processing tile {tile_id}")

        success_flags = {'nav': False, 'naming': False, 'poi': False}

        # 1. Download road network (STREETS_NAV)
        nav_gdf = self.download_osm_network(bounds)
        if nav_gdf is not None and not nav_gdf.empty:
            try:
                output_path = DIR_NAV / f"nav_{tile_id}.geojson"
                nav_gdf.to_file(output_path, driver='GeoJSON')
                logger.info(f"‚úÖ Saved navigation data: {len(nav_gdf)} road segments")
                success_flags['nav'] = True
            except Exception as e:
                logger.error(f"Error saving navigation data: {e}")
        else:
            # Create empty file
            empty_gdf = gpd.GeoDataFrame(columns=['LINK_ID', 'name', 'MULTIDIGIT'], crs=self.crs_wgs84)
            output_path = DIR_NAV / f"nav_{tile_id}.geojson"
            empty_gdf.to_file(output_path, driver='GeoJSON')
            logger.info("Created empty navigation file")

        # 2. Create road naming data (STREETS_NAMING_ADDRESSING)
        success_flags['naming'] = self.create_naming_data(nav_gdf, tile_id)

        # 3. Download and process POIs
        pois_gdf = self.download_osm_pois(bounds)
        if pois_gdf is not None and not pois_gdf.empty and nav_gdf is not None and not nav_gdf.empty:
            try:
                # Calculate POI attributes
                pois_processed = self.calculate_poi_attributes(pois_gdf, nav_gdf)

                # Save processed POIs as CSV
                output_path = DIR_POI / f"pois_{tile_id}.csv"
                columns_to_save = ["POI_ID", "LINK_ID", "PERCFRREF", "POI_ST_SD", "name", "category"]
                available_columns = [col for col in columns_to_save if col in pois_processed.columns]

                if available_columns:
                    pois_processed[available_columns].to_csv(output_path, index=False)
                    logger.info(f"‚úÖ Saved POI data: {len(pois_processed)} POIs")
                    success_flags['poi'] = True
                else:
                    # Create empty file
                    output_path.touch()
                    logger.info("Created empty POI file")
            except Exception as e:
                logger.error(f"Error processing POIs: {e}")
                # Create empty file
                output_path = DIR_POI / f"pois_{tile_id}.csv"
                output_path.touch()
        else:
            # Create empty POI file
            output_path = DIR_POI / f"pois_{tile_id}.csv"
            output_path.touch()
            logger.info("Created empty POI file")

        return any(success_flags.values())

def main():
    """Main execution function."""
    logger.info("üÜì Starting Singapore GeoData Download (FREE OSM Version)")
    logger.info("üìç Downloading data for Singapore using OpenStreetMap")

    processor = OSMDataProcessor()

    # Create tile grid
    tiles = processor.create_tile_grid()
    logger.info(f"Processing {len(tiles)} tiles covering Singapore")

    success_count = 0
    failed_tiles = []

    # Process each tile
    for tile in tqdm(tiles, desc="Processing tiles"):
        try:
            if processor.process_tile(tile):
                success_count += 1
                logger.info(f"‚úÖ Successfully processed tile {tile['tile_id']}")
            else:
                failed_tiles.append(tile['tile_id'])
                logger.error(f"‚ùå Failed to process tile {tile['tile_id']}")
        except Exception as e:
            failed_tiles.append(tile['tile_id'])
            logger.error(f"‚ùå Error processing tile {tile['tile_id']}: {e}")

    # Summary
    logger.info(f"\nüìä DOWNLOAD SUMMARY")
    logger.info(f"Successful: {success_count}/{len(tiles)} tiles")
    logger.info(f"Failed: {len(failed_tiles)} tiles")

    if failed_tiles:
        logger.error(f"Failed tiles: {failed_tiles}")

    if success_count > 0:
        logger.info(f"\n‚úÖ Data downloaded successfully! Files saved to:")
        logger.info(f"   üìÅ Navigation: {DIR_NAV}")
        logger.info(f"   üìÅ Naming: {DIR_NAMING}")
        logger.info(f"   üìÅ POIs: {DIR_POI}")
        logger.info(f"\nüöÄ Ready for GeoFix295! Try:")
        logger.info(f"   python Main.py --mode test   # quick check")
        logger.info(f"   python Main.py --mode full   # process all tiles")
        logger.info(f"\nüÜì All data sourced from OpenStreetMap - completely free!")
    else:
        logger.error("‚ùå No tiles were successfully processed!")
        logger.error("Check your internet connection and try again.")

if __name__ == "__main__":
    # Install required packages if needed
    try:
        import osmnx
        import geopandas
        import pyproj
    except ImportError as e:
        print("‚ùå Missing required packages. Install them with:")
        print("pip install osmnx geopandas pyproj tqdm pandas numpy requests")
        print("Note: This may take a few minutes as it installs spatial libraries.")
        exit(1)

    main()

#!/usr/bin/env python
"""
DIY Singapore GeoData Downloader using FREE OpenStreetMap data
Creates the same folder structure as HERE API version for GeoFix295 compatibility.

üÜì Uses OpenStreetMap data via OSMnx - completely free!
üá∏üá¨ Focused on Singapore with proper tile-based organization
üìÅ Generates same folder structure: STREETS_NAV, STREETS_NAMING_ADDRESSING, POIs
"""
import os
import pandas as pd
import geopandas as gpd
import osmnx as ox
import numpy as np
from pathlib import Path
from shapely.geometry import Point, Polygon, box
from shapely.ops import transform
import pyproj
from tqdm import tqdm
import logging
from typing import List, Tuple, Dict, Optional
import requests
import json
import warnings
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configure OSMnx settings (updated for newer versions)
ox.settings.use_cache = True
ox.settings.log_console = True

# Singapore bounding box (approximate)
SINGAPORE_BOUNDS = {
    'north': 1.4504,
    'south': 1.2303,
    'east': 104.0117,
    'west': 103.5986
}

# Level 11 tile size approximation (for organizing data)
TILE_SIZE_DEGREES = 0.02  # Approximate tile size in degrees

# Directory structure - matches HERE API version
BASE_DIR = Path.cwd()
DIR_NAV = BASE_DIR / "STREETS_NAV"
DIR_NAMING = BASE_DIR / "STREETS_NAMING_ADDRESSING"
DIR_POI = BASE_DIR / "POIs"

# Create directories
for directory in (DIR_NAV, DIR_NAMING, DIR_POI):
    directory.mkdir(exist_ok=True)
    logger.info(f"Created/verified directory: {directory}")

class OSMDataProcessor:
    """Processes OpenStreetMap data to match GeoFix295 expected format."""

    def __init__(self):
        self.crs_utm = "EPSG:3414"  # Singapore's projected CRS (SVY21)
        self.crs_wgs84 = "EPSG:4326"
        self.transformer_to_utm = pyproj.Transformer.from_crs(self.crs_wgs84, self.crs_utm, always_xy=True)
        self.transformer_to_wgs84 = pyproj.Transformer.from_crs(self.crs_utm, self.crs_wgs84, always_xy=True)

    def create_tile_grid(self) -> List[Dict]:
        """Create a grid of tiles covering Singapore."""
        tiles = []
        tile_id = 10000  # Start with a base tile ID

        lat_start = SINGAPORE_BOUNDS['south']
        lng_start = SINGAPORE_BOUNDS['west']

        lat = lat_start
        row = 0
        while lat < SINGAPORE_BOUNDS['north']:
            lng = lng_start
            col = 0
            while lng < SINGAPORE_BOUNDS['east']:
                tile = {
                    'tile_id': tile_id + row * 100 + col,
                    'bounds': {
                        'west': lng,
                        'south': lat,
                        'east': min(lng + TILE_SIZE_DEGREES, SINGAPORE_BOUNDS['east']),
                        'north': min(lat + TILE_SIZE_DEGREES, SINGAPORE_BOUNDS['north'])
                    }
                }
                tiles.append(tile)
                lng += TILE_SIZE_DEGREES
                col += 1
            lat += TILE_SIZE_DEGREES
            row += 1

        logger.info(f"Created {len(tiles)} tiles covering Singapore")
        return tiles

    def download_osm_network(self, bounds: Dict) -> Optional[gpd.GeoDataFrame]:
        """Download road network from OpenStreetMap for a tile."""
        try:
            # Create polygon from bounds
            polygon = box(bounds['west'], bounds['south'], bounds['east'], bounds['north'])

            # Download road network
            logger.info(f"Downloading OSM network for bounds: {bounds}")
            G = ox.graph_from_polygon(
                polygon,
                network_type='drive',
                simplify=True,
                retain_all=False,
                truncate_by_edge=True
            )

            if len(G.edges) == 0:
                logger.warning("No road network found in this tile")
                return None

            # Convert to GeoDataFrame
            gdf_edges = ox.graph_to_gdfs(G, nodes=False, edges=True)

            # Reset index to get edge IDs as columns
            gdf_edges = gdf_edges.reset_index()

            # Create LINK_ID (unique identifier for each road segment)
            gdf_edges['LINK_ID'] = gdf_edges.apply(
                lambda row: f"{row['u']}_{row['v']}_{row.get('key', 0)}", axis=1
            )

            # Extract road names and clean them
            gdf_edges['name'] = gdf_edges.get('name', '').fillna('')
            gdf_edges['name'] = gdf_edges['name'].apply(self._clean_road_name)

            # Add MULTIDIGIT flag (check if parallel roads share same name within 5m)
            gdf_edges['MULTIDIGIT'] = self._calculate_multidigit_flag(gdf_edges)

            # Add other required fields
            gdf_edges['ROAD_CLASS'] = gdf_edges.get('highway', 'unclassified')
            gdf_edges['SPEED_LIMIT'] = gdf_edges.get('maxspeed', '').apply(self._parse_speed_limit)
            gdf_edges['LENGTH_M'] = gdf_edges.geometry.length

            # Select required columns for navigation
            nav_columns = ['LINK_ID', 'name', 'MULTIDIGIT', 'ROAD_CLASS', 'SPEED_LIMIT', 'LENGTH_M', 'geometry']
            available_columns = [col for col in nav_columns if col in gdf_edges.columns]

            return gdf_edges[available_columns]

        except Exception as e:
            logger.error(f"Error downloading OSM network: {e}")
            return None

    def _clean_road_name(self, name) -> str:
        """Clean and standardize road names."""
        if pd.isna(name) or name == '':
            return 'UNNAMED'

        # Handle lists of names (OSM sometimes returns multiple names)
        if isinstance(name, list):
            name = name[0] if name else 'UNNAMED'

        # Basic cleaning
        name = str(name).strip()
        if not name:
            return 'UNNAMED'

        return name

    def _parse_speed_limit(self, speed) -> int:
        """Parse speed limit from OSM data."""
        if pd.isna(speed) or speed == '':
            return 50  # Default speed limit

        try:
            # Extract numeric part
            if isinstance(speed, str):
                speed = ''.join(filter(str.isdigit, speed))
            return int(speed) if speed else 50
        except:
            return 50

    def _calculate_multidigit_flag(self, gdf: gpd.GeoDataFrame) -> pd.Series:
        """Calculate MULTIDIGIT flag - True if parallel roads share same name within 5m."""
        multidigit = pd.Series(False, index=gdf.index)

        if gdf.empty:
            return multidigit

        try:
            # Convert to UTM for accurate distance calculation
            gdf_utm = gdf.to_crs(self.crs_utm)

            # Group by road name (excluding unnamed roads)
            named_roads = gdf_utm[gdf_utm['name'] != 'UNNAMED']

            for name, group in named_roads.groupby('name'):
                if len(group) < 2:
                    continue

                # Check distances between roads with same name
                for i, road1 in group.iterrows():
                    for j, road2 in group.iterrows():
                        if i >= j:
                            continue

                        try:
                            distance = road1.geometry.distance(road2.geometry)
                            if distance <= 5:  # Within 5 meters
                                multidigit.loc[i] = True
                                multidigit.loc[j] = True
                        except:
                            continue

            return multidigit

        except Exception as e:
            logger.warning(f"Error calculating MULTIDIGIT flag: {e}")
            return multidigit

    def create_naming_data(self, nav_gdf: gpd.GeoDataFrame, tile_id: int) -> bool:
        """Create road naming data from navigation data."""
        try:
            if nav_gdf is None or nav_gdf.empty:
                # Create empty file
                empty_data = gpd.GeoDataFrame(columns=['LINK_ID', 'ST_NAME'], crs=self.crs_wgs84)
                output_path = DIR_NAMING / f"naming_{tile_id}.geojson"
                empty_data.to_file(output_path, driver='GeoJSON')
                return True

            # Create naming data with same LINK_ID and road name as ST_NAME
            naming_data = nav_gdf[['LINK_ID', 'name', 'geometry']].copy()
            naming_data = naming_data.rename(columns={'name': 'ST_NAME'})

            # Save as GeoJSON
            output_path = DIR_NAMING / f"naming_{tile_id}.geojson"
            naming_data.to_file(output_path, driver='GeoJSON')

            logger.info(f"‚úÖ Created naming data for tile {tile_id}: {len(naming_data)} roads")
            return True

        except Exception as e:
            logger.error(f"Error creating naming data for tile {tile_id}: {e}")
            return False

    def download_osm_pois(self, bounds: Dict) -> Optional[gpd.GeoDataFrame]:
        """Download POIs from OpenStreetMap."""
        try:
            # Create polygon from bounds
            polygon = box(bounds['west'], bounds['south'], bounds['east'], bounds['north'])

            # Download POI features from OSM
            tags = {
                'amenity': True,
                'shop': True,
                'tourism': True,
                'leisure': True,
                'office': True,
                'building': ['commercial', 'retail', 'office', 'hospital', 'school']
            }

            logger.info(f"Downloading OSM POIs for bounds: {bounds}")
            pois = ox.features_from_polygon(polygon, tags=tags)

            if pois.empty:
                logger.info("No POIs found in this tile")
                return gpd.GeoDataFrame(columns=['POI_ID', 'name', 'category'], crs=self.crs_wgs84)

            # Convert to points (take centroids for non-point geometries)
            pois = pois.copy()
            pois['geometry'] = pois.geometry.apply(
                lambda geom: geom.centroid if geom.geom_type != 'Point' else geom
            )

            # Create POI_ID and extract relevant information
            pois['POI_ID'] = pois.index.astype(str)
            pois['name'] = pois.get('name', '').fillna('UNNAMED')

            # Create category from OSM tags
            pois['category'] = pois.apply(self._determine_poi_category, axis=1)

            # Select required columns
            poi_columns = ['POI_ID', 'name', 'category', 'geometry']
            available_columns = [col for col in poi_columns if col in pois.columns]

            return pois[available_columns].reset_index(drop=True)

        except Exception as e:
            logger.error(f"Error downloading OSM POIs: {e}")
            return gpd.GeoDataFrame(columns=['POI_ID', 'name', 'category'], crs=self.crs_wgs84)

    def _determine_poi_category(self, row) -> str:
        """Determine POI category from OSM tags."""
        if not pd.isna(row.get('amenity')):
            return f"amenity_{row['amenity']}"
        elif not pd.isna(row.get('shop')):
            return f"shop_{row['shop']}"
        elif not pd.isna(row.get('tourism')):
            return f"tourism_{row['tourism']}"
        elif not pd.isna(row.get('leisure')):
            return f"leisure_{row['leisure']}"
        elif not pd.isna(row.get('office')):
            return f"office_{row['office']}"
        elif not pd.isna(row.get('building')):
            return f"building_{row['building']}"
        else:
            return 'unknown'

    def calculate_poi_attributes(self, pois_gdf: gpd.GeoDataFrame,
                               roads_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
        """Calculate POI attributes (LINK_ID, PERCFRREF, POI_ST_SD)."""
        if pois_gdf.empty or roads_gdf.empty:
            logger.warning("Empty POI or road data, skipping attribute calculation")
            return pois_gdf

        logger.info(f"Calculating POI-to-road relationships for {len(pois_gdf)} POIs...")

        try:
            # Perform spatial join to find nearest roads
            joined = gpd.sjoin_nearest(
                pois_gdf,
                roads_gdf[["LINK_ID", "geometry"]],
                how="left",
                distance_col="dist_m"
            )

            # Calculate percentage along road and side of street
            def calculate_road_attributes(row):
                if pd.isna(row.get('LINK_ID')):
                    return 0.0, 0

                try:
                    # Get the road geometry
                    road_matches = roads_gdf.loc[roads_gdf.LINK_ID == row.LINK_ID, "geometry"]
                    if road_matches.empty:
                        return 0.0, 0

                    road_geom = road_matches.iloc[0]
                    poi_point = row.geometry

                    # Calculate percentage along road
                    if road_geom.length > 0:
                        percent_along = round(100 * road_geom.project(poi_point) / road_geom.length, 3)
                        percent_along = max(0.0, min(100.0, percent_along))  # Clamp to [0, 100]
                    else:
                        percent_along = 0.0

                    # Calculate side of street (simplified approach)
                    try:
                        # Convert to UTM for accurate offset calculation
                        road_utm = transform(self.transformer_to_utm.transform, road_geom)
                        poi_utm = transform(self.transformer_to_utm.transform, poi_point)

                        # Create offset lines
                        right_offset = road_utm.parallel_offset(5, "right")
                        left_offset = road_utm.parallel_offset(5, "left")

                        right_dist = poi_utm.distance(right_offset)
                        left_dist = poi_utm.distance(left_offset)

                        if right_dist < left_dist:
                            side = 1  # Right side
                        else:
                            side = 2  # Left side
                    except:
                        side = 0  # Unknown

                    return percent_along, side

                except Exception as e:
                    logger.debug(f"Error calculating road attributes: {e}")
                    return 0.0, 0

            # Apply calculations with progress bar
            tqdm.pandas(desc="Processing POI attributes")
            road_attrs = joined.progress_apply(calculate_road_attributes, axis=1, result_type='expand')
            joined["PERCFRREF"] = road_attrs[0]
            joined["POI_ST_SD"] = road_attrs[1]

            return joined

        except Exception as e:
            logger.error(f"Error in POI attribute calculation: {e}")
            return pois_gdf

    def process_tile(self, tile: Dict) -> bool:
        """Process a single tile - download and save all data types."""
        tile_id = tile['tile_id']
        bounds = tile['bounds']

        logger.info(f"\nüìç Processing tile {tile_id}")

        success_flags = {'nav': False, 'naming': False, 'poi': False}

        # 1. Download road network (STREETS_NAV)
        nav_gdf = self.download_osm_network(bounds)
        if nav_gdf is not None and not nav_gdf.empty:
            try:
                output_path = DIR_NAV / f"nav_{tile_id}.geojson"
                nav_gdf.to_file(output_path, driver='GeoJSON')
                logger.info(f"‚úÖ Saved navigation data: {len(nav_gdf)} road segments")
                success_flags['nav'] = True
            except Exception as e:
                logger.error(f"Error saving navigation data: {e}")
        else:
            # Create empty file
            empty_gdf = gpd.GeoDataFrame(columns=['LINK_ID', 'name', 'MULTIDIGIT'], crs=self.crs_wgs84)
            output_path = DIR_NAV / f"nav_{tile_id}.geojson"
            empty_gdf.to_file(output_path, driver='GeoJSON')
            logger.info("Created empty navigation file")

        # 2. Create road naming data (STREETS_NAMING_ADDRESSING)
        success_flags['naming'] = self.create_naming_data(nav_gdf, tile_id)

        # 3. Download and process POIs
        pois_gdf = self.download_osm_pois(bounds)
        if pois_gdf is not None and not pois_gdf.empty and nav_gdf is not None and not nav_gdf.empty:
            try:
                # Calculate POI attributes
                pois_processed = self.calculate_poi_attributes(pois_gdf, nav_gdf)

                # Save processed POIs as CSV
                output_path = DIR_POI / f"pois_{tile_id}.csv"
                columns_to_save = ["POI_ID", "LINK_ID", "PERCFRREF", "POI_ST_SD", "name", "category"]
                available_columns = [col for col in columns_to_save if col in pois_processed.columns]

                if available_columns:
                    pois_processed[available_columns].to_csv(output_path, index=False)
                    logger.info(f"‚úÖ Saved POI data: {len(pois_processed)} POIs")
                    success_flags['poi'] = True
                else:
                    # Create empty file
                    output_path.touch()
                    logger.info("Created empty POI file")
            except Exception as e:
                logger.error(f"Error processing POIs: {e}")
                # Create empty file
                output_path = DIR_POI / f"pois_{tile_id}.csv"
                output_path.touch()
        else:
            # Create empty POI file
            output_path = DIR_POI / f"pois_{tile_id}.csv"
            output_path.touch()
            logger.info("Created empty POI file")

        return any(success_flags.values())

def main():
    """Main execution function."""
    logger.info("üÜì Starting Singapore GeoData Download (FREE OSM Version)")
    logger.info("üìç Downloading data for Singapore using OpenStreetMap")

    processor = OSMDataProcessor()

    # Create tile grid
    tiles = processor.create_tile_grid()
    logger.info(f"Processing {len(tiles)} tiles covering Singapore")

    success_count = 0
    failed_tiles = []

    # Process each tile
    for tile in tqdm(tiles, desc="Processing tiles"):
        try:
            if processor.process_tile(tile):
                success_count += 1
                logger.info(f"‚úÖ Successfully processed tile {tile['tile_id']}")
            else:
                failed_tiles.append(tile['tile_id'])
                logger.error(f"‚ùå Failed to process tile {tile['tile_id']}")
        except Exception as e:
            failed_tiles.append(tile['tile_id'])
            logger.error(f"‚ùå Error processing tile {tile['tile_id']}: {e}")

    # Summary
    logger.info(f"\nüìä DOWNLOAD SUMMARY")
    logger.info(f"Successful: {success_count}/{len(tiles)} tiles")
    logger.info(f"Failed: {len(failed_tiles)} tiles")

    if failed_tiles:
        logger.error(f"Failed tiles: {failed_tiles}")

    if success_count > 0:
        logger.info(f"\n‚úÖ Data downloaded successfully! Files saved to:")
        logger.info(f"   üìÅ Navigation: {DIR_NAV}")
        logger.info(f"   üìÅ Naming: {DIR_NAMING}")
        logger.info(f"   üìÅ POIs: {DIR_POI}")
        logger.info(f"\nüöÄ Ready for GeoFix295! Try:")
        logger.info(f"   python Main.py --mode test   # quick check")
        logger.info(f"   python Main.py --mode full   # process all tiles")
        logger.info(f"\nüÜì All data sourced from OpenStreetMap - completely free!")
    else:
        logger.error("‚ùå No tiles were successfully processed!")
        logger.error("Check your internet connection and try again.")

if __name__ == "__main__":
    # Install required packages if needed
    try:
        import osmnx
        import geopandas
        import pyproj
    except ImportError as e:
        print("‚ùå Missing required packages. Install them with:")
        print("pip install osmnx geopandas pyproj tqdm pandas numpy requests")
        print("Note: This may take a few minutes as it installs spatial libraries.")
        exit(1)

    main()

from dbfread import DBF, FieldParser

class SafeFieldParser(FieldParser):
    def parseF(self, field, data):
        try:
            return float(data)
        except:
            return None  # or a default value like 0.0

# Load DBF with the custom parser
table = DBF("/content/Streets.dbf", encoding='latin-1', parserclass=SafeFieldParser)

# Convert to DataFrame
df = pd.DataFrame(iter(table))

# Save to CSV
df.to_csv("output_streets.csv", index=False)

import pandas as pd
import geopandas as gpd
from shapely import wkt

# Load CSV
streets_df = pd.read_csv("/content/output_streets.csv")

# Function to safely parse WKT
def safe_wkt_load(val):
    try:
        if isinstance(val, str) and val.strip().upper().startswith(('LINESTRING', 'MULTILINESTRING', 'POINT')):
            return wkt.loads(val)
    except:
        return None

# Apply only to valid geometries
streets_df['geometry'] = streets_df['ENH_GEOM'].apply(safe_wkt_load)

# Drop rows with invalid or missing geometries
streets_df = streets_df[streets_df['geometry'].notnull()]

# Convert to GeoDataFrame
streets_gdf = gpd.GeoDataFrame(streets_df, geometry='geometry', crs="EPSG:4326")

import pandas as pd
import geopandas as gpd
from shapely import wkt

poi_df = pd.read_csv('/content/output.csv')


# Parse WKT into geometry, then to GeoDataFrame (EPSG:4326)
streets_df['geometry'] = streets_df['ENH_GEOM'].apply(wkt.loads)
streets_gdf = gpd.GeoDataFrame(streets_df, geometry='geometry', crs="EPSG:4326")

poi_df['geometry'] = poi_df.apply(lambda row: Point(row.ROUTING_LO, row.ROUTING_LA), axis=1)
poi_gdf = gpd.GeoDataFrame(poi_df, geometry='geometry', crs="EPSG:4326")

"""Geometry Calculation (Snapping POIs to Roads)"""

from shapely.geometry import Point

def snap_point_to_line(poi_point, line_geom):
    # Compute distance along the line to the point
    dist_along = line_geom.project(poi_point)
    # Interpolate point on line at that distance
    return line_geom.interpolate(dist_along)

poi_gdf['snapped_geometry'] = None
for idx, poi in poi_gdf.iterrows():
    link_id = poi['NEAR_FID']
    link = streets_gdf[streets_gdf['LINK_ID']==link_id]
    if not link.empty:
        line = link.iloc[0].geometry
        orig_pt = poi.geometry
        snapped_pt = snap_point_to_line(orig_pt, line)
        poi_gdf.at[idx, 'snapped_geometry'] = snapped_pt
poi_gdf.set_geometry('snapped_geometry', inplace=True)

# Commented out IPython magic to ensure Python compatibility.
# %pip install rtree

from rtree import index

# Index road segments flagged multi-digit
multi_idx = [i for i, link in streets_gdf.iterrows() if link['MULTIDIGIT']=='Y']
idx = index.Index()
for i in multi_idx:
    idx.insert(i, streets_gdf.at[i, 'geometry'].bounds)

# Find pairs of multi-digit links with same name
pairs = []
for i in multi_idx:
    link1 = streets_gdf.loc[i]
    name1 = link1.ST_NAME
    geom1 = link1.geometry
    # query index for neighbors
    for j in idx.intersection(geom1.bounds):
        if j <= i:
            continue
        link2 = streets_gdf.loc[j]
        if link2['MULTIDIGIT']!='Y' or link2.ST_NAME!=name1:
            continue
        geom2 = link2.geometry
        # Check physical proximity (<50m)
        if geom1.distance(geom2) < 50/111000:
            pairs.append((i, j))

import shapely.ops as ops

violations = []
buffer_dist = 10/111000  # ~10 meters in degrees
for i, j in pairs:
    link1, link2 = streets_gdf.loc[i], streets_gdf.loc[j]
    buf_union = link1.geometry.buffer(buffer_dist).union(link2.geometry.buffer(buffer_dist))
    if buf_union.geom_type != 'Polygon':
        buf_union = buf_union.convex_hull
    # Check all POIs that reference either link
    pois = poi_gdf[poi_gdf['NEAR_FID'].isin([link1.LINK_ID, link2.LINK_ID])]
    for _, poi in pois.iterrows():
        poi_pt = poi.geometry
        if buf_union.contains(poi_pt):
            violations.append({'POI_ID': poi.POI_ID, 'link_i': i, 'link_j': j})

"""Scenario Classification"""

for violation in violations:
    poi = poi_gdf.loc[poi_gdf.POI_ID==violation['POI_ID']].iloc[0]
    poi_pt = poi.geometry
    link_id = poi.NEAR_FID
    # Find any other link within 10m
    nearby = streets_gdf[streets_gdf.geometry.distance(poi_pt) < 10/111000]
    correct_link = None
    for _, other in nearby.iterrows():
        if other.LINK_ID != link_id and other.geometry.distance(poi_pt) < 5/111000:
            correct_link = other.LINK_ID
            break
    if correct_link:
        violation['scenario'] = 2
        violation['action'] = f"Update LINK_ID to {correct_link}"
        continue
    # (else continue to next rules)

"""unset MULTIDIGIT"""

for violation in violations:
    if violation['scenario'] not in [1,2]:
        i,j = violation['link_i'], violation['link_j']
        violation['scenario'] = 3
        violation['action'] = f"Set MULTIDIGIT='N' for links {i}, {j}"

validations = []
for vio in violations:  # ‚Üê corrected from classified_violations
    poi_id = vio['POI_ID']
    scen = vio['scenario']
    action = vio['action']

    poi_idx = poi_gdf[poi_gdf.POI_ID == poi_id].index
    if poi_idx.empty:
        continue  # skip if POI not found

    poi_idx = poi_idx[0]
    poi_gdf.at[poi_idx, 'scenario'] = scen
    poi_gdf.at[poi_idx, 'action'] = action
    poi_gdf.at[poi_idx, 'EXCEPTION'] = 'Y' if scen == 4 else None

    if scen == 1:
        poi_gdf.drop(index=poi_idx, inplace=True)

    elif scen == 2:
        try:
            new_link = int(action.split()[-1])
            poi_gdf.at[poi_idx, 'NEAR_FID'] = new_link
        except Exception as e:
            print(f"Error parsing new LINK_ID for POI {poi_id}: {e}")

    elif scen == 3:
        # parse link IDs from action text
        ids = [int(s) for s in action.split() if s.isdigit()]
        for lid in ids:
            idxs = streets_gdf[streets_gdf['LINK_ID'] == lid].index
            if not idxs.empty:
                streets_gdf.at[idxs[0], 'MULTIDIGIT'] = 'N'

    validations.append({'POI_ID': str(poi_id), 'Action': action, 'scenario': scen})

validations_df = pd.DataFrame(validations)

poi_gdf['x'] = poi_gdf.geometry.x
poi_gdf['y'] = poi_gdf.geometry.y
poi_gdf.drop(columns=['geometry']).to_csv('corrected_pois.csv', index=False)

streets_gdf.to_file('corrected_streets.geojson', driver='GeoJSON')

import folium

# Base map centered on average POI location
avg_lat = poi_gdf['y'].mean()
avg_lon = poi_gdf['x'].mean()
m = folium.Map(location=[avg_lat, avg_lon], zoom_start=12)

# Define a color for each scenario
colors = {1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}
for _, row in poi_gdf.iterrows():
    scen = row['scenario']
    color = colors.get(scen, 'gray')
    popup = f"POI {row.POI_ID}: {row.action}"
    folium.CircleMarker(
        location=(row.y, row.x),
        radius=5,
        color=color,
        fill=True,
        fill_opacity=0.7,
        popup=popup
    ).add_to(m)

# Optionally, overlay street segments (GeoJSON) in gray
folium.GeoJson('corrected_streets.geojson',
               style_function=lambda feat: {'color': 'gray', 'weight': 2}
              ).add_to(m)

# Save the map
m.save('poi_corrections_map.html')

# Remove rows with missing or invalid geometry
poi_gdf = poi_gdf[poi_gdf.geometry.notnull()]
poi_gdf = poi_gdf[poi_gdf.geometry.apply(lambda g: g.is_valid if g else False)]

poi_gdf['x'] = poi_gdf.geometry.x
poi_gdf['y'] = poi_gdf.geometry.y

import geopandas as gpd
from shapely.geometry import Point

# Recreate geometry column from x/y or ROUTING_LO/ROUTING_LA
poi_gdf['geometry'] = poi_gdf.apply(
    lambda row: Point(row['ROUTING_LO'], row['ROUTING_LA']), axis=1
)

# Convert to GeoDataFrame
poi_gdf = gpd.GeoDataFrame(poi_gdf, geometry='geometry', crs="EPSG:4326")

print(poi_gdf.shape)
print(poi_gdf.columns)

import pandas as pd

poi_df = pd.read_csv("/content/output.csv")

import geopandas as gpd

shp_gdf = gpd.read_file("/content/Singapore_Prime_LAT.shp")
shp_gdf = shp_gdf.reset_index(drop=True)  # ensure clean index

import pandas as pd

csv_df = pd.read_csv("/content/output.csv")

assert len(csv_df) == len(shp_gdf), "Mismatch in number of rows between CSV and Shapefile"

import geopandas as gpd

poi_gdf = gpd.GeoDataFrame(csv_df, geometry='geometry', crs="EPSG:4326")

# Add x/y for mapping
poi_gdf['x'] = poi_gdf.geometry.x
poi_gdf['y'] = poi_gdf.geometry.y

import pandas as pd
import geopandas as gpd

# Step 1: Load your original CSV
csv_df = pd.read_csv("/content/output.csv")

# Step 2: Load your shapefile (which has only geometry)
shp_gdf = gpd.read_file("/content/Singapore_Prime_LAT.shp").reset_index(drop=True)

# Step 3: Add shapefile geometry to the CSV DataFrame (row-wise)
csv_df['geometry'] = shp_gdf['geometry'].values  # ‚Üê this is what you asked for

# Step 4: Convert to a GeoDataFrame
poi_gdf = gpd.GeoDataFrame(csv_df, geometry='geometry', crs="EPSG:4326")

# Step 5: Add x and y columns for plotting or folium use
poi_gdf['x'] = poi_gdf.geometry.x
poi_gdf['y'] = poi_gdf.geometry.y

# Check results
poi_gdf.to_csv("final_output_with_geometry.csv", index=False)

print(poi_gdf.shape)
print(poi_gdf.columns)

import folium

# Base map centered on average POI location
avg_lat = poi_gdf['y'].mean()
avg_lon = poi_gdf['x'].mean()
m = folium.Map(location=[avg_lat, avg_lon], zoom_start=12)

# Define a color for each scenario
colors = {1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}
for _, row in poi_gdf.iterrows():
    scen = row['scenario']
    color = colors.get(scen, 'gray')
    popup = f"POI {row.POI_ID}: {row.action}"
    folium.CircleMarker(
        location=(row.y, row.x),
        radius=5,
        color=color,
        fill=True,
        fill_opacity=0.7,
        popup=popup
    ).add_to(m)

# Optionally, overlay street segments (GeoJSON) in gray
folium.GeoJson('corrected_streets.geojson',
               style_function=lambda feat: {'color': 'gray', 'weight': 2}
              ).add_to(m)

# Save the map
m.save('poi_corrections_map.html')

import folium

# First, let's check what columns are available
print("Available columns in poi_gdf:")
print(poi_gdf.columns.tolist())

# Base map centered on average POI location
avg_lat = poi_gdf['y'].mean()
avg_lon = poi_gdf['x'].mean()
m = folium.Map(location=[avg_lat, avg_lon], zoom_start=12)

# Define colors for scenarios (if scenario column exists)
colors = {1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}

for _, row in poi_gdf.iterrows():
    # Check if 'scenario' column exists, otherwise use a default
    if 'scenario' in poi_gdf.columns:
        scen = row['scenario']
        color = colors.get(scen, 'gray')
    else:
        # If no scenario column, use a single color or create scenarios based on other criteria
        color = 'blue'  # default color

    # Create popup text - adjust based on available columns
    popup_parts = []
    if 'POI_ID' in poi_gdf.columns:
        popup_parts.append(f"POI {row['POI_ID']}")
    if 'action' in poi_gdf.columns:
        popup_parts.append(f"{row['action']}")

    popup = ": ".join(popup_parts) if popup_parts else "POI"

    folium.CircleMarker(
        location=(row.y, row.x),
        radius=5,
        color=color,
        fill=True,
        fill_opacity=0.7,
        popup=popup
    ).add_to(m)

# Optionally, overlay street segments (GeoJSON) in gray
try:
    folium.GeoJson('corrected_streets.geojson',
                   style_function=lambda feat: {'color': 'gray', 'weight': 2}
                  ).add_to(m)
except FileNotFoundError:
    print("corrected_streets.geojson not found, skipping street overlay")

# Save the map
m.save('poi_corrections_map.html')
print("Map saved as 'poi_corrections_map.html'")

import os
import pandas as pd
import geopandas as gpd
import numpy as np
from shapely.geometry import Point, LineString
from shapely.ops import nearest_points, transform
from scipy.spatial import cKDTree
from rtree import index
import folium
import pyproj
from functools import partial
import warnings
from osgeo import gdal
warnings.filterwarnings('ignore')

# Set GDAL config to restore missing .shx files
gdal.SetConfigOption('SHAPE_RESTORE_SHX', 'YES')

class POIRoadAligner:
    def __init__(self):
        self.poi_gdf = None
        self.streets_gdf = None
        self.utm_crs = None
        self.original_crs = "EPSG:4326"  # Singapore uses WGS84

    def load_data(self, poi_csv_path, streets_csv_path, streets_shp_path=None):
        """Load POI CSV, Streets CSV, and optionally Streets Shapefile"""
        print("üìä Loading data...")

        # Load POI data
        poi_df = pd.read_csv(poi_csv_path)
        if 'geometry' in poi_df.columns:
            # Handle geometry column if exists
            from shapely import wkt
            poi_df['geometry'] = poi_df['geometry'].apply(wkt.loads)
            self.poi_gdf = gpd.GeoDataFrame(poi_df, geometry='geometry', crs=self.original_crs)
        else:
            # Create geometry from x,y columns
            geometry = [Point(xy) for xy in zip(poi_df['x'], poi_df['y'])]
            self.poi_gdf = gpd.GeoDataFrame(poi_df, geometry=geometry, crs=self.original_crs)

        # Load Streets CSV
        streets_df = pd.read_csv(streets_csv_path)

        # Try to load shapefile if provided
        if streets_shp_path:
            try:
                print("üîÑ Loading shapefile...")
                streets_shp = gpd.read_file(streets_shp_path)
                print("‚úÖ Shapefile loaded successfully")

                # Merge CSV attributes with Shapefile geometry on LINK_ID
                if 'LINK_ID' in streets_shp.columns and 'LINK_ID' in streets_df.columns:
                    self.streets_gdf = streets_shp.merge(streets_df, on='LINK_ID', how='inner', suffixes=('', '_csv'))
                else:
                    # If LINK_ID merge fails, use shapefile as-is
                    self.streets_gdf = streets_shp.copy()

            except Exception as e:
                print(f"‚ö†Ô∏è Shapefile load failed: {e}")
                print("üîÑ Using CSV geometry instead...")
                streets_shp = None

        # If no shapefile or shapefile failed, use CSV geometry
        if not streets_shp_path or 'streets_shp' not in locals() or streets_shp is None:
            if 'ENH_GEOM' in streets_df.columns:
                print("üìç Using ENH_GEOM from CSV as WKT geometry")
                from shapely import wkt
                streets_df['geometry'] = streets_df['ENH_GEOM'].apply(
                    lambda x: wkt.loads(x) if pd.notnull(x) else None
                )
                self.streets_gdf = gpd.GeoDataFrame(streets_df, geometry='geometry', crs=self.original_crs)
            else:
                raise Exception("No geometry source found. Please provide either working shapefile or ENH_GEOM in CSV")

        # Ensure proper CRS
        self.streets_gdf = self.streets_gdf.to_crs(self.original_crs)

        # Remove rows with null geometry
        self.streets_gdf = self.streets_gdf[self.streets_gdf.geometry.notnull()]

        # Determine best UTM zone for Singapore (UTM Zone 48N)
        self.utm_crs = "EPSG:32648"  # UTM Zone 48N for Singapore

        print(f"‚úÖ Loaded {len(self.poi_gdf)} POIs and {len(self.streets_gdf)} street segments")

    def create_spatial_index(self):
        """Create R-tree spatial index for efficient spatial queries"""
        print("üåç Creating spatial index...")
        self.spatial_idx = index.Index()

        for idx, street in self.streets_gdf.iterrows():
            self.spatial_idx.insert(idx, street.geometry.bounds)

    def find_nearest_road_with_geometry(self, poi_point, search_radius_m=50):
        """Find nearest road using advanced geometric analysis"""
        # Convert to UTM for accurate distance calculations
        poi_utm = gpd.GeoSeries([poi_point], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]

        # Buffer search area
        search_buffer = poi_utm.buffer(search_radius_m)
        search_buffer_wgs84 = gpd.GeoSeries([search_buffer], crs=self.utm_crs).to_crs(self.original_crs).iloc[0]

        # Find candidate roads within buffer
        candidates = []
        for idx in self.spatial_idx.intersection(search_buffer_wgs84.bounds):
            street = self.streets_gdf.iloc[idx]
            street_utm = gpd.GeoSeries([street.geometry], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]

            # Calculate precise distance
            distance = poi_utm.distance(street_utm)
            if distance <= search_radius_m:
                candidates.append({
                    'idx': idx,
                    'distance': distance,
                    'street': street,
                    'street_utm': street_utm
                })

        if not candidates:
            return None

        # Return closest road
        return min(candidates, key=lambda x: x['distance'])

    def calculate_position_along_road(self, poi_point, road_geometry):
        """Calculate percentage position along road and side of street"""
        # Convert to UTM for accurate calculations
        poi_utm = gpd.GeoSeries([poi_point], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]
        road_utm = gpd.GeoSeries([road_geometry], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]

        # Find closest point on road centerline
        closest_point_on_road = nearest_points(poi_utm, road_utm)[1]

        # Calculate position along road (0.0 to 1.0)
        if road_utm.geom_type == 'LineString':
            position_along = road_utm.project(closest_point_on_road, normalized=True)
        else:
            position_along = 0.5  # Default for complex geometries

        # Determine side of street using cross product
        side = self.determine_side_of_street(poi_utm, road_utm, closest_point_on_road)

        return position_along, side, closest_point_on_road

    def determine_side_of_street(self, poi_point, road_line, closest_point):
        """Determine if POI is on left (L) or right (R) side of street"""
        if road_line.geom_type != 'LineString':
            return 'R'  # Default

        coords = list(road_line.coords)

        # Find the segment containing the closest point
        min_dist = float('inf')
        segment_idx = 0

        for i in range(len(coords) - 1):
            seg_start = Point(coords[i])
            seg_end = Point(coords[i + 1])
            segment = LineString([seg_start, seg_end])
            dist = segment.distance(closest_point)

            if dist < min_dist:
                min_dist = dist
                segment_idx = i

        # Get segment direction vector
        start_coord = coords[segment_idx]
        end_coord = coords[segment_idx + 1]

        # Calculate cross product to determine side
        dx = end_coord[0] - start_coord[0]
        dy = end_coord[1] - start_coord[1]

        # Vector from segment start to POI
        poi_dx = poi_point.x - start_coord[0]
        poi_dy = poi_point.y - start_coord[1]

        # Cross product (negative = left, positive = right in typical coordinate systems)
        cross_product = dx * poi_dy - dy * poi_dx

        return 'L' if cross_product < 0 else 'R'

    def calculate_optimal_poi_position(self, road_geometry, position_along, side, offset_meters=8):
        """Calculate optimal POI position offset from road centerline"""
        # Convert to UTM for accurate offset calculations
        road_utm = gpd.GeoSeries([road_geometry], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]

        if road_utm.geom_type != 'LineString':
            return road_geometry.centroid

        # Get point along road centerline
        point_on_road = road_utm.interpolate(position_along, normalized=True)

        # Calculate perpendicular offset
        offset_point = self.calculate_perpendicular_offset(road_utm, position_along, offset_meters, side)

        # Convert back to WGS84
        offset_point_wgs84 = gpd.GeoSeries([offset_point], crs=self.utm_crs).to_crs(self.original_crs).iloc[0]

        return offset_point_wgs84

    def calculate_perpendicular_offset(self, road_line, position_along, offset_meters, side):
        """Calculate perpendicular offset from road centerline"""
        coords = list(road_line.coords)

        # Get point on line
        point_on_line = road_line.interpolate(position_along, normalized=True)

        # Find nearest segment to calculate perpendicular direction
        min_dist = float('inf')
        best_segment = None

        for i in range(len(coords) - 1):
            segment = LineString([coords[i], coords[i + 1]])
            dist = segment.distance(point_on_line)
            if dist < min_dist:
                min_dist = dist
                best_segment = (coords[i], coords[i + 1])

        if best_segment is None:
            return point_on_line

        # Calculate perpendicular direction
        dx = best_segment[1][0] - best_segment[0][0]
        dy = best_segment[1][1] - best_segment[0][1]

        # Normalize direction vector
        length = np.sqrt(dx**2 + dy**2)
        if length == 0:
            return point_on_line

        dx_norm = dx / length
        dy_norm = dy / length

        # Perpendicular vector (rotate 90 degrees)
        perp_dx = -dy_norm
        perp_dy = dx_norm

        # Apply side multiplier
        side_multiplier = -1 if side == 'L' else 1

        # Calculate offset point
        offset_x = point_on_line.x + (perp_dx * offset_meters * side_multiplier)
        offset_y = point_on_line.y + (perp_dy * offset_meters * side_multiplier)

        return Point(offset_x, offset_y)

    def process_poi_alignment(self, building_offset_meters=8):
        """Main processing function to align POIs with roads"""
        print("üîÑ Processing POI alignment...")

        self.create_spatial_index()

        results = []
        processed_count = 0

        for idx, poi in self.poi_gdf.iterrows():
            processed_count += 1
            if processed_count % 100 == 0:
                print(f"   Processed {processed_count}/{len(self.poi_gdf)} POIs")

            # Find nearest road
            nearest_road = self.find_nearest_road_with_geometry(poi.geometry)

            if nearest_road is None:
                # Keep original position if no road found
                results.append({
                    'OBJECTID': poi.get('OBJECTID', idx),
                    'STREET_NAM': poi.get('STREET_NAM', ''),
                    'original_geometry': poi.geometry,
                    'corrected_geometry': poi.geometry,
                    'LINK_ID': None,
                    'PERCFRREF': None,
                    'POI_ST_SD': None,
                    'distance_to_road': None,
                    'correction_applied': False,
                    'correction_distance_m': 0
                })
                continue

            street = nearest_road['street']

            # Calculate position along road and side
            position_along, side, closest_point = self.calculate_position_along_road(
                poi.geometry, street.geometry
            )

            # Calculate optimal POI position
            corrected_position = self.calculate_optimal_poi_position(
                street.geometry, position_along, side, building_offset_meters
            )

            # Calculate correction distance
            original_utm = gpd.GeoSeries([poi.geometry], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]
            corrected_utm = gpd.GeoSeries([corrected_position], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]
            correction_distance = original_utm.distance(corrected_utm)

            results.append({
                'OBJECTID': poi.get('OBJECTID', idx),
                'STREET_NAM': poi.get('STREET_NAM', ''),
                'original_geometry': poi.geometry,
                'corrected_geometry': corrected_position,
                'LINK_ID': street['LINK_ID'],
                'PERCFRREF': position_along * 100,  # Convert to percentage
                'POI_ST_SD': side,
                'distance_to_road': nearest_road['distance'],
                'correction_applied': correction_distance > 1,  # Only if moved >1m
                'correction_distance_m': correction_distance
            })

        self.results_df = pd.DataFrame(results)
        print(f"‚úÖ Processed all POIs. Applied corrections to {sum(self.results_df['correction_applied'])} POIs")

    def create_corrected_geodataframe(self):
        """Create corrected POI GeoDataFrame"""
        # Merge original POI attributes with corrections
        corrected_gdf = self.poi_gdf.copy()

        # Add correction information
        for idx, result in self.results_df.iterrows():
            poi_idx = corrected_gdf[corrected_gdf.get('OBJECTID', corrected_gdf.index) == result['OBJECTID']].index
            if len(poi_idx) > 0:
                poi_idx = poi_idx[0]
                corrected_gdf.at[poi_idx, 'geometry'] = result['corrected_geometry']
                corrected_gdf.at[poi_idx, 'LINK_ID'] = result['LINK_ID']
                corrected_gdf.at[poi_idx, 'PERCFRREF'] = result['PERCFRREF']
                corrected_gdf.at[poi_idx, 'POI_ST_SD'] = result['POI_ST_SD']
                corrected_gdf.at[poi_idx, 'correction_applied'] = result['correction_applied']
                corrected_gdf.at[poi_idx, 'correction_distance_m'] = result['correction_distance_m']

        return corrected_gdf

    def save_results(self, output_dir='output'):
        """Save all results"""
        os.makedirs(output_dir, exist_ok=True)

        # Create corrected GeoDataFrame
        corrected_gdf = self.create_corrected_geodataframe()

        # Save corrected POIs
        corrected_gdf.to_file(f'{output_dir}/corrected_pois.geojson', driver='GeoJSON')
        corrected_gdf.to_file(f'{output_dir}/corrected_pois.shp')

        # Save analysis report
        self.results_df.to_csv(f'{output_dir}/poi_correction_report.csv', index=False)

        # Save streets with any updates
        self.streets_gdf.to_file(f'{output_dir}/streets_processed.geojson', driver='GeoJSON')

        print(f"üíæ Results saved to {output_dir}/")

        return corrected_gdf

    def create_visualization_map(self, output_dir='output', sample_size=500):
        """Create interactive Folium map showing corrections"""
        print("üó∫Ô∏è Creating visualization map...")

        corrected_gdf = self.create_corrected_geodataframe()

        # Sample data if too large
        if len(corrected_gdf) > sample_size:
            corrected_sample = corrected_gdf.sample(n=sample_size, random_state=42)
        else:
            corrected_sample = corrected_gdf

        # Calculate map center
        center_lat = corrected_sample.geometry.y.mean()
        center_lon = corrected_sample.geometry.x.mean()

        # Create map
        m = folium.Map(location=[center_lat, center_lon], zoom_start=12)

        # Add street network (sample)
        streets_sample = self.streets_gdf.sample(n=min(100, len(self.streets_gdf)), random_state=42)
        for _, street in streets_sample.iterrows():
            if street.geometry.geom_type == 'LineString':
                coords = [[point[1], point[0]] for point in street.geometry.coords]
                folium.PolyLine(
                    coords,
                    color='gray',
                    weight=2,
                    opacity=0.6
                ).add_to(m)

        # Add POIs with color coding
        for _, poi in corrected_sample.iterrows():
            if poi['correction_applied']:
                color = 'red'
                popup_text = f"POI {poi.get('OBJECTID', 'N/A')}<br>CORRECTED<br>Distance moved: {poi['correction_distance_m']:.1f}m<br>Street: {poi.get('STREET_NAM', 'N/A')}<br>Side: {poi.get('POI_ST_SD', 'N/A')}"
            else:
                color = 'green'
                popup_text = f"POI {poi.get('OBJECTID', 'N/A')}<br>NO CORRECTION NEEDED<br>Street: {poi.get('STREET_NAM', 'N/A')}"

            folium.CircleMarker(
                location=[poi.geometry.y, poi.geometry.x],
                radius=6,
                color=color,
                fill=True,
                fillOpacity=0.7,
                popup=folium.Popup(popup_text, max_width=300)
            ).add_to(m)

        # Add legend
        legend_html = '''
        <div style="position: fixed;
                    bottom: 50px; left: 50px; width: 150px; height: 90px;
                    background-color: white; border:2px solid grey; z-index:9999;
                    font-size:14px; padding: 10px">
        <p><b>POI Status</b></p>
        <p><i class="fa fa-circle" style="color:red"></i> Corrected</p>
        <p><i class="fa fa-circle" style="color:green"></i> No correction</p>
        </div>
        '''
        m.get_root().html.add_child(folium.Element(legend_html))

        # Save map
        map_path = f'{output_dir}/poi_corrections_map.html'
        m.save(map_path)
        print(f"üó∫Ô∏è Map saved to {map_path}")

        return m

# Simplified main function - you provide the paths manually
def run_poi_alignment(poi_csv_path, streets_csv_path, streets_shp_path=None):
    """
    Run POI alignment with manual file paths"""


    poi_csv = "/content/final_output_with_geometry.csv"
    streets_csv = "/content/output_streets.csv"



    # Initialize the aligner
    aligner = POIRoadAligner()

    try:
        # Load data
        aligner.load_data(poi_csv_path, streets_csv_path, streets_shp_path)

        # Process POI alignment (8 meters offset for building entrances)
        aligner.process_poi_alignment(building_offset_meters=8)

        # Save results
        corrected_gdf = aligner.save_results()

        # Create visualization
        aligner.create_visualization_map()

        # Print summary
        total_pois = len(corrected_gdf)
        corrected_pois = sum(corrected_gdf['correction_applied'])
        avg_correction = corrected_gdf[corrected_gdf['correction_applied']]['correction_distance_m'].mean()

        print(f"\nüìä SUMMARY:")
        print(f"   Total POIs processed: {total_pois}")
        print(f"   POIs corrected: {corrected_pois} ({corrected_pois/total_pois*100:.1f}%)")
        print(f"   Average correction distance: {avg_correction:.1f} meters")
        print(f"   Output files saved in 'output/' directory")

        return corrected_gdf

    except Exception as e:
        print(f"‚ùå Error: {e}")
        print("Please check your file paths and data format")
        return None

# Usage example:
# Replace with your actual file paths

poi_csv = "/content/final_output_with_geometry.csv"
streets_csv = "/content/output_streets.csv"
streets_shp = "/content/street"



result = run_poi_alignment(poi_csv, streets_csv, streets_shp)

import os
import pandas as pd
import geopandas as gpd
import numpy as np
from shapely.geometry import Point, LineString
from shapely.ops import nearest_points, transform
from scipy.spatial import cKDTree
from rtree import index
import folium
import pyproj
from functools import partial
import warnings
from osgeo import gdal
warnings.filterwarnings('ignore')

# Set GDAL config to restore missing .shx files
gdal.SetConfigOption('SHAPE_RESTORE_SHX', 'YES')

class POIRoadAligner:
    def __init__(self):
        self.poi_gdf = None
        self.streets_gdf = None
        self.utm_crs = None
        self.original_crs = "EPSG:4326"  # Singapore uses WGS84

    def load_data(self, poi_csv_path, streets_csv_path, streets_shp_path=None):
        """Load POI CSV, Streets CSV, and optionally Streets Shapefile"""
        print("üìä Loading data...")

        # Load POI data
        poi_df = pd.read_csv(poi_csv_path)
        if 'geometry' in poi_df.columns:
            # Handle geometry column if exists
            from shapely import wkt
            poi_df['geometry'] = poi_df['geometry'].apply(wkt.loads)
            self.poi_gdf = gpd.GeoDataFrame(poi_df, geometry='geometry', crs=self.original_crs)
        else:
            # Create geometry from x,y columns
            geometry = [Point(xy) for xy in zip(poi_df['x'], poi_df['y'])]
            self.poi_gdf = gpd.GeoDataFrame(poi_df, geometry=geometry, crs=self.original_crs)

        # Load Streets CSV
        streets_df = pd.read_csv(streets_csv_path)

        # Try to load shapefile if provided
        if streets_shp_path:
            try:
                print("üîÑ Loading shapefile...")
                streets_shp = gpd.read_file(streets_shp_path)
                print("‚úÖ Shapefile loaded successfully")

                # Merge CSV attributes with Shapefile geometry on LINK_ID
                if 'LINK_ID' in streets_shp.columns and 'LINK_ID' in streets_df.columns:
                    self.streets_gdf = streets_shp.merge(streets_df, on='LINK_ID', how='inner', suffixes=('', '_csv'))
                else:
                    # If LINK_ID merge fails, use shapefile as-is
                    self.streets_gdf = streets_shp.copy()

            except Exception as e:
                print(f"‚ö†Ô∏è Shapefile load failed: {e}")
                print("üîÑ Using CSV geometry instead...")
                streets_shp = None

        # If no shapefile or shapefile failed, use CSV geometry
        if not streets_shp_path or 'streets_shp' not in locals() or streets_shp is None:
            if 'ENH_GEOM' in streets_df.columns:
                print("üìç Using ENH_GEOM from CSV as WKT geometry")
                print("üîÑ Cleaning and parsing geometry data...")

                # Clean and parse WKT geometry with error handling
                def safe_wkt_load(wkt_string):
                    try:
                        if pd.isna(wkt_string) or wkt_string == '' or str(wkt_string).upper() == 'N':
                            return None
                        # Convert to string and clean
                        wkt_clean = str(wkt_string).strip()
                        if wkt_clean.upper() in ['N', 'NULL', 'NONE', '']:
                            return None
                        from shapely import wkt
                        return wkt.loads(wkt_clean)
                    except Exception as e:
                        print(f"   Warning: Failed to parse geometry: {str(wkt_string)[:50]}... Error: {e}")
                        return None

                streets_df['geometry'] = streets_df['ENH_GEOM'].apply(safe_wkt_load)

                # Count valid geometries
                valid_geom_count = streets_df['geometry'].notna().sum()
                total_count = len(streets_df)
                print(f"   Parsed {valid_geom_count}/{total_count} valid geometries")

                if valid_geom_count == 0:
                    raise Exception("No valid geometries found in ENH_GEOM column")

                self.streets_gdf = gpd.GeoDataFrame(streets_df, geometry='geometry', crs=self.original_crs)
            else:
                raise Exception("No geometry source found. Please provide either working shapefile or ENH_GEOM in CSV")

        # Ensure proper CRS
        self.streets_gdf = self.streets_gdf.to_crs(self.original_crs)

        # Remove rows with null geometry
        self.streets_gdf = self.streets_gdf[self.streets_gdf.geometry.notnull()]

        # Determine best UTM zone for Singapore (UTM Zone 48N)
        self.utm_crs = "EPSG:32648"  # UTM Zone 48N for Singapore

        print(f"‚úÖ Loaded {len(self.poi_gdf)} POIs and {len(self.streets_gdf)} street segments")

    def create_spatial_index(self):
        """Create R-tree spatial index for efficient spatial queries"""
        print("üåç Creating spatial index...")
        self.spatial_idx = index.Index()

        for idx, street in self.streets_gdf.iterrows():
            self.spatial_idx.insert(idx, street.geometry.bounds)

    def find_nearest_road_with_geometry(self, poi_point, search_radius_m=50):
        """Find nearest road using advanced geometric analysis"""
        # Convert to UTM for accurate distance calculations
        poi_utm = gpd.GeoSeries([poi_point], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]

        # Buffer search area
        search_buffer = poi_utm.buffer(search_radius_m)
        search_buffer_wgs84 = gpd.GeoSeries([search_buffer], crs=self.utm_crs).to_crs(self.original_crs).iloc[0]

        # Find candidate roads within buffer
        candidates = []
        for idx in self.spatial_idx.intersection(search_buffer_wgs84.bounds):
            street = self.streets_gdf.iloc[idx]
            street_utm = gpd.GeoSeries([street.geometry], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]

            # Calculate precise distance
            distance = poi_utm.distance(street_utm)
            if distance <= search_radius_m:
                candidates.append({
                    'idx': idx,
                    'distance': distance,
                    'street': street,
                    'street_utm': street_utm
                })

        if not candidates:
            return None

        # Return closest road
        return min(candidates, key=lambda x: x['distance'])

    def calculate_position_along_road(self, poi_point, road_geometry):
        """Calculate percentage position along road and side of street"""
        # Convert to UTM for accurate calculations
        poi_utm = gpd.GeoSeries([poi_point], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]
        road_utm = gpd.GeoSeries([road_geometry], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]

        # Find closest point on road centerline
        closest_point_on_road = nearest_points(poi_utm, road_utm)[1]

        # Calculate position along road (0.0 to 1.0)
        if road_utm.geom_type == 'LineString':
            position_along = road_utm.project(closest_point_on_road, normalized=True)
        else:
            position_along = 0.5  # Default for complex geometries

        # Determine side of street using cross product
        side = self.determine_side_of_street(poi_utm, road_utm, closest_point_on_road)

        return position_along, side, closest_point_on_road

    def determine_side_of_street(self, poi_point, road_line, closest_point):
        """Determine if POI is on left (L) or right (R) side of street"""
        if road_line.geom_type != 'LineString':
            return 'R'  # Default

        coords = list(road_line.coords)

        # Find the segment containing the closest point
        min_dist = float('inf')
        segment_idx = 0

        for i in range(len(coords) - 1):
            seg_start = Point(coords[i])
            seg_end = Point(coords[i + 1])
            segment = LineString([seg_start, seg_end])
            dist = segment.distance(closest_point)

            if dist < min_dist:
                min_dist = dist
                segment_idx = i

        # Get segment direction vector
        start_coord = coords[segment_idx]
        end_coord = coords[segment_idx + 1]

        # Calculate cross product to determine side
        dx = end_coord[0] - start_coord[0]
        dy = end_coord[1] - start_coord[1]

        # Vector from segment start to POI
        poi_dx = poi_point.x - start_coord[0]
        poi_dy = poi_point.y - start_coord[1]

        # Cross product (negative = left, positive = right in typical coordinate systems)
        cross_product = dx * poi_dy - dy * poi_dx

        return 'L' if cross_product < 0 else 'R'

    def calculate_optimal_poi_position(self, road_geometry, position_along, side, offset_meters=8):
        """Calculate optimal POI position offset from road centerline"""
        # Convert to UTM for accurate offset calculations
        road_utm = gpd.GeoSeries([road_geometry], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]

        if road_utm.geom_type != 'LineString':
            return road_geometry.centroid

        # Get point along road centerline
        point_on_road = road_utm.interpolate(position_along, normalized=True)

        # Calculate perpendicular offset
        offset_point = self.calculate_perpendicular_offset(road_utm, position_along, offset_meters, side)

        # Convert back to WGS84
        offset_point_wgs84 = gpd.GeoSeries([offset_point], crs=self.utm_crs).to_crs(self.original_crs).iloc[0]

        return offset_point_wgs84

    def calculate_perpendicular_offset(self, road_line, position_along, offset_meters, side):
        """Calculate perpendicular offset from road centerline"""
        coords = list(road_line.coords)

        # Get point on line
        point_on_line = road_line.interpolate(position_along, normalized=True)

        # Find nearest segment to calculate perpendicular direction
        min_dist = float('inf')
        best_segment = None

        for i in range(len(coords) - 1):
            segment = LineString([coords[i], coords[i + 1]])
            dist = segment.distance(point_on_line)
            if dist < min_dist:
                min_dist = dist
                best_segment = (coords[i], coords[i + 1])

        if best_segment is None:
            return point_on_line

        # Calculate perpendicular direction
        dx = best_segment[1][0] - best_segment[0][0]
        dy = best_segment[1][1] - best_segment[0][1]

        # Normalize direction vector
        length = np.sqrt(dx**2 + dy**2)
        if length == 0:
            return point_on_line

        dx_norm = dx / length
        dy_norm = dy / length

        # Perpendicular vector (rotate 90 degrees)
        perp_dx = -dy_norm
        perp_dy = dx_norm

        # Apply side multiplier
        side_multiplier = -1 if side == 'L' else 1

        # Calculate offset point
        offset_x = point_on_line.x + (perp_dx * offset_meters * side_multiplier)
        offset_y = point_on_line.y + (perp_dy * offset_meters * side_multiplier)

        return Point(offset_x, offset_y)

    def process_poi_alignment(self, building_offset_meters=8):
        """Main processing function to align POIs with roads"""
        print("üîÑ Processing POI alignment...")

        self.create_spatial_index()

        results = []
        processed_count = 0

        for idx, poi in self.poi_gdf.iterrows():
            processed_count += 1
            if processed_count % 100 == 0:
                print(f"   Processed {processed_count}/{len(self.poi_gdf)} POIs")

            # Find nearest road
            nearest_road = self.find_nearest_road_with_geometry(poi.geometry)

            if nearest_road is None:
                # Keep original position if no road found
                results.append({
                    'OBJECTID': poi.get('OBJECTID', idx),
                    'STREET_NAM': poi.get('STREET_NAM', ''),
                    'original_geometry': poi.geometry,
                    'corrected_geometry': poi.geometry,
                    'LINK_ID': None,
                    'PERCFRREF': None,
                    'POI_ST_SD': None,
                    'distance_to_road': None,
                    'correction_applied': False,
                    'correction_distance_m': 0
                })
                continue

            street = nearest_road['street']

            # Calculate position along road and side
            position_along, side, closest_point = self.calculate_position_along_road(
                poi.geometry, street.geometry
            )

            # Calculate optimal POI position
            corrected_position = self.calculate_optimal_poi_position(
                street.geometry, position_along, side, building_offset_meters
            )

            # Calculate correction distance
            original_utm = gpd.GeoSeries([poi.geometry], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]
            corrected_utm = gpd.GeoSeries([corrected_position], crs=self.original_crs).to_crs(self.utm_crs).iloc[0]
            correction_distance = original_utm.distance(corrected_utm)

            results.append({
                'OBJECTID': poi.get('OBJECTID', idx),
                'STREET_NAM': poi.get('STREET_NAM', ''),
                'original_geometry': poi.geometry,
                'corrected_geometry': corrected_position,
                'LINK_ID': street['LINK_ID'],
                'PERCFRREF': position_along * 100,  # Convert to percentage
                'POI_ST_SD': side,
                'distance_to_road': nearest_road['distance'],
                'correction_applied': correction_distance > 1,  # Only if moved >1m
                'correction_distance_m': correction_distance
            })

        self.results_df = pd.DataFrame(results)
        print(f"‚úÖ Processed all POIs. Applied corrections to {sum(self.results_df['correction_applied'])} POIs")

    def create_corrected_geodataframe(self):
        """Create corrected POI GeoDataFrame"""
        # Merge original POI attributes with corrections
        corrected_gdf = self.poi_gdf.copy()

        # Add correction information
        for idx, result in self.results_df.iterrows():
            poi_idx = corrected_gdf[corrected_gdf.get('OBJECTID', corrected_gdf.index) == result['OBJECTID']].index
            if len(poi_idx) > 0:
                poi_idx = poi_idx[0]
                corrected_gdf.at[poi_idx, 'geometry'] = result['corrected_geometry']
                corrected_gdf.at[poi_idx, 'LINK_ID'] = result['LINK_ID']
                corrected_gdf.at[poi_idx, 'PERCFRREF'] = result['PERCFRREF']
                corrected_gdf.at[poi_idx, 'POI_ST_SD'] = result['POI_ST_SD']
                corrected_gdf.at[poi_idx, 'correction_applied'] = result['correction_applied']
                corrected_gdf.at[poi_idx, 'correction_distance_m'] = result['correction_distance_m']

        return corrected_gdf

    def save_results(self, output_dir='output'):
        """Save all results"""
        os.makedirs(output_dir, exist_ok=True)

        # Create corrected GeoDataFrame
        corrected_gdf = self.create_corrected_geodataframe()

        # Save corrected POIs
        corrected_gdf.to_file(f'{output_dir}/corrected_pois.geojson', driver='GeoJSON')
        corrected_gdf.to_file(f'{output_dir}/corrected_pois.shp')

        # Save analysis report
        self.results_df.to_csv(f'{output_dir}/poi_correction_report.csv', index=False)

        # Save streets with any updates
        self.streets_gdf.to_file(f'{output_dir}/streets_processed.geojson', driver='GeoJSON')

        print(f"üíæ Results saved to {output_dir}/")

        return corrected_gdf

    def create_visualization_map(self, output_dir='output', sample_size=500):
        """Create interactive Folium map showing corrections"""
        print("üó∫Ô∏è Creating visualization map...")

        corrected_gdf = self.create_corrected_geodataframe()

        # Sample data if too large
        if len(corrected_gdf) > sample_size:
            corrected_sample = corrected_gdf.sample(n=sample_size, random_state=42)
        else:
            corrected_sample = corrected_gdf

        # Calculate map center
        center_lat = corrected_sample.geometry.y.mean()
        center_lon = corrected_sample.geometry.x.mean()

        # Create map
        m = folium.Map(location=[center_lat, center_lon], zoom_start=12)

        # Add street network (sample)
        streets_sample = self.streets_gdf.sample(n=min(100, len(self.streets_gdf)), random_state=42)
        for _, street in streets_sample.iterrows():
            if street.geometry.geom_type == 'LineString':
                coords = [[point[1], point[0]] for point in street.geometry.coords]
                folium.PolyLine(
                    coords,
                    color='gray',
                    weight=2,
                    opacity=0.6
                ).add_to(m)

        # Add POIs with color coding
        for _, poi in corrected_sample.iterrows():
            if poi['correction_applied']:
                color = 'red'
                popup_text = f"POI {poi.get('OBJECTID', 'N/A')}<br>CORRECTED<br>Distance moved: {poi['correction_distance_m']:.1f}m<br>Street: {poi.get('STREET_NAM', 'N/A')}<br>Side: {poi.get('POI_ST_SD', 'N/A')}"
            else:
                color = 'green'
                popup_text = f"POI {poi.get('OBJECTID', 'N/A')}<br>NO CORRECTION NEEDED<br>Street: {poi.get('STREET_NAM', 'N/A')}"

            folium.CircleMarker(
                location=[poi.geometry.y, poi.geometry.x],
                radius=6,
                color=color,
                fill=True,
                fillOpacity=0.7,
                popup=folium.Popup(popup_text, max_width=300)
            ).add_to(m)

        # Add legend
        legend_html = '''
        <div style="position: fixed;
                    bottom: 50px; left: 50px; width: 150px; height: 90px;
                    background-color: white; border:2px solid grey; z-index:9999;
                    font-size:14px; padding: 10px">
        <p><b>POI Status</b></p>
        <p><i class="fa fa-circle" style="color:red"></i> Corrected</p>
        <p><i class="fa fa-circle" style="color:green"></i> No correction</p>
        </div>
        '''
        m.get_root().html.add_child(folium.Element(legend_html))

        # Save map
        map_path = f'{output_dir}/poi_corrections_map.html'
        m.save(map_path)
        print(f"üó∫Ô∏è Map saved to {map_path}")

        return m

# Simplified main function - you provide the paths manually
def run_poi_alignment(poi_csv_path, streets_csv_path, streets_shp_path=None):
    """
    Run POI alignment with manual file paths

    Parameters:
    - poi_csv_path: Path to POI CSV file
    - streets_csv_path: Path to Streets CSV file
    - streets_shp_path: Path to Streets shapefile (optional)
    """

    # Initialize the aligner
    aligner = POIRoadAligner()

    try:
        # Load data
        aligner.load_data(poi_csv_path, streets_csv_path, streets_shp_path)

        # Process POI alignment (8 meters offset for building entrances)
        aligner.process_poi_alignment(building_offset_meters=8)

        # Save results
        corrected_gdf = aligner.save_results()

        # Create visualization
        aligner.create_visualization_map()

        # Print summary
        total_pois = len(corrected_gdf)
        corrected_pois = sum(corrected_gdf['correction_applied'])
        avg_correction = corrected_gdf[corrected_gdf['correction_applied']]['correction_distance_m'].mean()

        print(f"\nüìä SUMMARY:")
        print(f"   Total POIs processed: {total_pois}")
        print(f"   POIs corrected: {corrected_pois} ({corrected_pois/total_pois*100:.1f}%)")
        print(f"   Average correction distance: {avg_correction:.1f} meters")
        print(f"   Output files saved in 'output/' directory")

        return corrected_gdf

    except Exception as e:
        print(f"‚ùå Error: {e}")
        print("Please check your file paths and data format")
        return None

# Diagnostic function to check your data
def check_data_quality(poi_csv_path, streets_csv_path):
    """
    Check the quality and format of your data files
    """
    print("üîç CHECKING DATA QUALITY...")

    # Check POI CSV
    print("\nüìä POI CSV Analysis:")
    try:
        poi_df = pd.read_csv(poi_csv_path)
        print(f"   Rows: {len(poi_df)}")
        print(f"   Columns: {list(poi_df.columns)}")

        # Check for coordinate columns
        coord_cols = ['x', 'y', 'X', 'Y', 'longitude', 'latitude', 'lon', 'lat']
        found_coords = [col for col in coord_cols if col in poi_df.columns]
        print(f"   Found coordinate columns: {found_coords}")

        if found_coords:
            # Check coordinate ranges
            for col in found_coords:
                if poi_df[col].dtype in ['float64', 'int64']:
                    print(f"   {col} range: {poi_df[col].min():.6f} to {poi_df[col].max():.6f}")
                    print(f"   {col} null values: {poi_df[col].isnull().sum()}")

    except Exception as e:
        print(f"   ‚ùå Error reading POI CSV: {e}")

    # Check Streets CSV
    print("\nüõ£Ô∏è Streets CSV Analysis:")
    try:
        streets_df = pd.read_csv(streets_csv_path)
        print(f"   Rows: {len(streets_df)}")
        print(f"   Columns: {list(streets_df.columns)}")

        # Check ENH_GEOM column
        if 'ENH_GEOM' in streets_df.columns:
            print(f"   ENH_GEOM null values: {streets_df['ENH_GEOM'].isnull().sum()}")

            # Sample unique values in ENH_GEOM
            unique_vals = streets_df['ENH_GEOM'].dropna().unique()[:5]
            print(f"   Sample ENH_GEOM values:")
            for i, val in enumerate(unique_vals):
                val_str = str(val)[:100] + "..." if len(str(val)) > 100 else str(val)
                print(f"     {i+1}: {val_str}")

            # Check for problematic values
            problem_vals = streets_df['ENH_GEOM'].value_counts().head(10)
            print(f"   Most common ENH_GEOM values:")
            for val, count in problem_vals.items():
                val_str = str(val)[:50] + "..." if len(str(val)) > 50 else str(val)
                print(f"     '{val_str}': {count} occurrences")

        else:
            print("   ‚ùå No ENH_GEOM column found")

    except Exception as e:
        print(f"   ‚ùå Error reading Streets CSV: {e}")

# Usage example with data checking:

check_data_quality("/content/final_output_with_geometry.csv", "/content/output_streets.csv")

poi_csv = "/content/final_output_with_geometry.csv"
streets_csv = "/content/output_streets.csv"
streets_shp = "/content/street"

result = run_poi_alignment(poi_csv, streets_csv, streets_shp)

import pandas as pd
from shapely import wkt

# Load your CSV (replace with your actual file path)
df = pd.read_csv("/content/output_poi.csv")

# Assuming 'geometry' column contains LINESTRING
df['geometry'] = df['geometry'].apply(wkt.loads)  # Convert WKT to shapely object

# Extract the first coordinate (x=lon, y=lat) from the LINESTRING
df['x'] = df['geometry'].apply(lambda geom: geom.coords[0][0])  # Longitude
df['y'] = df['geometry'].apply(lambda geom: geom.coords[0][1])  # Latitude

# Save updated DataFrame with x, y columns
df.to_csv("poi_with_latlong.csv", index=False)

import pandas as pd
import geopandas as gpd
import numpy as np
from shapely.geometry import Point, LineString, Polygon
from shapely.ops import nearest_points
import folium
from folium.plugins import HeatMap, MarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class POICorrector:
    """
    Multi-layered POI correction system with spatial analysis, ML, and Kalman filtering
    """

    def __init__(self, poi_data, streets_data, crs_source='EPSG:4326', crs_target='EPSG:3414'):
        """
        Initialize the POI correction system

        Args:
            poi_data: GeoDataFrame of POI points
            streets_data: GeoDataFrame of street lines
            crs_source: Source coordinate system (WGS84)
            crs_target: Target coordinate system (SVY21 for Singapore)
        """
        self.poi_gdf = poi_data.to_crs(crs_target) if poi_data.crs != crs_target else poi_data
        self.streets_gdf = streets_data.to_crs(crs_target) if streets_data.crs != crs_target else streets_data
        self.crs_source = crs_source
        self.crs_target = crs_target

        # Results storage
        self.analysis_results = {}
        self.corrections = []
        self.confidence_scores = {}

    def layer1_spatial_analysis(self, min_distance=5, max_distance=7):
        """
        Layer 1: Traditional spatial analysis to identify misplaced POIs
        """
        print("üîç Layer 1: Spatial Analysis...")

        # Create spatial index for efficient queries
        spatial_index = self.streets_gdf.sindex

        misplaced_pois = []

        for idx, poi in self.poi_gdf.iterrows():
            poi_point = poi.geometry

            # Find nearby streets using spatial index
            possible_matches_index = list(spatial_index.intersection(poi_point.buffer(max_distance).bounds))
            possible_matches = self.streets_gdf.iloc[possible_matches_index]

            if len(possible_matches) == 0:
                continue

            # Calculate distances to all nearby streets
            distances = possible_matches.geometry.distance(poi_point)
            min_dist = distances.min()
            closest_street_idx = distances.idxmin()

            # Identify misplaced POIs
            if min_distance <= min_dist <= max_distance:
                # POI is in the suspicious range
                closest_street = self.streets_gdf.loc[closest_street_idx]

                # Calculate snap point on closest street
                snap_point = self._snap_to_roadside(poi_point, closest_street.geometry)

                misplaced_pois.append({
                    'poi_idx': idx,
                    'original_point': poi_point,
                    'closest_street_idx': closest_street_idx,
                    'distance_to_road': min_dist,
                    'suggested_point': snap_point,
                    'confidence': self._calculate_spatial_confidence(min_dist, min_distance, max_distance),
                    'street_name': closest_street.get('ST_NAME', 'Unknown'),
                    'method': 'spatial_analysis'
                })

        self.analysis_results['layer1'] = misplaced_pois
        print(f"‚úì Found {len(misplaced_pois)} potentially misplaced POIs")
        return misplaced_pois

    def layer2_ml_anomaly_detection(self):
        """
        Layer 2: Machine Learning anomaly detection for POI placement patterns
        """
        print("ü§ñ Layer 2: ML Anomaly Detection...")

        # Extract features for each POI
        features = []
        poi_indices = []

        for idx, poi in self.poi_gdf.iterrows():
            poi_point = poi.geometry

            # Find nearby streets
            nearby_streets = self.streets_gdf[self.streets_gdf.geometry.distance(poi_point) <= 50]

            if len(nearby_streets) == 0:
                continue

            # Feature engineering
            distances = nearby_streets.geometry.distance(poi_point)

            feature_vector = [
                distances.min(),  # Distance to closest road
                distances.mean(), # Average distance to nearby roads
                distances.std() if len(distances) > 1 else 0,  # Std dev of distances
                len(nearby_streets),  # Number of nearby streets
                poi_point.x,  # X coordinate (for spatial patterns)
                poi_point.y,  # Y coordinate (for spatial patterns)
            ]

            # Add street density around POI
            buffer_area = poi_point.buffer(100)  # 100m buffer
            streets_in_buffer = self.streets_gdf[self.streets_gdf.geometry.intersects(buffer_area)]
            total_street_length = streets_in_buffer.geometry.length.sum()
            street_density = total_street_length / buffer_area.area

            feature_vector.append(street_density)

            features.append(feature_vector)
            poi_indices.append(idx)

        if len(features) == 0:
            self.analysis_results['layer2'] = []
            return []

        # Normalize features
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)

        # Apply Isolation Forest for anomaly detection
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        anomaly_labels = iso_forest.fit_predict(features_scaled)
        anomaly_scores = iso_forest.score_samples(features_scaled)

        # Apply DBSCAN for spatial clustering
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        cluster_labels = dbscan.fit_predict(features_scaled[:, [4, 5]])  # Use only x, y coordinates

        # Identify anomalous POIs
        anomalous_pois = []
        for i, (poi_idx, anomaly_label, anomaly_score, cluster_label) in enumerate(
            zip(poi_indices, anomaly_labels, anomaly_scores, cluster_labels)
        ):
            if anomaly_label == -1:  # Anomaly detected
                poi = self.poi_gdf.loc[poi_idx]

                # Find correction suggestion
                closest_street_idx = self.streets_gdf.geometry.distance(poi.geometry).idxmin()
                closest_street = self.streets_gdf.loc[closest_street_idx]
                suggested_point = self._snap_to_roadside(poi.geometry, closest_street.geometry)

                anomalous_pois.append({
                    'poi_idx': poi_idx,
                    'original_point': poi.geometry,
                    'closest_street_idx': closest_street_idx,
                    'distance_to_road': closest_street.geometry.distance(poi.geometry),
                    'suggested_point': suggested_point,
                    'confidence': abs(anomaly_score),  # Higher absolute score = higher confidence
                    'cluster_id': cluster_label,
                    'street_name': closest_street.get('ST_NAME', 'Unknown'),
                    'method': 'ml_anomaly'
                })

        self.analysis_results['layer2'] = anomalous_pois
        print(f"‚úì ML detected {len(anomalous_pois)} anomalous POIs")
        return anomalous_pois

    def layer3_kalman_filtering(self, process_noise=0.1, measurement_noise=1.0):
        """
        Layer 3: Kalman filtering for trajectory smoothing and position refinement
        """
        print("üîÑ Layer 3: Kalman Filtering...")

        # Combine results from previous layers
        all_corrections = self.analysis_results.get('layer1', []) + self.analysis_results.get('layer2', [])

        if len(all_corrections) == 0:
            self.analysis_results['layer3'] = []
            return []

        # Group corrections by street or spatial proximity
        corrected_pois = []

        for correction in all_corrections:
            # Simple Kalman filter implementation for position refinement
            original_pos = np.array([correction['original_point'].x, correction['original_point'].y])
            suggested_pos = np.array([correction['suggested_point'].x, correction['suggested_point'].y])

            # Initialize Kalman filter
            # State: [x, y, vx, vy] (position and velocity)
            state = np.array([original_pos[0], original_pos[1], 0, 0])

            # Process noise covariance
            Q = np.eye(4) * process_noise
            Q[2:, 2:] *= 0.1  # Lower process noise for velocity

            # Measurement noise covariance
            R = np.eye(2) * measurement_noise

            # State transition matrix (constant velocity model)
            F = np.array([[1, 0, 1, 0],
                         [0, 1, 0, 1],
                         [0, 0, 1, 0],
                         [0, 0, 0, 1]])

            # Measurement matrix
            H = np.array([[1, 0, 0, 0],
                         [0, 1, 0, 0]])

            # Initial covariance
            P = np.eye(4) * 10

            # Kalman filter steps
            # Predict
            state_pred = F @ state
            P_pred = F @ P @ F.T + Q

            # Update with measurement (suggested position)
            y = suggested_pos - H @ state_pred  # Innovation
            S = H @ P_pred @ H.T + R  # Innovation covariance
            K = P_pred @ H.T @ np.linalg.inv(S)  # Kalman gain

            state_updated = state_pred + K @ y
            P_updated = (np.eye(4) - K @ H) @ P_pred

            # Extract refined position
            refined_pos = state_updated[:2]
            refined_point = Point(refined_pos[0], refined_pos[1])

            # Calculate confidence based on covariance
            position_uncertainty = np.sqrt(P_updated[0, 0] + P_updated[1, 1])
            kalman_confidence = 1.0 / (1.0 + position_uncertainty)

            corrected_pois.append({
                **correction,
                'kalman_refined_point': refined_point,
                'kalman_confidence': kalman_confidence,
                'position_uncertainty': position_uncertainty,
                'method': f"{correction['method']}_kalman"
            })

        self.analysis_results['layer3'] = corrected_pois
        print(f"‚úì Kalman refined {len(corrected_pois)} POI corrections")
        return corrected_pois

    def layer4_consensus_validation(self, min_consensus=2):
        """
        Layer 4: Consensus validation - combine results from all layers
        """
        print("üéØ Layer 4: Consensus Validation...")

        # Collect all corrections
        all_corrections = {}

        # Process each layer's results
        for layer_name, corrections in self.analysis_results.items():
            for correction in corrections:
                poi_idx = correction['poi_idx']
                if poi_idx not in all_corrections:
                    all_corrections[poi_idx] = []
                all_corrections[poi_idx].append(correction)

        # Find consensus corrections
        consensus_corrections = []

        for poi_idx, corrections in all_corrections.items():
            if len(corrections) >= min_consensus:
                # Calculate weighted average of suggested positions
                total_weight = 0
                weighted_x = 0
                weighted_y = 0

                for correction in corrections:
                    # Get the best available point (Kalman refined or original suggestion)
                    if 'kalman_refined_point' in correction:
                        point = correction['kalman_refined_point']
                        confidence = correction['kalman_confidence']
                    else:
                        point = correction['suggested_point']
                        confidence = correction['confidence']

                    weighted_x += point.x * confidence
                    weighted_y += point.y * confidence
                    total_weight += confidence

                if total_weight > 0:
                    consensus_x = weighted_x / total_weight
                    consensus_y = weighted_y / total_weight
                    consensus_point = Point(consensus_x, consensus_y)

                    # Calculate overall confidence
                    overall_confidence = total_weight / len(corrections)

                    consensus_corrections.append({
                        'poi_idx': poi_idx,
                        'original_point': corrections[0]['original_point'],
                        'consensus_point': consensus_point,
                        'overall_confidence': overall_confidence,
                        'num_methods_agree': len(corrections),
                        'contributing_methods': [c['method'] for c in corrections],
                        'street_name': corrections[0]['street_name']
                    })

        self.analysis_results['consensus'] = consensus_corrections
        print(f"‚úì {len(consensus_corrections)} POIs passed consensus validation")
        return consensus_corrections

    def _snap_to_roadside(self, poi_point, street_line, offset_distance=3):
        """
        Snap POI to roadside with specified offset
        """
        # Find closest point on street
        closest_point = nearest_points(poi_point, street_line)[1]

        # Calculate perpendicular offset
        if hasattr(street_line, 'coords'):
            coords = list(street_line.coords)
        else:
            coords = [closest_point.coords[0]]

        if len(coords) < 2:
            return closest_point

        # Find the closest segment
        min_dist = float('inf')
        best_segment = None

        for i in range(len(coords) - 1):
            segment = LineString([coords[i], coords[i + 1]])
            dist = segment.distance(poi_point)
            if dist < min_dist:
                min_dist = dist
                best_segment = segment

        if best_segment is None:
            return closest_point

        # Calculate perpendicular direction
        p1, p2 = best_segment.coords[0], best_segment.coords[1]
        dx = p2[0] - p1[0]
        dy = p2[1] - p1[1]
        length = np.sqrt(dx**2 + dy**2)

        if length == 0:
            return closest_point

        # Normalize and rotate 90 degrees
        perp_x = -dy / length
        perp_y = dx / length

        # Determine which side to offset to (closest side)
        offset_point1 = Point(closest_point.x + perp_x * offset_distance,
                             closest_point.y + perp_y * offset_distance)
        offset_point2 = Point(closest_point.x - perp_x * offset_distance,
                             closest_point.y - perp_y * offset_distance)

        # Choose the offset point closer to original POI
        if offset_point1.distance(poi_point) < offset_point2.distance(poi_point):
            return offset_point1
        else:
            return offset_point2

    def _calculate_spatial_confidence(self, distance, min_dist, max_dist):
        """
        Calculate confidence score based on distance from road
        """
        if distance <= min_dist:
            return 0.9  # High confidence - clearly misplaced
        elif distance >= max_dist:
            return 0.1  # Low confidence - might be legitimate
        else:
            # Linear interpolation between min and max
            return 0.9 - 0.8 * (distance - min_dist) / (max_dist - min_dist)

    def run_full_analysis(self):
        """
        Run all layers of analysis
        """
        print("üöÄ Starting Multi-Layer POI Correction Analysis...")

        # Layer 1: Spatial Analysis
        self.layer1_spatial_analysis()

        # Layer 2: ML Anomaly Detection
        self.layer2_ml_anomaly_detection()

        # Layer 3: Kalman Filtering
        self.layer3_kalman_filtering()

        # Layer 4: Consensus Validation
        final_corrections = self.layer4_consensus_validation()

        print(f"\n‚úÖ Analysis Complete!")
        print(f"üìä Summary:")
        print(f"   - Layer 1 (Spatial): {len(self.analysis_results.get('layer1', []))} detections")
        print(f"   - Layer 2 (ML): {len(self.analysis_results.get('layer2', []))} detections")
        print(f"   - Layer 3 (Kalman): {len(self.analysis_results.get('layer3', []))} refinements")
        print(f"   - Final Consensus: {len(final_corrections)} corrections")

        return final_corrections

    def create_visualization(self, output_file='poi_corrections_map.html'):
        """
        Create interactive map showing all corrections
        """
        print(f"üó∫Ô∏è Creating visualization: {output_file}")

        # Convert back to WGS84 for mapping
        poi_wgs84 = self.poi_gdf.to_crs('EPSG:4326')
        streets_wgs84 = self.streets_gdf.to_crs('EPSG:4326')

        # Get consensus corrections
        corrections = self.analysis_results.get('consensus', [])

        if len(corrections) == 0:
            print("No corrections to visualize")
            return

        # Calculate map center
        all_points = [poi_wgs84.iloc[c['poi_idx']].geometry for c in corrections]
        center_lat = np.mean([p.y for p in all_points])
        center_lon = np.mean([p.x for p in all_points])

        # Create map
        m = folium.Map(location=[center_lat, center_lon], zoom_start=13)

        # Add street network
        folium.GeoJson(
            streets_wgs84,
            style_function=lambda x: {
                'color': 'gray',
                'weight': 2,
                'opacity': 0.7
            },
            name='Street Network'
        ).add_to(m)

        # Add corrections
        for correction in corrections:
            poi_idx = correction['poi_idx']
            original_poi = poi_wgs84.iloc[poi_idx]

            # Convert consensus point back to WGS84
            consensus_point_svg = correction['consensus_point']
            consensus_gdf = gpd.GeoDataFrame([1], geometry=[consensus_point_svg], crs='EPSG:3414')
            consensus_wgs84 = consensus_gdf.to_crs('EPSG:4326').iloc[0].geometry

            # Original position (red)
            folium.Marker(
                location=[original_poi.geometry.y, original_poi.geometry.x],
                icon=folium.Icon(color='red', icon='exclamation-sign'),
                popup=f"""
                <b>Original POI</b><br>
                Street: {original_poi.get('STREET_NAM', 'Unknown')}<br>
                Confidence: {correction['overall_confidence']:.2f}<br>
                Methods: {', '.join(correction['contributing_methods'])}
                """
            ).add_to(m)

            # Corrected position (green)
            folium.Marker(
                location=[consensus_wgs84.y, consensus_wgs84.x],
                icon=folium.Icon(color='green', icon='ok-sign'),
                popup=f"""
                <b>Corrected POI</b><br>
                Street: {correction['street_name']}<br>
                Confidence: {correction['overall_confidence']:.2f}<br>
                Methods Agree: {correction['num_methods_agree']}
                """
            ).add_to(m)

            # Connection line
            folium.PolyLine(
                locations=[
                    [original_poi.geometry.y, original_poi.geometry.x],
                    [consensus_wgs84.y, consensus_wgs84.x]
                ],
                color='blue',
                weight=2,
                opacity=0.7
            ).add_to(m)

        # Add layer control
        folium.LayerControl().add_to(m)

        # Save map
        m.save(output_file)
        print(f"‚úÖ Map saved to {output_file}")

    def export_corrected_data(self, output_file='corrected_pois.csv'):
        """
        Export corrected POI data
        """
        corrections = self.analysis_results.get('consensus', [])

        if len(corrections) == 0:
            print("No corrections to export")
            return

        # Create corrected dataset
        corrected_pois = self.poi_gdf.copy()

        for correction in corrections:
            poi_idx = correction['poi_idx']
            consensus_point = correction['consensus_point']

            # Update geometry
            corrected_pois.loc[poi_idx, 'geometry'] = consensus_point

            # Add correction metadata
            corrected_pois.loc[poi_idx, 'CORRECTED'] = True
            corrected_pois.loc[poi_idx, 'CORRECTION_CONFIDENCE'] = correction['overall_confidence']
            corrected_pois.loc[poi_idx, 'CORRECTION_METHODS'] = ','.join(correction['contributing_methods'])

        # Convert back to WGS84 and export
        corrected_pois_wgs84 = corrected_pois.to_crs('EPSG:4326')
        corrected_pois_wgs84['x'] = corrected_pois_wgs84.geometry.x
        corrected_pois_wgs84['y'] = corrected_pois_wgs84.geometry.y

        # Export to CSV
        corrected_pois_wgs84.drop(columns=['geometry']).to_csv(output_file, index=False)
        print(f"‚úÖ Corrected POI data exported to {output_file}")

# Example usage function
def correct_singapore_pois(poi_csv_path, streets_shp_path):
    """
    Main function to correct Singapore POIs
    """
    print("üìç Loading Singapore POI correction system...")

    # Load data
    poi_df = pd.read_csv("/content/poi_with_latlong.csv")
    poi_gdf = gpd.GeoDataFrame(
        poi_df,
        geometry=gpd.points_from_xy(poi_df['x'], poi_df['y']),
        crs='EPSG:4326'
    )

    streets_gdf = gpd.read_file("/content/Streets.shp")

    # Initialize corrector
    corrector = POICorrector(poi_gdf, streets_gdf)

    # Run analysis
    corrections = corrector.run_full_analysis()

    # Create outputs
    corrector.create_visualization()
    corrector.export_corrected_data()

    return corrector, corrections

# Usage example:
corrector, corrections = correct_singapore_pois('pois.csv', 'streets.shp')

import pandas as pd
import geopandas as gpd
import numpy as np
from shapely.geometry import Point, LineString, Polygon
from shapely.ops import nearest_points
import folium
from folium.plugins import HeatMap, MarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class POICorrector:
    """
    Multi-layered POI correction system with spatial analysis, ML, and Kalman filtering
    """

    def __init__(self, poi_data, streets_data, crs_source='EPSG:4326', crs_target='EPSG:3414'):
        """
        Initialize the POI correction system

        Args:
            poi_data: GeoDataFrame of POI points
            streets_data: GeoDataFrame of street lines
            crs_source: Source coordinate system (WGS84)
            crs_target: Target coordinate system (SVY21 for Singapore)
        """
        # Ensure input GeoDataFrames have a CRS
        if poi_data.crs is None:
            print(f"‚ö†Ô∏è POI data has no CRS defined. Setting to {crs_source}")
            poi_data.set_crs(crs_source, inplace=True)
        if streets_data.crs is None:
            print(f"‚ö†Ô∏è Streets data has no CRS defined. Setting to {crs_source}")
            streets_data.set_crs(crs_source, inplace=True)

        # Transform to target CRS if needed
        self.poi_gdf = poi_data.to_crs(crs_target) if poi_data.crs != crs_target else poi_data
        self.streets_gdf = streets_data.to_crs(crs_target) if streets_data.crs != crs_target else streets_data
        self.crs_source = crs_source
        self.crs_target = crs_target

        # Results storage
        self.analysis_results = {}
        self.corrections = []
        self.confidence_scores = {}

    def layer1_spatial_analysis(self, min_distance=5, max_distance=7):
        """
        Layer 1: Traditional spatial analysis to identify misplaced POIs
        """
        print("üîç Layer 1: Spatial Analysis...")

        # Create spatial index for efficient queries
        spatial_index = self.streets_gdf.sindex

        misplaced_pois = []

        for idx, poi in self.poi_gdf.iterrows():
            poi_point = poi.geometry

            # Find nearby streets using spatial index
            possible_matches_index = list(spatial_index.intersection(poi_point.buffer(max_distance).bounds))
            possible_matches = self.streets_gdf.iloc[possible_matches_index]

            if len(possible_matches) == 0:
                continue

            # Calculate distances to all nearby streets
            distances = possible_matches.geometry.distance(poi_point)
            min_dist = distances.min()
            closest_street_idx = distances.idxmin()

            # Identify misplaced POIs
            if min_distance <= min_dist <= max_distance:
                # POI is in the suspicious range
                closest_street = self.streets_gdf.loc[closest_street_idx]

                # Calculate snap point on closest street
                snap_point = self._snap_to_roadside(poi_point, closest_street.geometry)

                misplaced_pois.append({
                    'poi_idx': idx,
                    'original_point': poi_point,
                    'closest_street_idx': closest_street_idx,
                    'distance_to_road': min_dist,
                    'suggested_point': snap_point,
                    'confidence': self._calculate_spatial_confidence(min_dist, min_distance, max_distance),
                    'street_name': closest_street.get('ST_NAME', 'Unknown'),
                    'method': 'spatial_analysis'
                })

        self.analysis_results['layer1'] = misplaced_pois
        print(f"‚úì Found {len(misplaced_pois)} potentially misplaced POIs")
        return misplaced_pois

    def layer2_ml_anomaly_detection(self):
        """
        Layer 2: Machine Learning anomaly detection for POI placement patterns
        """
        print("ü§ñ Layer 2: ML Anomaly Detection...")

        # Extract features for each POI
        features = []
        poi_indices = []

        for idx, poi in self.poi_gdf.iterrows():
            poi_point = poi.geometry

            # Find nearby streets
            nearby_streets = self.streets_gdf[self.streets_gdf.geometry.distance(poi_point) <= 50]

            if len(nearby_streets) == 0:
                continue

            # Feature engineering
            distances = nearby_streets.geometry.distance(poi_point)

            feature_vector = [
                distances.min(),  # Distance to closest road
                distances.mean(), # Average distance to nearby roads
                distances.std() if len(distances) > 1 else 0,  # Std dev of distances
                len(nearby_streets),  # Number of nearby streets
                poi_point.x,  # X coordinate (for spatial patterns)
                poi_point.y,  # Y coordinate (for spatial patterns)
            ]

            # Add street density around POI
            buffer_area = poi_point.buffer(100)  # 100m buffer
            streets_in_buffer = self.streets_gdf[self.streets_gdf.geometry.intersects(buffer_area)]
            total_street_length = streets_in_buffer.geometry.length.sum()
            street_density = total_street_length / buffer_area.area

            feature_vector.append(street_density)

            features.append(feature_vector)
            poi_indices.append(idx)

        if len(features) == 0:
            self.analysis_results['layer2'] = []
            return []

        # Normalize features
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)

        # Apply Isolation Forest for anomaly detection
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        anomaly_labels = iso_forest.fit_predict(features_scaled)
        anomaly_scores = iso_forest.score_samples(features_scaled)

        # Apply DBSCAN for spatial clustering
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        cluster_labels = dbscan.fit_predict(features_scaled[:, [4, 5]])  # Use only x, y coordinates

        # Identify anomalous POIs
        anomalous_pois = []
        for i, (poi_idx, anomaly_label, anomaly_score, cluster_label) in enumerate(
            zip(poi_indices, anomaly_labels, anomaly_scores, cluster_labels)
        ):
            if anomaly_label == -1:  # Anomaly detected
                poi = self.poi_gdf.loc[poi_idx]

                # Find correction suggestion
                closest_street_idx = self.streets_gdf.geometry.distance(poi.geometry).idxmin()
                closest_street = self.streets_gdf.loc[closest_street_idx]
                suggested_point = self._snap_to_roadside(poi.geometry, closest_street.geometry)

                anomalous_pois.append({
                    'poi_idx': poi_idx,
                    'original_point': poi.geometry,
                    'closest_street_idx': closest_street_idx,
                    'distance_to_road': closest_street.geometry.distance(poi.geometry),
                    'suggested_point': suggested_point,
                    'confidence': abs(anomaly_score),  # Higher absolute score = higher confidence
                    'cluster_id': cluster_label,
                    'street_name': closest_street.get('ST_NAME', 'Unknown'),
                    'method': 'ml_anomaly'
                })

        self.analysis_results['layer2'] = anomalous_pois
        print(f"‚úì ML detected {len(anomalous_pois)} anomalous POIs")
        return anomalous_pois

    def layer3_kalman_filtering(self, process_noise=0.1, measurement_noise=1.0):
        """
        Layer 3: Kalman filtering for trajectory smoothing and position refinement
        """
        print("üîÑ Layer 3: Kalman Filtering...")

        # Combine results from previous layers
        all_corrections = self.analysis_results.get('layer1', []) + self.analysis_results.get('layer2', [])

        if len(all_corrections) == 0:
            self.analysis_results['layer3'] = []
            return []

        # Group corrections by street or spatial proximity
        corrected_pois = []

        for correction in all_corrections:
            # Simple Kalman filter implementation for position refinement
            original_pos = np.array([correction['original_point'].x, correction['original_point'].y])
            suggested_pos = np.array([correction['suggested_point'].x, correction['suggested_point'].y])

            # Initialize Kalman filter
            # State: [x, y, vx, vy] (position and velocity)
            state = np.array([original_pos[0], original_pos[1], 0, 0])

            # Process noise covariance
            Q = np.eye(4) * process_noise
            Q[2:, 2:] *= 0.1  # Lower process noise for velocity

            # Measurement noise covariance
            R = np.eye(2) * measurement_noise

            # State transition matrix (constant velocity model)
            F = np.array([[1, 0, 1, 0],
                         [0, 1, 0, 1],
                         [0, 0, 1, 0],
                         [0, 0, 0, 1]])

            # Measurement matrix
            H = np.array([[1, 0, 0, 0],
                         [0, 1, 0, 0]])

            # Initial covariance
            P = np.eye(4) * 10

            # Kalman filter steps
            # Predict
            state_pred = F @ state
            P_pred = F @ P @ F.T + Q

            # Update with measurement (suggested position)
            y = suggested_pos - H @ state_pred  # Innovation
            S = H @ P_pred @ H.T + R  # Innovation covariance
            K = P_pred @ H.T @ np.linalg.inv(S)  # Kalman gain

            state_updated = state_pred + K @ y
            P_updated = (np.eye(4) - K @ H) @ P_pred

            # Extract refined position
            refined_pos = state_updated[:2]
            refined_point = Point(refined_pos[0], refined_pos[1])

            # Calculate confidence based on covariance
            position_uncertainty = np.sqrt(P_updated[0, 0] + P_updated[1, 1])
            kalman_confidence = 1.0 / (1.0 + position_uncertainty)

            corrected_pois.append({
                **correction,
                'kalman_refined_point': refined_point,
                'kalman_confidence': kalman_confidence,
                'position_uncertainty': position_uncertainty,
                'method': f"{correction['method']}_kalman"
            })

        self.analysis_results['layer3'] = corrected_pois
        print(f"‚úì Kalman refined {len(corrected_pois)} POI corrections")
        return corrected_pois

    def layer4_consensus_validation(self, min_consensus=2):
        """
        Layer 4: Consensus validation - combine results from all layers
        """
        print("üéØ Layer 4: Consensus Validation...")

        # Collect all corrections
        all_corrections = {}

        # Process each layer's results
        for layer_name, corrections in self.analysis_results.items():
            for correction in corrections:
                poi_idx = correction['poi_idx']
                if poi_idx not in all_corrections:
                    all_corrections[poi_idx] = []
                all_corrections[poi_idx].append(correction)

        # Find consensus corrections
        consensus_corrections = []

        for poi_idx, corrections in all_corrections.items():
            if len(corrections) >= min_consensus:
                # Calculate weighted average of suggested positions
                total_weight = 0
                weighted_x = 0
                weighted_y = 0

                for correction in corrections:
                    # Get the best available point (Kalman refined or original suggestion)
                    if 'kalman_refined_point' in correction:
                        point = correction['kalman_refined_point']
                        confidence = correction['kalman_confidence']
                    else:
                        point = correction['suggested_point']
                        confidence = correction['confidence']

                    weighted_x += point.x * confidence
                    weighted_y += point.y * confidence
                    total_weight += confidence

                if total_weight > 0:
                    consensus_x = weighted_x / total_weight
                    consensus_y = weighted_y / total_weight
                    consensus_point = Point(consensus_x, consensus_y)

                    # Calculate overall confidence
                    overall_confidence = total_weight / len(corrections)

                    consensus_corrections.append({
                        'poi_idx': poi_idx,
                        'original_point': corrections[0]['original_point'],
                        'consensus_point': consensus_point,
                        'overall_confidence': overall_confidence,
                        'num_methods_agree': len(corrections),
                        'contributing_methods': [c['method'] for c in corrections],
                        'street_name': corrections[0]['street_name']
                    })

        self.analysis_results['consensus'] = consensus_corrections
        print(f"‚úì {len(consensus_corrections)} POIs passed consensus validation")
        return consensus_corrections

    def _snap_to_roadside(self, poi_point, street_line, offset_distance=3):
        """
        Snap POI to roadside with specified offset
        """
        # Find closest point on street
        closest_point = nearest_points(poi_point, street_line)[1]

        # Calculate perpendicular offset
        if hasattr(street_line, 'coords'):
            coords = list(street_line.coords)
        else:
            coords = [closest_point.coords[0]]

        if len(coords) < 2:
            return closest_point

        # Find the closest segment
        min_dist = float('inf')
        best_segment = None

        for i in range(len(coords) - 1):
            segment = LineString([coords[i], coords[i + 1]])
            dist = segment.distance(poi_point)
            if dist < min_dist:
                min_dist = dist
                best_segment = segment

        if best_segment is None:
            return closest_point

        # Calculate perpendicular direction
        p1, p2 = best_segment.coords[0], best_segment.coords[1]
        dx = p2[0] - p1[0]
        dy = p2[1] - p1[1]
        length = np.sqrt(dx**2 + dy**2)

        if length == 0:
            return closest_point

        # Normalize and rotate 90 degrees
        perp_x = -dy / length
        perp_y = dx / length

        # Determine which side to offset to (closest side)
        offset_point1 = Point(closest_point.x + perp_x * offset_distance,
                             closest_point.y + perp_y * offset_distance)
        offset_point2 = Point(closest_point.x - perp_x * offset_distance,
                             closest_point.y - perp_y * offset_distance)

        # Choose the offset point closer to original POI
        if offset_point1.distance(poi_point) < offset_point2.distance(poi_point):
            return offset_point1
        else:
            return offset_point2

    def _calculate_spatial_confidence(self, distance, min_dist, max_dist):
        """
        Calculate confidence score based on distance from road
        """
        if distance <= min_dist:
            return 0.9  # High confidence - clearly misplaced
        elif distance >= max_dist:
            return 0.1  # Low confidence - might be legitimate
        else:
            # Linear interpolation between min and max
            return 0.9 - 0.8 * (distance - min_dist) / (max_dist - min_dist)

    def run_full_analysis(self):
        """
        Run all layers of analysis
        """
        print("üöÄ Starting Multi-Layer POI Correction Analysis...")

        # Layer 1: Spatial Analysis
        self.layer1_spatial_analysis()

        # Layer 2: ML Anomaly Detection
        self.layer2_ml_anomaly_detection()

        # Layer 3: Kalman Filtering
        self.layer3_kalman_filtering()

        # Layer 4: Consensus Validation
        final_corrections = self.layer4_consensus_validation()

        print(f"\n‚úÖ Analysis Complete!")
        print(f"üìä Summary:")
        print(f"   - Layer 1 (Spatial): {len(self.analysis_results.get('layer1', []))} detections")
        print(f"   - Layer 2 (ML): {len(self.analysis_results.get('layer2', []))} detections")
        print(f"   - Layer 3 (Kalman): {len(self.analysis_results.get('layer3', []))} refinements")
        print(f"   - Final Consensus: {len(final_corrections)} corrections")

        return final_corrections

    def create_visualization(self, output_file='poi_corrections_map.html'):
        """
        Create interactive map showing all corrections
        """
        print(f"üó∫Ô∏è Creating visualization: {output_file}")

        # Convert back to WGS84 for mapping
        poi_wgs84 = self.poi_gdf.to_crs('EPSG:4326')
        streets_wgs84 = self.streets_gdf.to_crs('EPSG:4326')

        # Get consensus corrections
        corrections = self.analysis_results.get('consensus', [])

        if len(corrections) == 0:
            print("No corrections to visualize")
            return

        # Calculate map center
        all_points = [poi_wgs84.iloc[c['poi_idx']].geometry for c in corrections]
        center_lat = np.mean([p.y for p in all_points])
        center_lon = np.mean([p.x for p in all_points])

        # Create map
        m = folium.Map(location=[center_lat, center_lon], zoom_start=13)

        # Add street network
        folium.GeoJson(
            streets_wgs84,
            style_function=lambda x: {
                'color': 'gray',
                'weight': 2,
                'opacity': 0.7
            },
            name='Street Network'
        ).add_to(m)

        # Add corrections
        for correction in corrections:
            poi_idx = correction['poi_idx']
            original_poi = poi_wgs84.iloc[poi_idx]

            # Convert consensus point back to WGS84
            consensus_point_svg = correction['consensus_point']
            consensus_gdf = gpd.GeoDataFrame([1], geometry=[consensus_point_svg], crs='EPSG:3414')
            consensus_wgs84 = consensus_gdf.to_crs('EPSG:4326').iloc[0].geometry

            # Original position (red)
            folium.Marker(
                location=[original_poi.geometry.y, original_poi.geometry.x],
                icon=folium.Icon(color='red', icon='exclamation-sign'),
                popup=f"""
                <b>Original POI</b><br>
                Street: {original_poi.get('STREET_NAM', 'Unknown')}<br>
                Confidence: {correction['overall_confidence']:.2f}<br>
                Methods: {', '.join(correction['contributing_methods'])}
                """
            ).add_to(m)

            # Corrected position (green)
            folium.Marker(
                location=[consensus_wgs84.y, consensus_wgs84.x],
                icon=folium.Icon(color='green', icon='ok-sign'),
                popup=f"""
                <b>Corrected POI</b><br>
                Street: {correction['street_name']}<br>
                Confidence: {correction['overall_confidence']:.2f}<br>
                Methods Agree: {correction['num_methods_agree']}
                """
            ).add_to(m)

            # Connection line
            folium.PolyLine(
                locations=[
                    [original_poi.geometry.y, original_poi.geometry.x],
                    [consensus_wgs84.y, consensus_wgs84.x]
                ],
                color='blue',
                weight=2,
                opacity=0.7
            ).add_to(m)

        # Add layer control
        folium.LayerControl().add_to(m)

        # Save map
        m.save(output_file)
        print(f"‚úÖ Map saved to {output_file}")

    def export_corrected_data(self, output_file='corrected_pois.csv'):
        """
        Export corrected POI data
        """
        corrections = self.analysis_results.get('consensus', [])

        if len(corrections) == 0:
            print("No corrections to export")
            return

        # Create corrected dataset
        corrected_pois = self.poi_gdf.copy()

        for correction in corrections:
            poi_idx = correction['poi_idx']
            consensus_point = correction['consensus_point']

            # Update geometry
            corrected_pois.loc[poi_idx, 'geometry'] = consensus_point

            # Add correction metadata
            corrected_pois.loc[poi_idx, 'CORRECTED'] = True
            corrected_pois.loc[poi_idx, 'CORRECTION_CONFIDENCE'] = correction['overall_confidence']
            corrected_pois.loc[poi_idx, 'CORRECTION_METHODS'] = ','.join(correction['contributing_methods'])

        # Convert back to WGS84 and export
        corrected_pois_wgs84 = corrected_pois.to_crs('EPSG:4326')
        corrected_pois_wgs84['x'] = corrected_pois_wgs84.geometry.x
        corrected_pois_wgs84['y'] = corrected_pois_wgs84.geometry.y

        # Export to CSV
        corrected_pois_wgs84.drop(columns=['geometry']).to_csv(output_file, index=False)
        print(f"‚úÖ Corrected POI data exported to {output_file}")

def correct_singapore_pois(poi_csv_path, streets_shp_path):
    """
    Main function to correct Singapore POIs
    """
    print("üìç Loading Singapore POI correction system...")

    # Load POI data
    try:
        poi_df = pd.read_csv(poi_csv_path)
    except FileNotFoundError:
        raise FileNotFoundError(f"POI CSV file not found at: {poi_csv_path}")

    # Create GeoDataFrame for POIs
    poi_gdf = gpd.GeoDataFrame(
        poi_df,
        geometry=gpd.points_from_xy(poi_df['x'], poi_df['y']),
        crs='EPSG:4326'  # Explicitly set CRS to WGS84
    )

    # Load streets data
    try:
        streets_gdf = gpd.read_file(streets_shp_path)
    except FileNotFoundError:
        raise FileNotFoundError(f"Streets shapefile not found at: {streets_shp_path}")

    # Ensure streets_gdf has a CRS
    if streets_gdf.crs is None:
        print("‚ö†Ô∏è Streets shapefile has no CRS defined. Setting to EPSG:4326 (WGS84).")
        streets_gdf.set_crs('EPSG:4326', inplace=True)
    else:
        print(f"Streets shapefile CRS: {streets_gdf.crs}")

    # Initialize corrector
    corrector = POICorrector(poi_gdf, streets_gdf)

    # Run analysis
    corrections = corrector.run_full_analysis()

    # Create outputs
    corrector.create_visualization()
    corrector.export_corrected_data()

    return corrector, corrections

# Usage example
if __name__ == "__main__":
    try:
        corrector, corrections = correct_singapore_pois('/content/poi_with_latlong.csv', '/content/Streets.shp')
    except Exception as e:
        print(f"Error running POI correction: {e}")

import pandas as pd
import geopandas as gpd
import numpy as np
from shapely.geometry import Point, LineString, Polygon
from shapely.ops import nearest_points
import folium
from folium.plugins import HeatMap, MarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class POICorrector:
    """
    Multi-layered POI correction system with spatial analysis, ML, and Kalman filtering
    """

    def __init__(self, poi_data, streets_data, crs_source='EPSG:4326', crs_target='EPSG:3414'):
        """
        Initialize the POI correction system

        Args:
            poi_data: GeoDataFrame of POI points
            streets_data: GeoDataFrame of street lines
            crs_source: Source coordinate system (WGS84)
            crs_target: Target coordinate system (SVY21 for Singapore)
        """
        # Ensure input GeoDataFrames have a CRS
        if poi_data.crs is None:
            print(f"‚ö†Ô∏è POI data has no CRS defined. Setting to {crs_source}")
            poi_data.set_crs(crs_source, inplace=True)
        if streets_data.crs is None:
            print(f"‚ö†Ô∏è Streets data has no CRS defined. Setting to {crs_source}")
            streets_data.set_crs(crs_source, inplace=True)

        # Transform to target CRS if needed
        self.poi_gdf = poi_data.to_crs(crs_target) if poi_data.crs != crs_target else poi_data
        self.streets_gdf = streets_data.to_crs(crs_target) if streets_data.crs != crs_target else streets_data
        self.crs_source = crs_source
        self.crs_target = crs_target

        # Results storage
        self.analysis_results = {}
        self.corrections = []
        self.confidence_scores = {}

    def layer1_spatial_analysis(self, min_distance=3, max_distance=5):
        """
        Layer 1: Traditional spatial analysis to identify misplaced POIs
        """
        print("üîç Layer 1: Spatial Analysis...")

        # Create spatial index for efficient queries
        spatial_index = self.streets_gdf.sindex

        misplaced_pois = []

        for idx, poi in self.poi_gdf.iterrows():
            poi_point = poi.geometry

            # Find nearby streets using spatial index
            possible_matches_index = list(spatial_index.intersection(poi_point.buffer(max_distance).bounds))
            possible_matches = self.streets_gdf.iloc[possible_matches_index]

            if len(possible_matches) == 0:
                continue

            # Calculate distances to all nearby streets
            distances = possible_matches.geometry.distance(poi_point)
            min_dist = distances.min()
            closest_street_idx = distances.idxmin()

            # Identify misplaced POIs
            if min_distance <= min_dist <= max_distance:
                # POI is in the suspicious range
                closest_street = self.streets_gdf.loc[closest_street_idx]

                # Calculate snap point on closest street
                snap_point = self._snap_to_roadside(poi_point, closest_street.geometry)

                misplaced_pois.append({
                    'poi_idx': idx,
                    'original_point': poi_point,
                    'closest_street_idx': closest_street_idx,
                    'distance_to_road': min_dist,
                    'suggested_point': snap_point,
                    'confidence': self._calculate_spatial_confidence(min_dist, min_distance, max_distance),
                    'street_name': closest_street.get('ST_NAME', 'Unknown'),
                    'method': 'spatial_analysis'
                })

        self.analysis_results['layer1'] = misplaced_pois
        print(f"‚úì Found {len(misplaced_pois)} potentially misplaced POIs")
        return misplaced_pois

    def layer2_ml_anomaly_detection(self):
        """
        Layer 2: Machine Learning anomaly detection for POI placement patterns
        """
        print("ü§ñ Layer 2: ML Anomaly Detection...")

        # Extract features for each POI
        features = []
        poi_indices = []

        for idx, poi in self.poi_gdf.iterrows():
            poi_point = poi.geometry

            # Find nearby streets
            nearby_streets = self.streets_gdf[self.streets_gdf.geometry.distance(poi_point) <= 50]

            if len(nearby_streets) == 0:
                continue

            # Feature engineering
            distances = nearby_streets.geometry.distance(poi_point)

            feature_vector = [
                distances.min(),  # Distance to closest road
                distances.mean(), # Average distance to nearby roads
                distances.std() if len(distances) > 1 else 0,  # Std dev of distances
                len(nearby_streets),  # Number of nearby streets
                poi_point.x,  # X coordinate (for spatial patterns)
                poi_point.y,  # Y coordinate (for spatial patterns)
            ]

            # Add street density around POI
            buffer_area = poi_point.buffer(100)  # 100m buffer
            streets_in_buffer = self.streets_gdf[self.streets_gdf.geometry.intersects(buffer_area)]
            total_street_length = streets_in_buffer.geometry.length.sum()
            street_density = total_street_length / buffer_area.area if buffer_area.area > 0 else 0

            feature_vector.append(street_density)

            features.append(feature_vector)
            poi_indices.append(idx)

        if len(features) == 0:
            self.analysis_results['layer2'] = []
            print("‚úì No features extracted for ML analysis")
            return []

        # Normalize features
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)

        # Apply Isolation Forest for anomaly detection
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        anomaly_labels = iso_forest.fit_predict(features_scaled)
        anomaly_scores = iso_forest.score_samples(features_scaled)

        # Apply DBSCAN for spatial clustering
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        cluster_labels = dbscan.fit_predict(features_scaled[:, [4, 5]])  # Use only x, y coordinates

        # Identify anomalous POIs
        anomalous_pois = []
        for i, (poi_idx, anomaly_label, anomaly_score, cluster_label) in enumerate(
            zip(poi_indices, anomaly_labels, anomaly_scores, cluster_labels)
        ):
            if anomaly_label == -1:  # Anomaly detected
                poi = self.poi_gdf.loc[poi_idx]

                # Find correction suggestion
                closest_street_idx = self.streets_gdf.geometry.distance(poi.geometry).idxmin()
                closest_street = self.streets_gdf.loc[closest_street_idx]
                suggested_point = self._snap_to_roadside(poi.geometry, closest_street.geometry)

                anomalous_pois.append({
                    'poi_idx': poi_idx,
                    'original_point': poi.geometry,
                    'closest_street_idx': closest_street_idx,
                    'distance_to_road': closest_street.geometry.distance(poi.geometry),
                    'suggested_point': suggested_point,
                    'confidence': abs(anomaly_score),  # Higher absolute score = higher confidence
                    'cluster_id': cluster_label,
                    'street_name': closest_street.get('ST_NAME', 'Unknown'),
                    'method': 'ml_anomaly'
                })

        self.analysis_results['layer2'] = anomalous_pois
        print(f"‚úì ML detected {len(anomalous_pois)} anomalous POIs")
        return anomalous_pois

    def layer3_kalman_filtering(self, process_noise=0.1, measurement_noise=1.0):
        """
        Layer 3: Kalman filtering for trajectory smoothing and position refinement
        """
        print("üîÑ Layer 3: Kalman Filtering...")

        # Combine results from previous layers
        all_corrections = self.analysis_results.get('layer1', []) + self.analysis_results.get('layer2', [])

        if len(all_corrections) == 0:
            self.analysis_results['layer3'] = []
            print("‚úì No corrections to refine with Kalman filtering")
            return []

        # Group corrections by street or spatial proximity
        corrected_pois = []

        for correction in all_corrections:
            # Simple Kalman filter implementation for position refinement
            original_pos = np.array([correction['original_point'].x, correction['original_point'].y])
            suggested_pos = np.array([correction['suggested_point'].x, correction['suggested_point'].y])

            # Initialize Kalman filter
            # State: [x, y, vx, vy] (position and velocity)
            state = np.array([original_pos[0], original_pos[1], 0, 0])

            # Process noise covariance
            Q = np.eye(4) * process_noise
            Q[2:, 2:] *= 0.1  # Lower process noise for velocity

            # Measurement noise covariance
            R = np.eye(2) * measurement_noise

            # State transition matrix (constant velocity model)
            F = np.array([[1, 0, 1, 0],
                         [0, 1, 0, 1],
                         [0, 0, 1, 0],
                         [0, 0, 0, 1]])

            # Measurement matrix
            H = np.array([[1, 0, 0, 0],
                         [0, 1, 0, 0]])

            # Initial covariance
            P = np.eye(4) * 10

            # Kalman filter steps
            # Predict
            state_pred = F @ state
            P_pred = F @ P @ F.T + Q

            # Update with measurement (suggested position)
            y = suggested_pos - H @ state_pred  # Innovation
            S = H @ P_pred @ H.T + R  # Innovation covariance
            K = P_pred @ H.T @ np.linalg.inv(S)  # Kalman gain

            state_updated = state_pred + K @ y
            P_updated = (np.eye(4) - K @ H) @ P_pred

            # Extract refined position
            refined_pos = state_updated[:2]
            refined_point = Point(refined_pos[0], refined_pos[1])

            # Calculate confidence based on covariance
            position_uncertainty = np.sqrt(P_updated[0, 0] + P_updated[1, 1])
            kalman_confidence = 1.0 / (1.0 + position_uncertainty)

            corrected_pois.append({
                **correction,
                'kalman_refined_point': refined_point,
                'kalman_confidence': kalman_confidence,
                'position_uncertainty': position_uncertainty,
                'method': f"{correction['method']}_kalman"
            })

        self.analysis_results['layer3'] = corrected_pois
        print(f"‚úì Kalman refined {len(corrected_pois)} POI corrections")
        return corrected_pois

    def layer4_consensus_validation(self, min_consensus=2):
        """
        Layer 4: Consensus validation - combine results from all layers
        """
        print("üéØ Layer 4: Consensus Validation...")

        # Collect all corrections
        all_corrections = {}

        # Process each layer's results
        for layer_name, corrections in self.analysis_results.items():
            for correction in corrections:
                poi_idx = correction['poi_idx']
                if poi_idx not in all_corrections:
                    all_corrections[poi_idx] = []
                all_corrections[poi_idx].append(correction)

        # Find consensus corrections
        consensus_corrections = []

        for poi_idx, corrections in all_corrections.items():
            if len(corrections) >= min_consensus:
                # Calculate weighted average of suggested positions
                total_weight = 0
                weighted_x = 0
                weighted_y = 0

                for correction in corrections:
                    # Get the best available point (Kalman refined or original suggestion)
                    if 'kalman_refined_point' in correction:
                        point = correction['kalman_refined_point']
                        confidence = correction['kalman_confidence']
                    else:
                        point = correction['suggested_point']
                        confidence = correction['confidence']

                    weighted_x += point.x * confidence
                    weighted_y += point.y * confidence
                    total_weight += confidence

                if total_weight > 0:
                    consensus_x = weighted_x / total_weight
                    consensus_y = weighted_y / total_weight
                    consensus_point = Point(consensus_x, consensus_y)

                    # Calculate overall confidence
                    overall_confidence = total_weight / len(corrections)

                    consensus_corrections.append({
                        'poi_idx': poi_idx,
                        'original_point': corrections[0]['original_point'],
                        'consensus_point': consensus_point,
                        'overall_confidence': overall_confidence,
                        'num_methods_agree': len(corrections),
                        'contributing_methods': [c['method'] for c in corrections],
                        'street_name': corrections[0]['street_name']
                    })

        self.analysis_results['consensus'] = consensus_corrections
        print(f"‚úì {len(consensus_corrections)} POIs passed consensus validation")
        return consensus_corrections

    def _snap_to_roadside(self, poi_point, street_line, offset_distance=3):
        """
        Snap POI to roadside with specified offset
        """
        # Find closest point on street
        closest_point = nearest_points(poi_point, street_line)[1]

        # Calculate perpendicular offset
        if hasattr(street_line, 'coords'):
            coords = list(street_line.coords)
        else:
            coords = [closest_point.coords[0]]

        if len(coords) < 2:
            return closest_point

        # Find the closest segment
        min_dist = float('inf')
        best_segment = None

        for i in range(len(coords) - 1):
            segment = LineString([coords[i], coords[i + 1]])
            dist = segment.distance(poi_point)
            if dist < min_dist:
                min_dist = dist
                best_segment = segment

        if best_segment is None:
            return closest_point

        # Calculate perpendicular direction
        p1, p2 = best_segment.coords[0], best_segment.coords[1]
        dx = p2[0] - p1[0]
        dy = p2[1] - p1[1]
        length = np.sqrt(dx**2 + dy**2)

        if length == 0:
            return closest_point

        # Normalize and rotate 90 degrees
        perp_x = -dy / length
        perp_y = dx / length

        # Determine which side to offset to (closest side)
        offset_point1 = Point(closest_point.x + perp_x * offset_distance,
                             closest_point.y + perp_y * offset_distance)
        offset_point2 = Point(closest_point.x - perp_x * offset_distance,
                             closest_point.y - perp_y * offset_distance)

        # Choose the offset point closer to original POI
        if offset_point1.distance(poi_point) < offset_point2.distance(poi_point):
            return offset_point1
        else:
            return offset_point2

    def _calculate_spatial_confidence(self, distance, min_dist, max_dist):
        """
        Calculate confidence score based on distance from road
        """
        if distance <= min_dist:
            return 0.9  # High confidence - clearly misplaced
        elif distance >= max_dist:
            return 0.1  # Low confidence - might be legitimate
        else:
            # Linear interpolation between min and max
            return 0.9 - 0.8 * (distance - min_dist) / (max_dist - min_dist)

    def run_full_analysis(self):
        """
        Run all layers of analysis
        """
        print("üöÄ Starting Multi-Layer POI Correction Analysis...")

        # Layer 1: Spatial Analysis
        self.layer1_spatial_analysis()

        # Layer 2: ML Anomaly Detection
        self.layer2_ml_anomaly_detection()

        # Layer 3: Kalman Filtering
        self.layer3_kalman_filtering()

        # Layer 4: Consensus Validation
        final_corrections = self.layer4_consensus_validation()

        print(f"\n‚úÖ Analysis Complete!")
        print(f"üìä Summary:")
        print(f"   - Layer 1 (Spatial): {len(self.analysis_results.get('layer1', []))} detections")
        print(f"   - Layer 2 (ML): {len(self.analysis_results.get('layer2', []))} detections")
        print(f"   - Layer 3 (Kalman): {len(self.analysis_results.get('layer3', []))} refinements")
        print(f"   - Final Consensus: {len(final_corrections)} corrections")

        return final_corrections

    def create_visualization(self, output_file='poi_corrections_map.html'):
        """
        Create interactive map showing all corrections
        """
        print(f"üó∫Ô∏è Creating visualization: {output_file}")

        # Convert back to WGS84 for mapping
        poi_wgs84 = self.poi_gdf.to_crs('EPSG:4326')
        streets_wgs84 = self.streets_gdf.to_crs('EPSG:4326')

        # Get consensus corrections
        corrections = self.analysis_results.get('consensus', [])

        if len(corrections) == 0:
            print("No corrections to visualize")
            return

        # Calculate map center
        all_points = [poi_wgs84.iloc[c['poi_idx']].geometry for c in corrections]
        center_lat = np.mean([p.y for p in all_points]) if all_points else 1.35
        center_lon = np.mean([p.x for p in all_points]) if all_points else 103.85

        # Create map
        m = folium.Map(location=[center_lat, center_lon], zoom_start=13)

        # Add street network
        folium.GeoJson(
            streets_wgs84,
            style_function=lambda x: {
                'color': 'gray',
                'weight': 2,
                'opacity': 0.7
            },
            name='Street Network'
        ).add_to(m)

        # Add corrections
        for correction in corrections:
            poi_idx = correction['poi_idx']
            original_poi = poi_wgs84.iloc[poi_idx]

            # Convert consensus point back to WGS84
            consensus_point_svg = correction['consensus_point']
            consensus_gdf = gpd.GeoDataFrame([1], geometry=[consensus_point_svg], crs='EPSG:3414')
            consensus_wgs84 = consensus_gdf.to_crs('EPSG:4326').iloc[0].geometry

            # Original position (red)
            folium.Marker(
                location=[original_poi.geometry.y, original_poi.geometry.x],
                icon=folium.Icon(color='red', icon='exclamation-sign', prefix='glyphicon'),
                popup=f"""
                <b>Original POI</b><br>
                Street: {original_poi.get('STREET_NAM', 'Unknown')}<br>
                Confidence: {correction['overall_confidence']:.2f}<br>
                Methods: {', '.join(correction['contributing_methods'])}
                """
            ).add_to(m)

            # Corrected position (green)
            folium.Marker(
                location=[consensus_wgs84.y, consensus_wgs84.x],
                icon=folium.Icon(color='green', icon='ok-sign', prefix='glyphicon'),
                popup=f"""
                <b>Corrected POI</b><br>
                Street: {correction['street_name']}<br>
                Confidence: {correction['overall_confidence']:.2f}<br>
                Methods Agree: {correction['num_methods_agree']}
                """
            ).add_to(m)

            # Connection line
            folium.PolyLine(
                locations=[
                    [original_poi.geometry.y, original_poi.geometry.x],
                    [consensus_wgs84.y, consensus_wgs84.x]
                ],
                color='blue',
                weight=2,
                opacity=0.7
            ).add_to(m)

        # Add layer control
        folium.LayerControl().add_to(m)

        # Save map
        m.save(output_file)
        print(f"‚úÖ Map saved to {output_file}")

    def export_corrected_data(self, output_file='corrected_pois.csv'):
        """
        Export corrected POI data
        """
        print(f"üíæ Exporting corrected data to: {output_file}")
        corrections = self.analysis_results.get('consensus', [])

        if len(corrections) == 0:
            print("No corrections to export")
            return

        # Create corrected dataset
        corrected_pois = self.poi_gdf.copy()

        for correction in corrections:
            poi_idx = correction['poi_idx']
            consensus_point = correction['consensus_point']

            # Update geometry
            corrected_pois.loc[poi_idx, 'geometry'] = consensus_point

            # Add correction metadata
            corrected_pois.loc[poi_idx, 'CORRECTED'] = True
            corrected_pois.loc[poi_idx, 'CORRECTION_CONFIDENCE'] = correction['overall_confidence']
            corrected_pois.loc[poi_idx, 'CORRECTION_METHODS'] = ','.join(correction['contributing_methods'])

        # Convert back to WGS84 and export
        corrected_pois_wgs84 = corrected_pois.to_crs('EPSG:4326')
        corrected_pois_wgs84['x'] = corrected_pois_wgs84.geometry.x
        corrected_pois_wgs84['y'] = corrected_pois_wgs84.geometry.y

        # Export to CSV
        corrected_pois_wgs84.drop(columns=['geometry']).to_csv(output_file, index=False)
        print(f"‚úÖ Corrected POI data exported to {output_file}")

def correct_singapore_pois(poi_csv_path=None, streets_shp_path=None, sample_size=1000):
    """
    Main function to correct Singapore POIs, with optional synthetic data for testing
    """
    print("üìç Loading Singapore POI correction system...")

    # Load or generate POI data
    if poi_csv_path:
        try:
            poi_df = pd.read_csv(poi_csv_path)
            # Sample for testing if dataset is large
            if len(poi_df) > sample_size:
                print(f"Sampling {sample_size} POIs from {len(poi_df)} for testing...")
                poi_df = poi_df.sample(n=sample_size, random_state=42)
        except FileNotFoundError:
            raise FileNotFoundError(f"POI CSV file not found at: {poi_csv_path}")
    else:
        # Generate synthetic POI data
        print("Generating synthetic POI data for testing...")
        np.random.seed(42)
        poi_data = {
            'x': 103.85 + np.random.uniform(-0.02, 0.02, sample_size),  # Longitude
            'y': 1.35 + np.random.uniform(-0.02, 0.02, sample_size),    # Latitude
            'STREET_NAM': [f"Street_{i % 5}" for i in range(sample_size)]  # Dummy street names
        }
        poi_df = pd.DataFrame(poi_data)

    # Create GeoDataFrame for POIs
    poi_gdf = gpd.GeoDataFrame(
        poi_df,
        geometry=gpd.points_from_xy(poi_df['x'], poi_df['y']),
        crs='EPSG:4326'
    )

    # Load or generate streets data
    if streets_shp_path:
        try:
            streets_gdf = gpd.read_file(streets_shp_path)
        except FileNotFoundError:
            raise FileNotFoundError(f"Streets shapefile not found at: {streets_shp_path}")
    else:
        # Generate synthetic streets data
        print("Generating synthetic streets data for testing...")
        streets_data = {
            'ST_NAME': ['Main Rd', 'Side St', 'Cross Rd'],
            'geometry': [
                LineString([(103.84, 1.34), (103.86, 1.34)]),  # Horizontal street
                LineString([(103.84, 1.36), (103.86, 1.36)]),  # Another horizontal street
                LineString([(103.85, 1.33), (103.85, 1.37)])   # Vertical street
            ]
        }
        streets_gdf = gpd.GeoDataFrame(streets_data, crs='EPSG:4326')

    # Ensure streets_gdf has a CRS
    if streets_gdf.crs is None:
        print("‚ö†Ô∏è Streets shapefile has no CRS defined. Setting to EPSG:4326 (WGS84).")
        streets_gdf.set_crs('EPSG:4326', inplace=True)
    else:
        print(f"Streets shapefile CRS: {streets_gdf.crs}")

    # Initialize corrector
    corrector = POICorrector(poi_gdf, streets_gdf)

    # Run analysis
    corrections = corrector.run_full_analysis()

    # Create outputs
    corrector.create_visualization(output_file='poi_corrections_map_sample.html')
    corrector.export_corrected_data(output_file='corrected_pois_sample.csv')

    return corrector, corrections

# Usage example
if __name__ == "__main__":
    try:
        # Use synthetic data for testing
        corrector, corrections = correct_singapore_pois()
        # For actual data, use:
        corrector, corrections = correct_singapore_pois('/content/poi_with_latlong.csv', '/content/Streets.shp')
    except Exception as e:
        print(f"Error running POI correction: {e}")

import pandas as pd
import geopandas as gpd
import numpy as np
from shapely.geometry import Point, LineString, Polygon
from shapely.ops import nearest_points
import folium
from folium.plugins import HeatMap, MarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class POICorrector:
    """
    Multi-layered POI correction system with spatial analysis, ML, and Kalman filtering
    Now includes visualization after each layer
    """

    def __init__(self, poi_data, streets_data, crs_source='EPSG:4326', crs_target='EPSG:3414'):
        """
        Initialize the POI correction system

        Args:
            poi_data: GeoDataFrame of POI points
            streets_data: GeoDataFrame of street lines
            crs_source: Source coordinate system (WGS84)
            crs_target: Target coordinate system (SVY21 for Singapore)
        """
        # Ensure input GeoDataFrames have a CRS
        if poi_data.crs is None:
            print(f"‚ö†Ô∏è POI data has no CRS defined. Setting to {crs_source}")
            poi_data.set_crs(crs_source, inplace=True)
        if streets_data.crs is None:
            print(f"‚ö†Ô∏è Streets data has no CRS defined. Setting to {crs_source}")
            streets_data.set_crs(crs_source, inplace=True)

        # Transform to target CRS if needed
        self.poi_gdf = poi_data.to_crs(crs_target) if poi_data.crs != crs_target else poi_data
        self.streets_gdf = streets_data.to_crs(crs_target) if streets_data.crs != crs_target else streets_data
        self.crs_source = crs_source
        self.crs_target = crs_target

        # Results storage
        self.analysis_results = {}
        self.corrections = []
        self.confidence_scores = {}

        # Store original data in WGS84 for visualization
        self.poi_wgs84 = poi_data.to_crs('EPSG:4326') if poi_data.crs != 'EPSG:4326' else poi_data
        self.streets_wgs84 = streets_data.to_crs('EPSG:4326') if streets_data.crs != 'EPSG:4326' else streets_data

    def _create_base_map(self, title="POI Analysis"):
        """
        Create base map with street network
        """
        # Calculate map center
        bounds = self.poi_wgs84.bounds
        center_lat = (bounds.miny.min() + bounds.maxy.max()) / 2
        center_lon = (bounds.minx.min() + bounds.maxx.max()) / 2

        # Create map
        m = folium.Map(
            location=[center_lat, center_lon],
            zoom_start=13,
            tiles='OpenStreetMap'
        )

        # Add title
        title_html = f'''
                     <h3 align="center" style="font-size:16px"><b>{title}</b></h3>
                     '''
        m.get_root().html.add_child(folium.Element(title_html))

        # Add street network
        folium.GeoJson(
            self.streets_wgs84,
            style_function=lambda x: {
                'color': 'gray',
                'weight': 1,
                'opacity': 0.6
            },
            name='Street Network'
        ).add_to(m)

        return m

    def layer1_spatial_analysis(self, min_distance=3, max_distance=5, create_viz=True):
        """
        Layer 1: Traditional spatial analysis to identify misplaced POIs
        """
        print("üîç Layer 1: Spatial Analysis...")

        # Create spatial index for efficient queries
        spatial_index = self.streets_gdf.sindex

        misplaced_pois = []

        for idx, poi in self.poi_gdf.iterrows():
            poi_point = poi.geometry

            # Find nearby streets using spatial index
            possible_matches_index = list(spatial_index.intersection(poi_point.buffer(max_distance).bounds))
            possible_matches = self.streets_gdf.iloc[possible_matches_index]

            if len(possible_matches) == 0:
                continue

            # Calculate distances to all nearby streets
            distances = possible_matches.geometry.distance(poi_point)
            min_dist = distances.min()
            closest_street_idx = distances.idxmin()

            # Identify misplaced POIs
            if min_distance <= min_dist <= max_distance:
                # POI is in the suspicious range
                closest_street = self.streets_gdf.loc[closest_street_idx]

                # Calculate snap point on closest street
                snap_point = self._snap_to_roadside(poi_point, closest_street.geometry)

                misplaced_pois.append({
                    'poi_idx': idx,
                    'original_point': poi_point,
                    'closest_street_idx': closest_street_idx,
                    'distance_to_road': min_dist,
                    'suggested_point': snap_point,
                    'confidence': self._calculate_spatial_confidence(min_dist, min_distance, max_distance),
                    'street_name': closest_street.get('ST_NAME', 'Unknown'),
                    'method': 'spatial_analysis'
                })

        self.analysis_results['layer1'] = misplaced_pois
        print(f"‚úì Found {len(misplaced_pois)} potentially misplaced POIs")

        # Create visualization for Layer 1
        if create_viz:
            self._visualize_layer1(misplaced_pois)

        return misplaced_pois

    def _visualize_layer1(self, misplaced_pois):
        """
        Create visualization for Layer 1 - Spatial Analysis
        """
        print("üìä Creating Layer 1 visualization...")

        m = self._create_base_map("Layer 1: Spatial Analysis Results")

        # Add all original POIs in light gray
        for idx, poi in self.poi_wgs84.iterrows():
            folium.CircleMarker(
                location=[poi.geometry.y, poi.geometry.x],
                radius=3,
                color='lightgray',
                fill=True,
                fillOpacity=0.4,
                popup=f"POI {idx} (Normal)",
                weight=1
            ).add_to(m)

        # Add detected misplaced POIs and their corrections
        for correction in misplaced_pois:
            poi_idx = correction['poi_idx']
            original_poi = self.poi_wgs84.iloc[poi_idx]

            # Convert suggested point to WGS84
            suggested_gdf = gpd.GeoDataFrame([1], geometry=[correction['suggested_point']], crs=self.crs_target)
            suggested_wgs84 = suggested_gdf.to_crs('EPSG:4326').iloc[0].geometry

            # Original position (red) - detected as misplaced
            folium.Marker(
                location=[original_poi.geometry.y, original_poi.geometry.x],
                icon=folium.Icon(color='red', icon='exclamation-sign', prefix='glyphicon'),
                popup=f"""
                <b>Misplaced POI (Layer 1)</b><br>
                POI Index: {poi_idx}<br>
                Distance to road: {correction['distance_to_road']:.2f}m<br>
                Confidence: {correction['confidence']:.2f}<br>
                Street: {correction['street_name']}
                """
            ).add_to(m)

            # Suggested position (orange)
            folium.Marker(
                location=[suggested_wgs84.y, suggested_wgs84.x],
                icon=folium.Icon(color='orange', icon='arrow-right', prefix='glyphicon'),
                popup=f"""
                <b>Spatial Suggestion</b><br>
                Method: Snap to roadside<br>
                Confidence: {correction['confidence']:.2f}
                """
            ).add_to(m)

            # Connection line
            folium.PolyLine(
                locations=[
                    [original_poi.geometry.y, original_poi.geometry.x],
                    [suggested_wgs84.y, suggested_wgs84.x]
                ],
                color='red',
                weight=2,
                opacity=0.7,
                dash_array='5, 5'
            ).add_to(m)

        # Add legend
        legend_html = '''
        <div style="position: fixed;
                    bottom: 50px; left: 50px; width: 200px; height: 120px;
                    background-color: white; border:2px solid grey; z-index:9999;
                    font-size:14px; padding: 10px">
        <b>Layer 1: Spatial Analysis</b><br>
        <i class="fa fa-circle" style="color:lightgray"></i> Normal POIs<br>
        <i class="fa fa-exclamation-triangle" style="color:red"></i> Detected Misplaced<br>
        <i class="fa fa-arrow-right" style="color:orange"></i> Spatial Suggestion<br>
        <hr style="margin: 5px 0;">
        Detections: {}<br>
        </div>
        '''.format(len(misplaced_pois))
        m.get_root().html.add_child(folium.Element(legend_html))

        # Save map
        filename = 'layer1_spatial_analysis.html'
        m.save(filename)
        print(f"‚úÖ Layer 1 visualization saved to {filename}")

    def layer2_ml_anomaly_detection(self, create_viz=True):
        """
        Layer 2: Machine Learning anomaly detection for POI placement patterns
        """
        print("ü§ñ Layer 2: ML Anomaly Detection...")

        # Extract features for each POI
        features = []
        poi_indices = []

        for idx, poi in self.poi_gdf.iterrows():
            poi_point = poi.geometry

            # Find nearby streets
            nearby_streets = self.streets_gdf[self.streets_gdf.geometry.distance(poi_point) <= 50]

            if len(nearby_streets) == 0:
                continue

            # Feature engineering
            distances = nearby_streets.geometry.distance(poi_point)

            feature_vector = [
                distances.min(),  # Distance to closest road
                distances.mean(), # Average distance to nearby roads
                distances.std() if len(distances) > 1 else 0,  # Std dev of distances
                len(nearby_streets),  # Number of nearby streets
                poi_point.x,  # X coordinate (for spatial patterns)
                poi_point.y,  # Y coordinate (for spatial patterns)
            ]

            # Add street density around POI
            buffer_area = poi_point.buffer(100)  # 100m buffer
            streets_in_buffer = self.streets_gdf[self.streets_gdf.geometry.intersects(buffer_area)]
            total_street_length = streets_in_buffer.geometry.length.sum()
            street_density = total_street_length / buffer_area.area if buffer_area.area > 0 else 0

            feature_vector.append(street_density)

            features.append(feature_vector)
            poi_indices.append(idx)

        if len(features) == 0:
            self.analysis_results['layer2'] = []
            print("‚úì No features extracted for ML analysis")
            return []

        # Normalize features
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)

        # Apply Isolation Forest for anomaly detection
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        anomaly_labels = iso_forest.fit_predict(features_scaled)
        anomaly_scores = iso_forest.score_samples(features_scaled)

        # Apply DBSCAN for spatial clustering
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        cluster_labels = dbscan.fit_predict(features_scaled[:, [4, 5]])  # Use only x, y coordinates

        # Identify anomalous POIs
        anomalous_pois = []
        for i, (poi_idx, anomaly_label, anomaly_score, cluster_label) in enumerate(
            zip(poi_indices, anomaly_labels, anomaly_scores, cluster_labels)
        ):
            if anomaly_label == -1:  # Anomaly detected
                poi = self.poi_gdf.loc[poi_idx]

                # Find correction suggestion
                closest_street_idx = self.streets_gdf.geometry.distance(poi.geometry).idxmin()
                closest_street = self.streets_gdf.loc[closest_street_idx]
                suggested_point = self._snap_to_roadside(poi.geometry, closest_street.geometry)

                anomalous_pois.append({
                    'poi_idx': poi_idx,
                    'original_point': poi.geometry,
                    'closest_street_idx': closest_street_idx,
                    'distance_to_road': closest_street.geometry.distance(poi.geometry),
                    'suggested_point': suggested_point,
                    'confidence': abs(anomaly_score),  # Higher absolute score = higher confidence
                    'cluster_id': cluster_label,
                    'street_name': closest_street.get('ST_NAME', 'Unknown'),
                    'method': 'ml_anomaly',
                    'anomaly_score': anomaly_score
                })

        self.analysis_results['layer2'] = anomalous_pois
        print(f"‚úì ML detected {len(anomalous_pois)} anomalous POIs")

        # Create visualization for Layer 2
        if create_viz:
            self._visualize_layer2(anomalous_pois, poi_indices, cluster_labels)

        return anomalous_pois

    def _visualize_layer2(self, anomalous_pois, poi_indices, cluster_labels):
        """
        Create visualization for Layer 2 - ML Anomaly Detection
        """
        print("üìä Creating Layer 2 visualization...")

        m = self._create_base_map("Layer 2: ML Anomaly Detection Results")

        # Color map for clusters
        cluster_colors = ['blue', 'green', 'purple', 'pink', 'gray', 'darkred', 'lightred',
                         'beige', 'darkblue', 'darkgreen', 'cadetblue', 'darkpurple', 'white', 'black']

        # Add all POIs with cluster colors
        poi_cluster_map = {}
        for i, poi_idx in enumerate(poi_indices):
            if i < len(cluster_labels):
                poi_cluster_map[poi_idx] = cluster_labels[i]

        for idx, poi in self.poi_wgs84.iterrows():
            cluster_id = poi_cluster_map.get(idx, -1)

            if cluster_id == -1:  # Noise/unassigned
                color = 'lightgray'
                popup_text = f"POI {idx} (Noise/Unassigned)"
            else:
                color = cluster_colors[cluster_id % len(cluster_colors)]
                popup_text = f"POI {idx} (Cluster {cluster_id})"

            folium.CircleMarker(
                location=[poi.geometry.y, poi.geometry.x],
                radius=4,
                color=color,
                fill=True,
                fillOpacity=0.6,
                popup=popup_text,
                weight=2
            ).add_to(m)

        # Add detected anomalous POIs and their corrections
        for correction in anomalous_pois:
            poi_idx = correction['poi_idx']
            original_poi = self.poi_wgs84.iloc[poi_idx]

            # Convert suggested point to WGS84
            suggested_gdf = gpd.GeoDataFrame([1], geometry=[correction['suggested_point']], crs=self.crs_target)
            suggested_wgs84 = suggested_gdf.to_crs('EPSG:4326').iloc[0].geometry

            # Original position (red star) - detected as anomaly
            folium.Marker(
                location=[original_poi.geometry.y, original_poi.geometry.x],
                icon=folium.Icon(color='red', icon='star', prefix='glyphicon'),
                popup=f"""
                <b>ML Anomaly (Layer 2)</b><br>
                POI Index: {poi_idx}<br>
                Anomaly Score: {correction['anomaly_score']:.3f}<br>
                Confidence: {correction['confidence']:.2f}<br>
                Cluster: {correction['cluster_id']}<br>
                Street: {correction['street_name']}
                """
            ).add_to(m)

            # ML Suggested position (blue)
            folium.Marker(
                location=[suggested_wgs84.y, suggested_wgs84.x],
                icon=folium.Icon(color='blue', icon='cog', prefix='glyphicon'),
                popup=f"""
                <b>ML Suggestion</b><br>
                Method: Isolation Forest + DBSCAN<br>
                Confidence: {correction['confidence']:.2f}
                """
            ).add_to(m)

            # Connection line
            folium.PolyLine(
                locations=[
                    [original_poi.geometry.y, original_poi.geometry.x],
                    [suggested_wgs84.y, suggested_wgs84.x]
                ],
                color='blue',
                weight=2,
                opacity=0.7,
                dash_array='10, 5'
            ).add_to(m)

        # Add legend
        legend_html = '''
        <div style="position: fixed;
                    bottom: 50px; left: 50px; width: 220px; height: 140px;
                    background-color: white; border:2px solid grey; z-index:9999;
                    font-size:14px; padding: 10px">
        <b>Layer 2: ML Anomaly Detection</b><br>
        <i class="fa fa-circle" style="color:blue"></i> Clustered POIs<br>
        <i class="fa fa-circle" style="color:lightgray"></i> Noise/Unassigned<br>
        <i class="fa fa-star" style="color:red"></i> ML Anomalies<br>
        <i class="fa fa-cog" style="color:blue"></i> ML Suggestions<br>
        <hr style="margin: 5px 0;">
        Anomalies: {}<br>
        </div>
        '''.format(len(anomalous_pois))
        m.get_root().html.add_child(folium.Element(legend_html))

        # Save map
        filename = 'layer2_ml_anomaly_detection.html'
        m.save(filename)
        print(f"‚úÖ Layer 2 visualization saved to {filename}")

    def layer3_kalman_filtering(self, process_noise=0.1, measurement_noise=1.0, create_viz=True):
        """
        Layer 3: Kalman filtering for trajectory smoothing and position refinement
        """
        print("üîÑ Layer 3: Kalman Filtering...")

        # Combine results from previous layers
        all_corrections = self.analysis_results.get('layer1', []) + self.analysis_results.get('layer2', [])

        if len(all_corrections) == 0:
            self.analysis_results['layer3'] = []
            print("‚úì No corrections to refine with Kalman filtering")
            return []

        # Group corrections by street or spatial proximity
        corrected_pois = []

        for correction in all_corrections:
            # Simple Kalman filter implementation for position refinement
            original_pos = np.array([correction['original_point'].x, correction['original_point'].y])
            suggested_pos = np.array([correction['suggested_point'].x, correction['suggested_point'].y])

            # Initialize Kalman filter
            # State: [x, y, vx, vy] (position and velocity)
            state = np.array([original_pos[0], original_pos[1], 0, 0])

            # Process noise covariance
            Q = np.eye(4) * process_noise
            Q[2:, 2:] *= 0.1  # Lower process noise for velocity

            # Measurement noise covariance
            R = np.eye(2) * measurement_noise

            # State transition matrix (constant velocity model)
            F = np.array([[1, 0, 1, 0],
                         [0, 1, 0, 1],
                         [0, 0, 1, 0],
                         [0, 0, 0, 1]])

            # Measurement matrix
            H = np.array([[1, 0, 0, 0],
                         [0, 1, 0, 0]])

            # Initial covariance
            P = np.eye(4) * 10

            # Kalman filter steps
            # Predict
            state_pred = F @ state
            P_pred = F @ P @ F.T + Q

            # Update with measurement (suggested position)
            y = suggested_pos - H @ state_pred  # Innovation
            S = H @ P_pred @ H.T + R  # Innovation covariance
            K = P_pred @ H.T @ np.linalg.inv(S)  # Kalman gain

            state_updated = state_pred + K @ y
            P_updated = (np.eye(4) - K @ H) @ P_pred

            # Extract refined position
            refined_pos = state_updated[:2]
            refined_point = Point(refined_pos[0], refined_pos[1])

            # Calculate confidence based on covariance
            position_uncertainty = np.sqrt(P_updated[0, 0] + P_updated[1, 1])
            kalman_confidence = 1.0 / (1.0 + position_uncertainty)

            corrected_pois.append({
                **correction,
                'kalman_refined_point': refined_point,
                'kalman_confidence': kalman_confidence,
                'position_uncertainty': position_uncertainty,
                'method': f"{correction['method']}_kalman"
            })

        self.analysis_results['layer3'] = corrected_pois
        print(f"‚úì Kalman refined {len(corrected_pois)} POI corrections")

        # Create visualization for Layer 3
        if create_viz:
            self._visualize_layer3(corrected_pois)

        return corrected_pois

    def _visualize_layer3(self, corrected_pois):
        """
        Create visualization for Layer 3 - Kalman Filtering
        """
        print("üìä Creating Layer 3 visualization...")

        m = self._create_base_map("Layer 3: Kalman Filtering Results")

        # Add all original POIs in light gray
        for idx, poi in self.poi_wgs84.iterrows():
            folium.CircleMarker(
                location=[poi.geometry.y, poi.geometry.x],
                radius=3,
                color='lightgray',
                fill=True,
                fillOpacity=0.4,
                popup=f"POI {idx} (Normal)",
                weight=1
            ).add_to(m)

        # Add Kalman-refined corrections
        for correction in corrected_pois:
            poi_idx = correction['poi_idx']
            original_poi = self.poi_wgs84.iloc[poi_idx]

            # Convert suggested and refined points to WGS84
            suggested_gdf = gpd.GeoDataFrame([1], geometry=[correction['suggested_point']], crs=self.crs_target)
            suggested_wgs84 = suggested_gdf.to_crs('EPSG:4326').iloc[0].geometry

            refined_gdf = gpd.GeoDataFrame([1], geometry=[correction['kalman_refined_point']], crs=self.crs_target)
            refined_wgs84 = refined_gdf.to_crs('EPSG:4326').iloc[0].geometry

            # Original position (red)
            folium.Marker(
                location=[original_poi.geometry.y, original_poi.geometry.x],
                icon=folium.Icon(color='red', icon='exclamation-sign', prefix='glyphicon'),
                popup=f"""
                <b>Original POI (Layer 3)</b><br>
                POI Index: {poi_idx}<br>
                Method: {correction['method']}<br>
                Original Confidence: {correction['confidence']:.2f}
                """
            ).add_to(m)

            # Previous suggestion (orange, smaller)
            folium.CircleMarker(
                location=[suggested_wgs84.y, suggested_wgs84.x],
                radius=4,
                color='orange',
                fill=True,
                fillOpacity=0.7,
                popup=f"""
                <b>Previous Suggestion</b><br>
                From: {correction['method'].replace('_kalman', '')}<br>
                """
            ).add_to(m)

            # Kalman refined position (green)
            folium.Marker(
                location=[refined_wgs84.y, refined_wgs84.x],
                icon=folium.Icon(color='green', icon='ok-sign', prefix='glyphicon'),
                popup=f"""
                <b>Kalman Refined Position</b><br>
                Method: Kalman Filter<br>
                Confidence: {correction['kalman_confidence']:.2f}<br>
                Uncertainty: {correction['position_uncertainty']:.2f}m
                """
            ).add_to(m)

            # Connection lines
            # Original to previous suggestion
            folium.PolyLine(
                locations=[
                    [original_poi.geometry.y, original_poi.geometry.x],
                    [suggested_wgs84.y, suggested_wgs84.x]
                ],
                color='orange',
                weight=1,
                opacity=0.5,
                dash_array='3, 3'
            ).add_to(m)

            # Previous suggestion to refined
            folium.PolyLine(
                locations=[
                    [suggested_wgs84.y, suggested_wgs84.x],
                    [refined_wgs84.y, refined_wgs84.x]
                ],
                color='green',
                weight=2,
                opacity=0.8
            ).add_to(m)

        # Add legend
        legend_html = '''
        <div style="position: fixed;
                    bottom: 50px; left: 50px; width: 240px; height: 160px;
                    background-color: white; border:2px solid grey; z-index:9999;
                    font-size:14px; padding: 10px">
        <b>Layer 3: Kalman Filtering</b><br>
        <i class="fa fa-circle" style="color:lightgray"></i> Normal POIs<br>
        <i class="fa fa-exclamation-triangle" style="color:red"></i> Original Position<br>
        <i class="fa fa-circle" style="color:orange"></i> Previous Suggestion<br>
        <i class="fa fa-check" style="color:green"></i> Kalman Refined<br>
        <hr style="margin: 5px 0;">
        Refined: {}<br>
        <small>Green lines show Kalman refinement</small>
        </div>
        '''.format(len(corrected_pois))
        m.get_root().html.add_child(folium.Element(legend_html))

        # Save map
        filename = 'layer3_kalman_filtering.html'
        m.save(filename)
        print(f"‚úÖ Layer 3 visualization saved to {filename}")

    def layer4_consensus_validation(self, min_consensus=2, create_viz=True):
        """
        Layer 4: Consensus validation - combine results from all layers
        """
        print("üéØ Layer 4: Consensus Validation...")

        # Collect all corrections
        all_corrections = {}

        # Process each layer's results
        for layer_name, corrections in self.analysis_results.items():
            for correction in corrections:
                poi_idx = correction['poi_idx']
                if poi_idx not in all_corrections:
                    all_corrections[poi_idx] = []
                all_corrections[poi_idx].append(correction)

        # Find consensus corrections
        consensus_corrections = []

        for poi_idx, corrections in all_corrections.items():
            if len(corrections) >= min_consensus:
                # Calculate weighted average of suggested positions
                total_weight = 0
                weighted_x = 0
                weighted_y = 0

                for correction in corrections:
                    # Get the best available point (Kalman refined or original suggestion)
                    if 'kalman_refined_point' in correction:
                        point = correction['kalman_refined_point']
                        confidence = correction['kalman_confidence']
                    else:
                        point = correction['suggested_point']
                        confidence = correction['confidence']

                    weighted_x += point.x * confidence
                    weighted_y += point.y * confidence
                    total_weight += confidence

                if total_weight > 0:
                    consensus_x = weighted_x / total_weight
                    consensus_y = weighted_y / total_weight
                    consensus_point = Point(consensus_x, consensus_y)

                    # Calculate overall confidence
                    overall_confidence = total_weight / len(corrections)

                    consensus_corrections.append({
                        'poi_idx': poi_idx,
                        'original_point': corrections[0]['original_point'],
                        'consensus_point': consensus_point,
                        'overall_confidence': overall_confidence,
                        'num_methods_agree': len(corrections),
                        'contributing_methods': [c['method'] for c in corrections],
                        'street_name': corrections[0]['street_name'],
                        'all_corrections': corrections  # Store all contributing corrections
                    })

        self.analysis_results['consensus'] = consensus_corrections
        print(f"‚úì {len(consensus_corrections)} POIs passed consensus validation")

        # Create visualization for Layer 4
        if create_viz:
            self._visualize_layer4(consensus_corrections)

        return consensus_corrections

    def _visualize_layer4(self, consensus_corrections):
        """
        Create visualization for Layer 4 - Consensus Validation
        """
        print("üìä Creating Layer 4 visualization...")

        m = self._create_base_map("Layer 4: Consensus Validation Results")

        # Add all original POIs in light gray
        for idx, poi in self.poi_wgs84.iterrows():
            folium.CircleMarker(
                location=[poi.geometry.y, poi.geometry.x],
                radius=3,
                color='lightgray',
                fill=True,
                fillOpacity=0.4,
                popup=f"POI {idx} (Normal)",
                weight=1
            ).add_to(m)

        # Add consensus corrections
        for correction in consensus_corrections:
            poi_idx = correction['poi_idx']
            original_poi = self.poi_wgs84.iloc[poi_idx]

            # Convert consensus point to WGS84
            consensus_gdf = gpd.GeoDataFrame([1], geometry=[correction['consensus_point']], crs=self.crs_target)
            consensus_wgs84 = consensus_gdf.to_crs('EPSG:4326').iloc[0].geometry

            # Show all contributing suggestions as small circles
            colors = ['orange', 'blue', 'purple', 'pink']
            for i, contrib_correction in enumerate(correction['all_corrections']):
                if 'kalman_refined_point' in contrib_correction:
                    point = contrib_correction['kalman_refined_point']
                else:
                    point = contrib_correction['suggested_point']

                point_gdf = gpd.GeoDataFrame([1], geometry=[point], crs=self.crs_target)
                point_wgs84 = point_gdf.to_crs('EPSG:4326').iloc[0].geometry

                folium.CircleMarker(
                    location=[point_wgs84.y, point_wgs84.x],
                    radius=3,
                    color=colors[i % len(colors)],
                    fill=True,
                    fillOpacity=0.8,
                    popup=f"Method: {contrib_correction['method']}",
                    weight=2
                ).add_to(m)

            # Original position (red)
            folium.Marker(
                location=[original_poi.geometry.y, original_poi.geometry.x],
                icon=folium.Icon(color='red', icon='exclamation-sign', prefix='glyphicon'),
                popup=f"""
                <b>Original POI (Consensus)</b><br>
                POI Index: {poi_idx}<br>
                Methods Agree: {correction['num_methods_agree']}<br>
                Contributing: {', '.join(correction['contributing_methods'])}
                """
            ).add_to(m)

            # Final consensus position (dark green)
            folium.Marker(
                location=[consensus_wgs84.y, consensus_wgs84.x],
                icon=folium.Icon(color='darkgreen', icon='star', prefix='glyphicon'),
                popup=f"""
                <b>FINAL CONSENSUS POSITION</b><br>
                Overall Confidence: {correction['overall_confidence']:.2f}<br>
                Methods Agree: {correction['num_methods_agree']}<br>
                Street: {correction['street_name']}<br>
                Methods: {', '.join(correction['contributing_methods'])}
                """
            ).add_to(m)

            # Connection line (thick green)
            folium.PolyLine(
                locations=[
                    [original_poi.geometry.y, original_poi.geometry.x],
                    [consensus_wgs84.y, consensus_wgs84.x]
                ],
                color='darkgreen',
                weight=4,
                opacity=0.8
            ).add_to(m)

        # Add legend
        legend_html = '''
        <div style="position: fixed;
                    bottom: 50px; left: 50px; width: 260px; height: 180px;
                    background-color: white; border:2px solid grey; z-index:9999;
                    font-size:14px; padding: 10px">
        <b>Layer 4: Consensus Validation</b><br>
        <i class="fa fa-circle" style="color:lightgray"></i> Normal POIs<br>
        <i class="fa fa-exclamation-triangle" style="color:red"></i> Original Position<br>
        <i class="fa fa-circle" style="color:orange"></i> Layer 1 Suggestion<br>
        <i class="fa fa-circle" style="color:blue"></i> Layer 2 Suggestion<br>
        <i class="fa fa-star" style="color:darkgreen"></i> <b>FINAL CONSENSUS</b><br>
        <hr style="margin: 5px 0;">
        Final Corrections: {}<br>
        <small>Thick green lines show final corrections</small>
        </div>
        '''.format(len(consensus_corrections))
        m.get_root().html.add_child(folium.Element(legend_html))

        # Save map
        filename = 'layer4_consensus_validation.html'
        m.save(filename)
        print(f"‚úÖ Layer 4 visualization saved to {filename}")

    def _snap_to_roadside(self, poi_point, street_line, offset_distance=3):
        """
        Snap POI to roadside with specified offset
        """
        # Find closest point on street
        closest_point = nearest_points(poi_point, street_line)[1]

        # Calculate perpendicular offset
        if hasattr(street_line, 'coords'):
            coords = list(street_line.coords)
        else:
            coords = [closest_point.coords[0]]

        if len(coords) < 2:
            return closest_point

        # Find the closest segment
        min_dist = float('inf')
        best_segment = None

        for i in range(len(coords) - 1):
            segment = LineString([coords[i], coords[i + 1]])
            dist = segment.distance(poi_point)
            if dist < min_dist:
                min_dist = dist
                best_segment = segment

        if best_segment is None:
            return closest_point

        # Calculate perpendicular direction
        p1, p2 = best_segment.coords[0], best_segment.coords[1]
        dx = p2[0] - p1[0]
        dy = p2[1] - p1[1]
        length = np.sqrt(dx**2 + dy**2)

        if length == 0:
            return closest_point

        # Normalize and rotate 90 degrees
        perp_x = -dy / length
        perp_y = dx / length

        # Determine which side to offset to (closest side)
        offset_point1 = Point(closest_point.x + perp_x * offset_distance,
                             closest_point.y + perp_y * offset_distance)
        offset_point2 = Point(closest_point.x - perp_x * offset_distance,
                             closest_point.y - perp_y * offset_distance)

        # Choose the offset point closer to original POI
        if offset_point1.distance(poi_point) < offset_point2.distance(poi_point):
            return offset_point1
        else:
            return offset_point2

    def _calculate_spatial_confidence(self, distance, min_dist, max_dist):
        """
        Calculate confidence score based on distance from road
        """
        if distance <= min_dist:
            return 0.9  # High confidence - clearly misplaced
        elif distance >= max_dist:
            return 0.1  # Low confidence - might be legitimate
        else:
            # Linear interpolation between min and max
            return 0.9 - 0.8 * (distance - min_dist) / (max_dist - min_dist)

    def run_full_analysis(self, create_layer_viz=True):
        """
        Run all layers of analysis with optional visualization after each layer
        """
        print("üöÄ Starting Multi-Layer POI Correction Analysis...")

        # Layer 1: Spatial Analysis
        self.layer1_spatial_analysis(create_viz=create_layer_viz)

        # Layer 2: ML Anomaly Detection
        self.layer2_ml_anomaly_detection(create_viz=create_layer_viz)

        # Layer 3: Kalman Filtering
        self.layer3_kalman_filtering(create_viz=create_layer_viz)

        # Layer 4: Consensus Validation
        final_corrections = self.layer4_consensus_validation(create_viz=create_layer_viz)

        print(f"\n‚úÖ Analysis Complete!")
        print(f"üìä Summary:")
        print(f"   - Layer 1 (Spatial): {len(self.analysis_results.get('layer1', []))} detections")
        print(f"   - Layer 2 (ML): {len(self.analysis_results.get('layer2', []))} detections")
        print(f"   - Layer 3 (Kalman): {len(self.analysis_results.get('layer3', []))} refinements")
        print(f"   - Final Consensus: {len(final_corrections)} corrections")

        if create_layer_viz:
            print(f"\nüìÅ Generated Visualizations:")
            print(f"   - layer1_spatial_analysis.html")
            print(f"   - layer2_ml_anomaly_detection.html")
            print(f"   - layer3_kalman_filtering.html")
            print(f"   - layer4_consensus_validation.html")

        return final_corrections

    def create_comprehensive_comparison(self, output_file='comprehensive_comparison.html'):
        """
        Create a comprehensive comparison showing all layers side by side
        """
        print(f"üó∫Ô∏è Creating comprehensive comparison: {output_file}")

        # Get all corrections
        layer1_corrections = self.analysis_results.get('layer1', [])
        layer2_corrections = self.analysis_results.get('layer2', [])
        layer3_corrections = self.analysis_results.get('layer3', [])
        consensus_corrections = self.analysis_results.get('consensus', [])

        if not any([layer1_corrections, layer2_corrections, layer3_corrections, consensus_corrections]):
            print("No corrections to visualize")
            return

        m = self._create_base_map("Comprehensive Multi-Layer POI Correction Analysis")

        # Add all original POIs
        for idx, poi in self.poi_wgs84.iterrows():
            folium.CircleMarker(
                location=[poi.geometry.y, poi.geometry.x],
                radius=2,
                color='lightgray',
                fill=True,
                fillOpacity=0.3,
                popup=f"POI {idx}",
                weight=1
            ).add_to(m)

        # Collect all POI indices that were corrected by any layer
        all_corrected_pois = set()
        for corrections in [layer1_corrections, layer2_corrections, layer3_corrections, consensus_corrections]:
            for correction in corrections:
                all_corrected_pois.add(correction['poi_idx'])

        # For each corrected POI, show the progression through layers
        for poi_idx in all_corrected_pois:
            original_poi = self.poi_wgs84.iloc[poi_idx]

            # Find corrections from each layer for this POI
            l1_correction = next((c for c in layer1_corrections if c['poi_idx'] == poi_idx), None)
            l2_correction = next((c for c in layer2_corrections if c['poi_idx'] == poi_idx), None)
            l3_correction = next((c for c in layer3_corrections if c['poi_idx'] == poi_idx), None)
            consensus_correction = next((c for c in consensus_corrections if c['poi_idx'] == poi_idx), None)

            # Original position (red)
            folium.Marker(
                location=[original_poi.geometry.y, original_poi.geometry.x],
                icon=folium.Icon(color='red', icon='home', prefix='glyphicon'),
                popup=f"""
                <b>POI {poi_idx} - Original Position</b><br>
                Layer 1: {'‚úì' if l1_correction else '‚úó'}<br>
                Layer 2: {'‚úì' if l2_correction else '‚úó'}<br>
                Layer 3: {'‚úì' if l3_correction else '‚úó'}<br>
                Consensus: {'‚úì' if consensus_correction else '‚úó'}
                """
            ).add_to(m)

            # Layer 1 suggestion (orange)
            if l1_correction:
                l1_gdf = gpd.GeoDataFrame([1], geometry=[l1_correction['suggested_point']], crs=self.crs_target)
                l1_wgs84 = l1_gdf.to_crs('EPSG:4326').iloc[0].geometry

                folium.CircleMarker(
                    location=[l1_wgs84.y, l1_wgs84.x],
                    radius=4,
                    color='orange',
                    fill=True,
                    fillOpacity=0.8,
                    popup=f"Layer 1: Spatial Analysis<br>Confidence: {l1_correction['confidence']:.2f}",
                    weight=2
                ).add_to(m)

            # Layer 2 suggestion (blue)
            if l2_correction:
                l2_gdf = gpd.GeoDataFrame([1], geometry=[l2_correction['suggested_point']], crs=self.crs_target)
                l2_wgs84 = l2_gdf.to_crs('EPSG:4326').iloc[0].geometry

                folium.CircleMarker(
                    location=[l2_wgs84.y, l2_wgs84.x],
                    radius=4,
                    color='blue',
                    fill=True,
                    fillOpacity=0.8,
                    popup=f"Layer 2: ML Anomaly<br>Confidence: {l2_correction['confidence']:.2f}",
                    weight=2
                ).add_to(m)

            # Layer 3 refinement (purple)
            if l3_correction:
                l3_gdf = gpd.GeoDataFrame([1], geometry=[l3_correction['kalman_refined_point']], crs=self.crs_target)
                l3_wgs84 = l3_gdf.to_crs('EPSG:4326').iloc[0].geometry

                folium.CircleMarker(
                    location=[l3_wgs84.y, l3_wgs84.x],
                    radius=4,
                    color='purple',
                    fill=True,
                    fillOpacity=0.8,
                    popup=f"Layer 3: Kalman Filter<br>Confidence: {l3_correction['kalman_confidence']:.2f}",
                    weight=2
                ).add_to(m)

            # Final consensus (dark green)
            if consensus_correction:
                consensus_gdf = gpd.GeoDataFrame([1], geometry=[consensus_correction['consensus_point']], crs=self.crs_target)
                consensus_wgs84 = consensus_gdf.to_crs('EPSG:4326').iloc[0].geometry

                folium.Marker(
                    location=[consensus_wgs84.y, consensus_wgs84.x],
                    icon=folium.Icon(color='darkgreen', icon='star', prefix='glyphicon'),
                    popup=f"""
                    <b>FINAL CONSENSUS</b><br>
                    Confidence: {consensus_correction['overall_confidence']:.2f}<br>
                    Methods: {consensus_correction['num_methods_agree']}
                    """
                ).add_to(m)

                # Final correction line
                folium.PolyLine(
                    locations=[
                        [original_poi.geometry.y, original_poi.geometry.x],
                        [consensus_wgs84.y, consensus_wgs84.x]
                    ],
                    color='darkgreen',
                    weight=3,
                    opacity=0.9
                ).add_to(m)

        # Comprehensive legend
        legend_html = '''
        <div style="position: fixed;
                    bottom: 50px; left: 50px; width: 280px; height: 220px;
                    background-color: white; border:2px solid grey; z-index:9999;
                    font-size:13px; padding: 10px">
        <b>Multi-Layer POI Correction Analysis</b><br>
        <hr style="margin: 3px 0;">
        <i class="fa fa-home" style="color:red"></i> Original POI Position<br>
        <i class="fa fa-circle" style="color:orange"></i> Layer 1: Spatial Analysis<br>
        <i class="fa fa-circle" style="color:blue"></i> Layer 2: ML Anomaly Detection<br>
        <i class="fa fa-circle" style="color:purple"></i> Layer 3: Kalman Filtering<br>
        <i class="fa fa-star" style="color:darkgreen"></i> <b>Final Consensus</b><br>
        <hr style="margin: 3px 0;">
        <b>Summary:</b><br>
        Layer 1: {} detections<br>
        Layer 2: {} detections<br>
        Layer 3: {} refinements<br>
        <b>Final: {} corrections</b><br>
        </div>
        '''.format(
            len(layer1_corrections),
            len(layer2_corrections),
            len(layer3_corrections),
            len(consensus_corrections)
        )
        m.get_root().html.add_child(folium.Element(legend_html))

        # Save map
        m.save(output_file)
        print(f"‚úÖ Comprehensive comparison saved to {output_file}")

    def create_visualization(self, output_file='poi_corrections_map.html'):
        """
        Create interactive map showing final corrections (backward compatibility)
        """
        return self.create_comprehensive_comparison(output_file)

    def export_corrected_data(self, output_file='corrected_pois.csv'):
        """
        Export corrected POI data
        """
        print(f"üíæ Exporting corrected data to: {output_file}")
        corrections = self.analysis_results.get('consensus', [])

        if len(corrections) == 0:
            print("No corrections to export")
            return

        # Create corrected dataset
        corrected_pois = self.poi_gdf.copy()

        for correction in corrections:
            poi_idx = correction['poi_idx']
            consensus_point = correction['consensus_point']

            # Update geometry
            corrected_pois.loc[poi_idx, 'geometry'] = consensus_point

            # Add correction metadata
            corrected_pois.loc[poi_idx, 'CORRECTED'] = True
            corrected_pois.loc[poi_idx, 'CORRECTION_CONFIDENCE'] = correction['overall_confidence']
            corrected_pois.loc[poi_idx, 'CORRECTION_METHODS'] = ','.join(correction['contributing_methods'])
            corrected_pois.loc[poi_idx, 'NUM_METHODS_AGREE'] = correction['num_methods_agree']

        # Convert back to WGS84 and export
        corrected_pois_wgs84 = corrected_pois.to_crs('EPSG:4326')
        corrected_pois_wgs84['x'] = corrected_pois_wgs84.geometry.x
        corrected_pois_wgs84['y'] = corrected_pois_wgs84.geometry.y

        # Export to CSV
        corrected_pois_wgs84.drop(columns=['geometry']).to_csv(output_file, index=False)
        print(f"‚úÖ Corrected POI data exported to {output_file}")

def correct_singapore_pois(poi_csv_path=None, streets_shp_path=None, sample_size=1000, create_layer_visualizations=True):
    """
    Main function to correct Singapore POIs, with optional synthetic data for testing
    Now includes layer-by-layer visualization option
    """
    print("üìç Loading Singapore POI correction system...")

    # Load or generate POI data
    if poi_csv_path:
        try:
            poi_df = pd.read_csv(poi_csv_path)
            # Sample for testing if dataset is large
            if len(poi_df) > sample_size:
                print(f"Sampling {sample_size} POIs from {len(poi_df)} for testing...")
                poi_df = poi_df.sample(n=sample_size, random_state=42)
        except FileNotFoundError:
            raise FileNotFoundError(f"POI CSV file not found at: {poi_csv_path}")
    else:
        # Generate synthetic POI data
        print("Generating synthetic POI data for testing...")
        np.random.seed(42)
        poi_data = {
            'x': 103.85 + np.random.uniform(-0.02, 0.02, sample_size),  # Longitude
            'y': 1.35 + np.random.uniform(-0.02, 0.02, sample_size),    # Latitude
            'STREET_NAM': [f"Street_{i % 5}" for i in range(sample_size)]  # Dummy street names
        }
        poi_df = pd.DataFrame(poi_data)

    # Create GeoDataFrame for POIs
    poi_gdf = gpd.GeoDataFrame(
        poi_df,
        geometry=gpd.points_from_xy(poi_df['x'], poi_df['y']),
        crs='EPSG:4326'
    )

    # Load or generate streets data
    if streets_shp_path:
        try:
            streets_gdf = gpd.read_file(streets_shp_path)
        except FileNotFoundError:
            raise FileNotFoundError(f"Streets shapefile not found at: {streets_shp_path}")
    else:
        # Generate synthetic streets data
        print("Generating synthetic streets data for testing...")
        streets_data = {
            'ST_NAME': ['Main Rd', 'Side St', 'Cross Rd'],
            'geometry': [
                LineString([(103.84, 1.34), (103.86, 1.34)]),  # Horizontal street
                LineString([(103.84, 1.36), (103.86, 1.36)]),  # Another horizontal street
                LineString([(103.85, 1.33), (103.85, 1.37)])   # Vertical street
            ]
        }
        streets_gdf = gpd.GeoDataFrame(streets_data, crs='EPSG:4326')

    # Ensure streets_gdf has a CRS
    if streets_gdf.crs is None:
        print("‚ö†Ô∏è Streets shapefile has no CRS defined. Setting to EPSG:4326 (WGS84).")
        streets_gdf.set_crs('EPSG:4326', inplace=True)
    else:
        print(f"Streets shapefile CRS: {streets_gdf.crs}")

    # Initialize corrector
    corrector = POICorrector(poi_gdf, streets_gdf)

    # Run analysis with layer visualizations
    corrections = corrector.run_full_analysis(create_layer_viz=create_layer_visualizations)

    # Create comprehensive comparison
    corrector.create_comprehensive_comparison(output_file='comprehensive_poi_analysis.html')

    # Export corrected data
    corrector.export_corrected_data(output_file='corrected_pois_final.csv')

    return corrector, corrections

# Usage example
if __name__ == "__main__":
    try:
        # Use synthetic data for testing with layer visualizations
        print("üéØ Running POI correction with layer-by-layer visualization...")
        corrector, corrections = correct_singapore_pois(create_layer_visualizations=True)

        # For actual data, use:
        # corrector, corrections = correct_singapore_pois(
        #     poi_csv_path='/path/to/poi_with_latlong.csv',
        #     streets_shp_path='/path/to/Streets.shp',
        #     create_layer_visualizations=True
        # )

        print("\nüéâ Analysis Complete! Check the following files:")
        print("üìÑ Individual Layer Results:")
        print("   - layer1_spatial_analysis.html")
        print("   - layer2_ml_anomaly_detection.html")
        print("   - layer3_kalman_filtering.html")
        print("   - layer4_consensus_validation.html")
        print("üìÑ Comprehensive Analysis:")
        print("   - comprehensive_poi_analysis.html")
        print("üìÑ Corrected Data:")
        print("   - corrected_pois_final.csv")

    except Exception as e:
        print(f"Error running POI correction: {e}")
        import traceback
        traceback.print_exc()

import pandas as pd
import geopandas as gpd
import numpy as np
from shapely.geometry import Point, LineString
from shapely.ops import nearest_points
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import warnings

warnings.filterwarnings('ignore')

class POICorrector:
    """
    Multi-layered POI correction system with spatial analysis, ML, and Kalman filtering
    """

    def __init__(self, poi_data, streets_data, crs_source='EPSG:4326', crs_target='EPSG:3414'):
        """
        Initialize the POI correction system

        Args:
            poi_data: GeoDataFrame of POI points
            streets_data: GeoDataFrame of street lines
            crs_source: Source coordinate system (WGS84)
            crs_target: Target coordinate system (SVY21 for Singapore)
        """
        if poi_data.crs is None:
            print(f"‚ö†Ô∏è POI data has no CRS defined. Setting to {crs_source}")
            poi_data.set_crs(crs_source, inplace=True)
        if streets_data.crs is None:
            print(f"‚ö†Ô∏è Streets data has no CRS defined. Setting to {crs_source}")
            streets_data.set_crs(crs_source, inplace=True)

        self.poi_gdf = poi_data.to_crs(crs_target) if poi_data.crs != crs_target else poi_data
        self.streets_gdf = streets_data.to_crs(crs_target) if streets_data.crs != crs_target else streets_data
        self.crs_source = crs_source
        self.crs_target = crs_target

        self.analysis_results = {}

    def layer1_spatial_analysis(self, min_distance=3, max_distance=5):
        print("üîç Layer 1: Spatial Analysis...")
        spatial_index = self.streets_gdf.sindex
        misplaced_pois = []

        for idx, poi in self.poi_gdf.iterrows():
            poi_point = poi.geometry
            possible_idx = list(spatial_index.intersection(poi_point.buffer(max_distance).bounds))
            possible = self.streets_gdf.iloc[possible_idx]
            if possible.empty:
                continue

            distances = possible.geometry.distance(poi_point)
            min_dist = distances.min()
            if min_distance <= min_dist <= max_distance:
                closest_idx = distances.idxmin()
                closest_street = self.streets_gdf.loc[closest_idx]
                snap_point = self._snap_to_roadside(poi_point, closest_street.geometry)
                misplaced_pois.append({
                    'poi_idx': idx,
                    'original_point': poi_point,
                    'closest_street_idx': closest_idx,
                    'distance_to_road': min_dist,
                    'suggested_point': snap_point,
                    'confidence': self._calculate_spatial_confidence(min_dist, min_distance, max_distance),
                    'street_name': closest_street.get('ST_NAME', 'Unknown'),
                    'method': 'spatial_analysis'
                })

        self.analysis_results['layer1'] = misplaced_pois
        print(f"‚úì Found {len(misplaced_pois)} potentially misplaced POIs")
        return misplaced_pois

    def layer2_ml_anomaly_detection(self):
        print("ü§ñ Layer 2: ML Anomaly Detection...")
        features, poi_indices = [], []

        for idx, poi in self.poi_gdf.iterrows():
            poi_point = poi.geometry
            nearby = self.streets_gdf[self.streets_gdf.geometry.distance(poi_point) <= 50]
            if nearby.empty:
                continue

            distances = nearby.geometry.distance(poi_point)
            feat = [
                distances.min(),
                distances.mean(),
                distances.std() if len(distances) > 1 else 0,
                len(nearby),
                poi_point.x,
                poi_point.y
            ]
            buffer_area = poi_point.buffer(100)
            streets_in_buffer = self.streets_gdf[self.streets_gdf.geometry.intersects(buffer_area)]
            total_len = streets_in_buffer.geometry.length.sum()
            feat.append(total_len / buffer_area.area if buffer_area.area > 0 else 0)

            features.append(feat)
            poi_indices.append(idx)

        if not features:
            self.analysis_results['layer2'] = []
            print("‚úì No features extracted for ML analysis")
            return []

        scaler = StandardScaler()
        feats_scaled = scaler.fit_transform(features)

        iso = IsolationForest(contamination=0.1, random_state=42)
        labels = iso.fit_predict(feats_scaled)
        scores = iso.score_samples(feats_scaled)

        dbscan = DBSCAN(eps=0.5, min_samples=5)
        cluster_labels = dbscan.fit_predict(feats_scaled[:, [4, 5]])

        anomalous = []
        for poi_idx, label, score, cluster in zip(poi_indices, labels, scores, cluster_labels):
            if label == -1:
                poi = self.poi_gdf.loc[poi_idx]
                closest_idx = self.streets_gdf.geometry.distance(poi.geometry).idxmin()
                closest_street = self.streets_gdf.loc[closest_idx]
                suggested = self._snap_to_roadside(poi.geometry, closest_street.geometry)
                anomalous.append({
                    'poi_idx': poi_idx,
                    'original_point': poi.geometry,
                    'closest_street_idx': closest_idx,
                    'distance_to_road': closest_street.geometry.distance(poi.geometry),
                    'suggested_point': suggested,
                    'confidence': abs(score),
                    'cluster_id': cluster,
                    'street_name': closest_street.get('ST_NAME', 'Unknown'),
                    'method': 'ml_anomaly'
                })

        self.analysis_results['layer2'] = anomalous
        print(f"‚úì ML detected {len(anomalous)} anomalous POIs")
        return anomalous

    def layer3_kalman_filtering(self, process_noise=0.1, measurement_noise=1.0):
        print("üîÑ Layer 3: Kalman Filtering...")
        all_corr = self.analysis_results.get('layer1', []) + self.analysis_results.get('layer2', [])
        if not all_corr:
            self.analysis_results['layer3'] = []
            print("‚úì No corrections to refine with Kalman filtering")
            return []

        refined = []
        for c in all_corr:
            orig = np.array([c['original_point'].x, c['original_point'].y])
            sugg = np.array([c['suggested_point'].x, c['suggested_point'].y])

            state = np.array([orig[0], orig[1], 0, 0])
            Q = np.eye(4) * process_noise
            Q[2:, 2:] *= 0.1
            R = np.eye(2) * measurement_noise
            F = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]])
            H = np.array([[1, 0, 0, 0], [0, 1, 0, 0]])
            P = np.eye(4) * 10

            state_pred = F @ state
            P_pred = F @ P @ F.T + Q

            y = sugg - H @ state_pred
            S = H @ P_pred @ H.T + R
            K = P_pred @ H.T @ np.linalg.inv(S)

            state_upd = state_pred + K @ y
            P_upd = (np.eye(4) - K @ H) @ P_pred

            refined_pos = state_upd[:2]
            refined_point = Point(refined_pos[0], refined_pos[1])
            unc = np.sqrt(P_upd[0, 0] + P_upd[1, 1])
            kalman_conf = 1.0 / (1.0 + unc)

            refined.append({
                **c,
                'kalman_refined_point': refined_point,
                'kalman_confidence': kalman_conf,
                'position_uncertainty': unc,
                'method': f"{c['method']}_kalman"
            })

        self.analysis_results['layer3'] = refined
        print(f"‚úì Kalman refined {len(refined)} POI corrections")
        return refined

    def layer4_consensus_validation(self, min_consensus=2):
        print("üéØ Layer 4: Consensus Validation...")
        groups = {}
        for layer, corrs in self.analysis_results.items():
            for c in corrs:
                idx = c['poi_idx']
                groups.setdefault(idx, []).append(c)

        consensus = []
        for idx, corrs in groups.items():
            if len(corrs) >= min_consensus:
                total_w = 0
                wx = 0
                wy = 0
                for c in corrs:
                    if 'kalman_refined_point' in c:
                        pt = c['kalman_refined_point']
                        w = c['kalman_confidence']
                    else:
                        pt = c['suggested_point']
                        w = c['confidence']
                    wx += pt.x * w
                    wy += pt.y * w
                    total_w += w
                cx = wx / total_w
                cy = wy / total_w
                consensus.append({
                    'poi_idx': idx,
                    'original_point': corrs[0]['original_point'],
                    'consensus_point': Point(cx, cy),
                    'overall_confidence': total_w / len(corrs),
                    'num_methods_agree': len(corrs),
                    'contributing_methods': [c['method'] for c in corrs],
                    'street_name': corrs[0]['street_name']
                })

        self.analysis_results['consensus'] = consensus
        print(f"‚úì {len(consensus)} POIs passed consensus validation")
        return consensus

    def _snap_to_roadside(self, poi_point, street_line, offset_distance=3):
        closest = nearest_points(poi_point, street_line)[1]
        coords = list(street_line.coords) if hasattr(street_line, 'coords') else [closest.coords[0]]
        if len(coords) < 2:
            return closest
        min_d = float('inf')
        best = None
        for i in range(len(coords) - 1):
            seg = LineString([coords[i], coords[i+1]])
            d = seg.distance(poi_point)
            if d < min_d:
                min_d = d
                best = seg
        if best is None:
            return closest
        p1, p2 = best.coords[0], best.coords[1]
        dx = p2[0] - p1[0]
        dy = p2[1] - p1[1]
        length = np.sqrt(dx**2 + dy**2)
        if length == 0:
            return closest
        perp_x = -dy / length
        perp_y = dx / length
        o1 = Point(closest.x + perp_x * offset_distance, closest.y + perp_y * offset_distance)
        o2 = Point(closest.x - perp_x * offset_distance, closest.y - perp_y * offset_distance)
        return o1 if o1.distance(poi_point) < o2.distance(poi_point) else o2

    def _calculate_spatial_confidence(self, distance, min_dist, max_dist):
        if distance <= min_dist:
            return 0.9
        if distance >= max_dist:
            return 0.1
        return 0.9 - 0.8 * (distance - min_dist) / (max_dist - min_dist)

    def run_full_analysis(self):
        print("üöÄ Starting Multi-Layer POI Correction Analysis...")
        self.layer1_spatial_analysis()
        self.layer2_ml_anomaly_detection()
        self.layer3_kalman_filtering()
        final = self.layer4_consensus_validation()
        print(f"\n‚úÖ Analysis Complete!")
        print(f"   - Layer 1 (Spatial): {len(self.analysis_results.get('layer1', []))}")
        print(f"   - Layer 2 (ML): {len(self.analysis_results.get('layer2', []))}")
        print(f"   - Layer 3 (Kalman): {len(self.analysis_results.get('layer3', []))}")
        print(f"   - Final Consensus: {len(final)}")
        return final

    def create_visualization(self, output_prefix='poi_corrections'):
        """
        Create two matplotlib-based visualizations showing:
          1. Scatter of original vs corrected POIs with arrows.
          2. Histogram of correction distances.
        Saves to:
          {output_prefix}_scatter.png
          {output_prefix}_histogram.png
        """
        print("üìà Creating graph-based visualizations...")
        corrections = self.analysis_results.get('consensus', [])
        if not corrections:
            print("No corrections to visualize")
            return
        poi_wgs84 = self.poi_gdf.to_crs('EPSG:4326')
        orig_pts, corr_pts, distances = [], [], []
        for c in corrections:
            orig = poi_wgs84.iloc[c['poi_idx']].geometry
            cons_gdf = gpd.GeoDataFrame(geometry=[c['consensus_point']], crs=self.crs_target).to_crs('EPSG:4326')
            cons = cons_gdf.geometry.iloc[0]
            orig_pts.append(orig)
            corr_pts.append(cons)
            distances.append(orig.distance(cons))
        lon0 = [p.x for p in orig_pts]
        lat0 = [p.y for p in orig_pts]
        lon1 = [p.x for p in corr_pts]
        lat1 = [p.y for p in corr_pts]
        plt.figure(figsize=(8, 6))
        plt.scatter(lon0, lat0, label='Original POIs', alpha=0.6)
        plt.scatter(lon1, lat1, label='Corrected POIs', alpha=0.6)
        for x0, y0, x1, y1 in zip(lon0, lat0, lon1, lat1):
            plt.arrow(x0, y0, x1 - x0, y1 - y0, head_width=0.0001, length_includes_head=True, alpha=0.4, linewidth=0.8)
        plt.title('Original vs. Corrected POI Locations')
        plt.xlabel('Longitude')
        plt.ylabel('Latitude')
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        scatter_file = f"{output_prefix}_scatter.png"
        plt.savefig(scatter_file, dpi=150)
        print(f"‚úì Scatter plot saved to {scatter_file}")
        plt.figure(figsize=(7, 5))
        sns.histplot(distances, bins=20, kde=False)
        plt.title('Distribution of POI Correction Distances')
        plt.xlabel('Distance (degrees')
        plt.ylabel('Count')
        plt.tight_layout()
        hist_file = f"{output_prefix}_histogram.png"
        plt.savefig(hist_file, dpi=150)
        print(f"‚úì Histogram saved to {hist_file}")

    def export_corrected_data(self, output_file='corrected_poi